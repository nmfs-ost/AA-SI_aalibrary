{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a320ce",
   "metadata": {},
   "source": [
    "# COMPARE LOCAL FILES TO NCEI FILES\n",
    "The purpose of this script is to be able to provide a single place for checking whether the files that exist locally, also exist on NCEI. This script also gives you the option of uploading files that are not in NCEI to GCP for intermittent storage.\n",
    "Just follow along with the instructions in the script to be able to compare and upload files to GCP.\n",
    "\n",
    "NOTE: This script will compare based on echosounder folders. E.g. Reuben_Lasker/RL2107/EK80/... This is done to help with chunking the survey into manageable pieces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79baa361",
   "metadata": {},
   "source": [
    "## Step 1: Fill Out These Variables To Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b02d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VARIABLES TO FILL OUT ---\n",
    "ship_name = \"Reuben Lasker\"\n",
    "cruise_name = \"RL2107\"\n",
    "echosounder = \"EK80\"\n",
    "local_echosounder_directory_path = \"../Reuben_Lasker/RL2107/EK80/\"\n",
    "# Set to True if you want to cache files that do not exist in NCEI by uploading\n",
    "# them to GCP.\n",
    "upload_non_s3_files_to_gcp = True\n",
    "gcp_project_id = \"ggn-nmfs-aa-dev-1\"\n",
    "gcp_bucket_name = \"ggn-nmfs-aa-dev-1-data\"\n",
    "# --- VARIABLES TO FILL OUT ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465bdad8",
   "metadata": {},
   "source": [
    "## Step 2: Run, But Ignore, The Cells Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1a0d3b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HELPER FUNCTIONS ---\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "from typing import Tuple, List, Union\n",
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config\n",
    "import glob\n",
    "import string\n",
    "from difflib import get_close_matches\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import traceback\n",
    "\n",
    "\n",
    "RAW_DATA_FILE_TYPES = [\"raw\", \"idx\", \"bot\"]\n",
    "CONVERTED_DATA_FILE_TYPES = [\"netcdf\", \"nc\"]\n",
    "\n",
    "\n",
    "def setup_gcp_storage_objs(\n",
    "    project_id: str = \"ggn-nmfs-aa-dev-1\",\n",
    "    gcp_bucket_name: str = \"ggn-nmfs-aa-dev-1-data\",\n",
    ") -> Tuple[storage.Client, str, storage.Client.bucket]:\n",
    "    \"\"\"Sets up Google Cloud Platform storage objects for use in accessing and\n",
    "    modifying storage buckets.\n",
    "\n",
    "    Args:\n",
    "        project_id (str, optional): The project id of the project you want to\n",
    "            access. Defaults to \"ggn-nmfs-aa-dev-1\".\n",
    "        gcp_bucket_name (str, optional): The name of the exact bucket you want\n",
    "            to access. Defaults to \"ggn-nmfs-aa-dev-1-data\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[storage.Client, str, storage.Client.bucket]: The storage client,\n",
    "            followed by the GCP bucket name (str) and then the actual bucket\n",
    "            object itself (which will be executing the commands used in this\n",
    "            api).\n",
    "    \"\"\"\n",
    "\n",
    "    gcp_stor_client = storage.Client(project=project_id)\n",
    "\n",
    "    gcp_bucket = gcp_stor_client.bucket(gcp_bucket_name)\n",
    "\n",
    "    return (gcp_stor_client, gcp_bucket_name, gcp_bucket)\n",
    "\n",
    "\n",
    "def create_s3_objs(bucket_name: str = \"noaa-wcsd-pds\") -> Tuple:\n",
    "    \"\"\"Creates the s3 objects needed for using boto3 for a particular bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket_name (str, optional): The bucket you want to refer to. The\n",
    "            default points to the NCEI bucket. Defaults to \"noaa-wcsd-pds\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple: The s3 client (used for certain portions of the boto3 api), the\n",
    "            s3 resource (newer, more used object for accessing s3 buckets), and\n",
    "            the actual s3 bucket itself.\n",
    "    \"\"\"\n",
    "    # Setup access to S3 bucket as an anonymous user\n",
    "    s3_client = boto3.client(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=\"\",\n",
    "        aws_secret_access_key=\"\",\n",
    "        config=Config(signature_version=UNSIGNED),\n",
    "    )\n",
    "    s3_resource = boto3.resource(\n",
    "        \"s3\",\n",
    "        aws_access_key_id=\"\",\n",
    "        aws_secret_access_key=\"\",\n",
    "        config=Config(signature_version=UNSIGNED),\n",
    "    )\n",
    "    s3_bucket = s3_resource.Bucket(bucket_name)\n",
    "    return s3_client, s3_resource, s3_bucket\n",
    "\n",
    "\n",
    "def get_all_file_names_in_a_surveys_echosounder_folder(\n",
    "    ship_name: str = \"\",\n",
    "    survey_name: str = \"\",\n",
    "    echosounder: str = \"\",\n",
    "    s3_resource: boto3.resource = None,\n",
    "    return_full_paths: bool = False,\n",
    ") -> List[str]:\n",
    "    \"\"\"Gets all of the file names from a particular NCEI survey's echosounder\n",
    "    folder.\n",
    "\n",
    "    Args:\n",
    "        ship_name (str, optional): The ship's name you want to get all surveys\n",
    "            from. Defaults to None.\n",
    "            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n",
    "            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n",
    "            ship names.\n",
    "        survey_name (str, optional): The survey name exactly as it is in NCEI.\n",
    "            Defaults to \"\".\n",
    "        echosounder (str, optional): The echosounder used. Defaults to \"\".\n",
    "        s3_resource (boto3.resource, optional): The resource used to perform\n",
    "            this operation. Defaults to None, but creates a client for you\n",
    "            instead.\n",
    "        return_full_paths (bool, optional): Whether or not you want a full\n",
    "            path from bucket root to the subdirectory returned. Set to false\n",
    "            if you only want the subdirectory names listed. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, each being the file name. Whether\n",
    "            these are full paths or just file names are specified by the\n",
    "            `return_full_paths` parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    survey_prefix = f\"data/raw/{ship_name}/{survey_name}/{echosounder}/\"\n",
    "    all_files = list_all_objects_in_s3_bucket_location(\n",
    "        prefix=survey_prefix,\n",
    "        s3_resource=s3_resource,\n",
    "        return_full_paths=return_full_paths,\n",
    "    )\n",
    "    return all_files\n",
    "\n",
    "\n",
    "def list_all_objects_in_s3_bucket_location(\n",
    "    prefix: str = \"\",\n",
    "    s3_resource: boto3.resource = None,\n",
    "    return_full_paths: bool = False,\n",
    "    bucket_name: str = \"noaa-wcsd-pds\",\n",
    ") -> List[str]:\n",
    "    \"\"\"Lists all of the objects in a s3 bucket location denoted by `prefix`.\n",
    "    Returns a list containing str. You get full paths if you specify the\n",
    "    `return_full_paths` parameter.\n",
    "\n",
    "    Args:\n",
    "        prefix (str, optional): The bucket location. Defaults to \"\".\n",
    "        s3_resource (boto3.resource, optional): The bucket resource object.\n",
    "            Defaults to None.\n",
    "        return_full_paths (bool, optional): Whether or not you want a full\n",
    "            path from bucket root to the subdirectory returned. Set to false\n",
    "            if you only want the subdirectory names listed. Defaults to False.\n",
    "        bucket_name (str, optional): The bucket name. Defaults to\n",
    "            \"noaa-wcsd-pds\".\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings containing either the objects name or\n",
    "            path, dependent on the `return_full_paths` parameter.\n",
    "    \"\"\"\n",
    "    if not s3_resource:\n",
    "        _, s3_resource, _ = create_s3_objs(bucket_name)\n",
    "\n",
    "    object_keys = set()\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    for obj in bucket.objects.filter(Prefix=prefix):\n",
    "        if return_full_paths:\n",
    "            object_keys.add(obj.key)\n",
    "        else:\n",
    "            object_keys.add(obj.key.split(\"/\")[-1])\n",
    "\n",
    "    return list(object_keys)\n",
    "\n",
    "\n",
    "def check_if_file_exists_in_s3(\n",
    "    object_key: str = \"\",\n",
    "    s3_resource: boto3.resource = None,\n",
    "    s3_bucket_name: str = \"\",\n",
    ") -> bool:\n",
    "    \"\"\"Checks to see if a file exists in an s3 bucket. Intended for use with\n",
    "    NCEI, but will work with other s3 buckets as well.\n",
    "\n",
    "    Args:\n",
    "        object_key (str, optional): The object key (location of the object).\n",
    "            Defaults to \"\".\n",
    "        s3_resource (boto3.resource, optional): The boto3 resource for this\n",
    "            particular bucket. Defaults to None.\n",
    "        s3_bucket_name (str, optional): The bucket name. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the file exists within the bucket. False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        s3_resource.Object(s3_bucket_name, object_key).load()\n",
    "        return True\n",
    "    except Exception:\n",
    "        # object key does not exist.\n",
    "        # print(e)\n",
    "        return False\n",
    "\n",
    "\n",
    "def get_file_size_from_s3(object_key, s3_resource):\n",
    "    \"\"\"Gets the file size of an object in s3.\"\"\"\n",
    "    obj = s3_resource.Object(\"noaa-wcsd-pds\", object_key)\n",
    "    file_size = obj.content_length\n",
    "    return file_size\n",
    "\n",
    "\n",
    "def get_checksum_sha256_from_s3(object_key, s3_resource):\n",
    "    \"\"\"Gets the SHA-256 checksum of the s3 object.\"\"\"\n",
    "    obj = s3_resource.Object(\"noaa-wcsd-pds\", object_key)\n",
    "    checksum = obj.checksum_sha256\n",
    "    return checksum\n",
    "\n",
    "\n",
    "def get_local_file_size(local_file_path: str) -> int:\n",
    "    \"\"\"Gets the size of a local file in bytes.\n",
    "\n",
    "    Args:\n",
    "        local_file_path (str): The local file path.\n",
    "\n",
    "    Returns:\n",
    "        int: The size of the file in bytes.\n",
    "    \"\"\"\n",
    "    return os.path.getsize(local_file_path)\n",
    "\n",
    "\n",
    "def get_local_sha256_checksum(local_file_path, chunk_size=65536) -> str:\n",
    "    \"\"\"\n",
    "    Calculates the SHA256 checksum of a file.\n",
    "\n",
    "    Args:\n",
    "        local_file_path (str): The path to the file.\n",
    "        chunk_size (int): The size of chunks to read the file in (in bytes).\n",
    "                          Larger chunks can be more efficient for large files.\n",
    "\n",
    "    Returns:\n",
    "        str: The SHA256 checksum of the file as a hexadecimal string.\n",
    "    \"\"\"\n",
    "\n",
    "    sha256_hash = hashlib.sha256()\n",
    "    try:\n",
    "        with open(local_file_path, \"rb\") as f:\n",
    "            # Read the file in chunks to handle large files efficiently\n",
    "            for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "                sha256_hash.update(chunk)\n",
    "        return sha256_hash.hexdigest()\n",
    "    except FileNotFoundError:\n",
    "        return \"File not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "\n",
    "def get_closest_ncei_formatted_ship_name(\n",
    "    ship_name: str = \"\",\n",
    "    s3_client: boto3.client = None,\n",
    ") -> Union[str, None]:\n",
    "    \"\"\"Gets the closest NCEI formatted ship name to the given ship name.\n",
    "    NOTE: Only use if the `data_source`==\"NCEI\".\n",
    "\n",
    "    Args:\n",
    "        ship_name (str, optional): The ship name to search the closest match\n",
    "            for.\n",
    "            Defaults to \"\".\n",
    "        s3_client (boto3.client, optional): The client used to perform this\n",
    "            operation. Defaults to None, but creates a client for you instead.\n",
    "\n",
    "    Returns:\n",
    "        Union[str, None]: The NCEI formatted ship name or None, if none\n",
    "            matched.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create client objects if they dont exist.\n",
    "    if s3_client is None:\n",
    "        s3_client, _, _ = create_s3_objs()\n",
    "\n",
    "    all_ship_names = get_all_ship_names_in_ncei(\n",
    "        normalize=False, s3_client=s3_client, return_full_paths=False\n",
    "    )\n",
    "    close_matches = get_close_matches(\n",
    "        ship_name, all_ship_names, n=3, cutoff=0.85\n",
    "    )\n",
    "    if len(close_matches) >= 1:\n",
    "        return close_matches[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_all_ship_names_in_ncei(\n",
    "    normalize: bool = False,\n",
    "    s3_client: boto3.client = None,\n",
    "    return_full_paths: bool = False,\n",
    "):\n",
    "    \"\"\"Gets all of the ship names from NCEI. This is based on all of the\n",
    "    folders listed under the `data/raw/` prefix.\n",
    "\n",
    "    Args:\n",
    "        normalize (bool, optional): Whether or not to normalize the ship_name\n",
    "            attribute to how GCP stores it. Defaults to False.\n",
    "        s3_client (boto3.client, optional): The client used to perform this\n",
    "            operation. Defaults to None, but creates a client for you instead.\n",
    "        return_full_paths (bool, optional): Whether or not you want a full\n",
    "            path from bucket root to the subdirectory returned. Set to false\n",
    "            if you only want the subdirectory names listed. Defaults to False.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create client objects if they dont exist.\n",
    "    if s3_client is None:\n",
    "        s3_client, _, _ = create_s3_objs()\n",
    "\n",
    "    # Get the initial subdirs\n",
    "    prefix = \"data/raw/\"\n",
    "    subdirs = get_subdirectories_in_s3_bucket_location(\n",
    "        prefix=prefix, s3_client=s3_client, return_full_paths=return_full_paths\n",
    "    )\n",
    "    if normalize:\n",
    "        subdirs = [normalize_ship_name(ship_name=subdir) for subdir in subdirs]\n",
    "    return subdirs\n",
    "\n",
    "\n",
    "def get_subdirectories_in_s3_bucket_location(\n",
    "    prefix: str = \"\",\n",
    "    s3_client: boto3.client = None,\n",
    "    return_full_paths: bool = False,\n",
    "    bucket_name: str = \"noaa-wcsd-pds\",\n",
    ") -> List[str]:\n",
    "    \"\"\"Gets a list of all the subdirectories in a specific bucket location\n",
    "    (called a prefix). The return can be with full paths (root to folder\n",
    "    inclusive), or just the folder names.\n",
    "\n",
    "    Args:\n",
    "        prefix (str, optional): The bucket folder location. Defaults to \"\".\n",
    "        s3_client (boto3.client, optional): The bucket client object.\n",
    "            Defaults to None.\n",
    "        return_full_paths (bool, optional): Whether or not you want a full\n",
    "            path from bucket root to the subdirectory returned. Set to false\n",
    "            if you only want the subdirectory names listed. Defaults to False.\n",
    "        bucket_name (str, optional): The bucket name. Defaults to\n",
    "            \"noaa-wcsd-pds\".\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, each being the subdirectory. Whether\n",
    "            these are full paths or just folder names are specified by the\n",
    "            `return_full_paths` parameter.\n",
    "    \"\"\"\n",
    "    if not s3_client:\n",
    "        s3_client, _, _ = create_s3_objs(bucket_name)\n",
    "\n",
    "    subdirs = set()\n",
    "    result = s3_client.list_objects(\n",
    "        Bucket=bucket_name, Prefix=prefix, Delimiter=\"/\"\n",
    "    )\n",
    "    for o in result.get(\"CommonPrefixes\"):\n",
    "        subdir_full_path_from_prefix = o.get(\"Prefix\")\n",
    "        if return_full_paths:\n",
    "            subdir = subdir_full_path_from_prefix\n",
    "        else:\n",
    "            subdir = subdir_full_path_from_prefix.replace(prefix, \"\")\n",
    "            subdir = subdir.replace(\"/\", \"\")\n",
    "        subdirs.add(subdir)\n",
    "    return list(subdirs)\n",
    "\n",
    "\n",
    "def normalize_ship_name(ship_name: str = \"\") -> str:\n",
    "    \"\"\"Normalizes a ship's name. This is necessary for creating a deterministic\n",
    "    file structure within our GCP storage bucket.\n",
    "    The ship name is returned as a Title_Cased_And_Snake_Cased ship name, with\n",
    "    no punctuation.\n",
    "    Ex. `HENRY B. BIGELOW` will return `Henry_B_Bigelow`\n",
    "\n",
    "    Args:\n",
    "        ship_name (str, optional): The ship name string. Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        str: The formatted and normalized version of the ship name.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lower case the string\n",
    "    ship_name = ship_name.lower()\n",
    "    # Un-normalize (replace `_` with ` ` to help further processing)\n",
    "    # In the edge-case that users include an underscore.\n",
    "    ship_name = ship_name.replace(\"_\", \" \")\n",
    "    # Remove all punctuation.\n",
    "    ship_name = \"\".join(\n",
    "        [char for char in ship_name if char not in string.punctuation]\n",
    "    )\n",
    "    # Title-case it\n",
    "    ship_name = ship_name.title()\n",
    "    # Snake-case it\n",
    "    ship_name = ship_name.replace(\" \", \"_\")\n",
    "\n",
    "    return ship_name\n",
    "\n",
    "\n",
    "def upload_file_to_gcp_bucket(\n",
    "    bucket: storage.Client.bucket,\n",
    "    blob_file_path: str,\n",
    "    local_file_path: str,\n",
    "):\n",
    "    \"\"\"Uploads a file to the blob storage bucket.\n",
    "\n",
    "    Args:\n",
    "        bucket (storage.Client.bucket): The bucket object used for uploading.\n",
    "        blob_file_path (str): The blob's file path.\n",
    "            Ex. \"data/itds/logs/execute_code_files/temp.csv\"\n",
    "            NOTE: This must include the file name as well as the extension.\n",
    "        local_file_path (str): The local file path you wish to upload to the\n",
    "            blob.\n",
    "    \"\"\"\n",
    "\n",
    "    if not bucket:\n",
    "        _, _, bucket = setup_gcp_storage_objs()\n",
    "\n",
    "    blob = bucket.blob(blob_file_path, chunk_size=1024 * 1024 * 1)\n",
    "    # Upload a new blob\n",
    "    try:\n",
    "        blob.upload_from_filename(local_file_path)\n",
    "    except Exception:\n",
    "        print(traceback.format_exc())\n",
    "        raise\n",
    "\n",
    "\n",
    "def parse_correct_gcp_storage_bucket_location(\n",
    "    file_name: str = \"\",\n",
    "    file_type: str = \"\",\n",
    "    ship_name: str = \"\",\n",
    "    survey_name: str = \"\",\n",
    "    echosounder: str = \"\",\n",
    "    data_source: str = \"\",\n",
    "    is_metadata: bool = False,\n",
    "    is_survey_metadata: bool = False,\n",
    "    debug: bool = False,\n",
    ") -> str:\n",
    "    \"\"\"Calculates the correct gcp storage location based on data source, file\n",
    "    type, and if the file is metadata or not.\n",
    "\n",
    "    Args:\n",
    "        file_name (str, optional): The file name (includes extension).\n",
    "            Defaults to \"\".\n",
    "        file_type (str, optional): The file type (not include the dot \".\").\n",
    "            Defaults to \"\".\n",
    "        ship_name (str, optional): The ship name associated with this survey.\n",
    "            Defaults to \"\".\n",
    "        survey_name (str, optional): The survey name/identifier. Defaults\n",
    "            to \"\".\n",
    "        echosounder (str, optional): The echosounder used to gather the data.\n",
    "            Defaults to \"\".\n",
    "        data_source (str, optional): The source of the data. Can be one of\n",
    "            [\"NCEI\", \"OMAO\"]. Defaults to \"\".\n",
    "        is_metadata (bool, optional): Whether or not the file is a metadata\n",
    "            file. Necessary since files that are considered metadata (metadata\n",
    "            json, or readmes) are stored in a separate directory. Defaults to\n",
    "            False.\n",
    "        is_survey_metadata (bool, optional): Whether or not the file is a\n",
    "            metadata file associated with a survey. The files are stored at\n",
    "            the survey level, in the `metadata/` folder. Defaults to False.\n",
    "        debug (bool, optional): Whether or not to print debug statements.\n",
    "            Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        str: The correctly parsed GCP storage bucket location.\n",
    "    \"\"\"\n",
    "\n",
    "    assert (\n",
    "        (is_metadata and is_survey_metadata is False)\n",
    "        or (is_metadata is False and is_survey_metadata)\n",
    "        or (is_metadata is False and is_survey_metadata is False)\n",
    "    ), (\n",
    "        \"Please make sure that only one of `is_metadata` and\"\n",
    "        \" `is_survey_metadata` is True. Or you can set both to False.\"\n",
    "    )\n",
    "\n",
    "    # Creating the correct upload location\n",
    "    if is_survey_metadata:\n",
    "        gcp_storage_bucket_location = (\n",
    "            f\"{data_source}/{ship_name}/{survey_name}/metadata/{file_name}\"\n",
    "        )\n",
    "    elif is_metadata:\n",
    "        gcp_storage_bucket_location = (\n",
    "            f\"{data_source}/{ship_name}/{survey_name}/{echosounder}/metadata/\"\n",
    "        )\n",
    "        # Figure out if its a raw or idx file (belongs in raw folder)\n",
    "        if file_type.lower() in RAW_DATA_FILE_TYPES:\n",
    "            gcp_storage_bucket_location = (\n",
    "                gcp_storage_bucket_location + f\"raw/{file_name}.json\"\n",
    "            )\n",
    "        elif file_type.lower() in CONVERTED_DATA_FILE_TYPES:\n",
    "            gcp_storage_bucket_location = (\n",
    "                gcp_storage_bucket_location + f\"netcdf/{file_name}.json\"\n",
    "            )\n",
    "    else:\n",
    "        # Figure out if its a raw or idx file (belongs in raw folder)\n",
    "        if file_type.lower() in RAW_DATA_FILE_TYPES:\n",
    "            gcp_storage_bucket_location = (\n",
    "                f\"{data_source}/{ship_name}/\"\n",
    "                f\"{survey_name}/{echosounder}/data/raw/{file_name}\"\n",
    "            )\n",
    "        elif file_type.lower() in CONVERTED_DATA_FILE_TYPES:\n",
    "            gcp_storage_bucket_location = (\n",
    "                f\"{data_source}/{ship_name}/\"\n",
    "                f\"{survey_name}/{echosounder}/data/netcdf/{file_name}\"\n",
    "            )\n",
    "\n",
    "    if debug:\n",
    "        print(\n",
    "            \"PARSED GCP_STORAGE_BUCKET_LOCATION: %s\",\n",
    "            gcp_storage_bucket_location,\n",
    "        )\n",
    "\n",
    "    return gcp_storage_bucket_location\n",
    "\n",
    "\n",
    "def check_if_file_exists_in_gcp(\n",
    "    bucket: storage.Bucket = None, file_path: str = \"\"\n",
    ") -> bool:\n",
    "    \"\"\"Checks whether a particular file exists in GCP using the file path\n",
    "    (blob).\n",
    "\n",
    "    Args:\n",
    "        bucket (storage.Bucket, optional): The bucket object used to check for\n",
    "            the file. Defaults to None.\n",
    "        file_path (str, optional): The blob file path within the bucket.\n",
    "            Defaults to \"\".\n",
    "\n",
    "    Returns:\n",
    "        Bool: True if the file already exists, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    return bucket.blob(file_path).exists()\n",
    "\n",
    "\n",
    "def compare_local_cruise_files_to_cloud(\n",
    "    local_cruise_file_path: str = \"\",\n",
    "    ship_name: str = \"\",\n",
    "    survey_name: str = \"\",\n",
    "    echosounder: str = \"\",\n",
    "    save_to_local_path: bool = False,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"Compares the locally stored cruise files (per echosounder) to what\n",
    "    exists on the cloud by number of files, file sizes, and\n",
    "    checksums. Reports any discrepancies in the console.\n",
    "\n",
    "    Args:\n",
    "        local_cruise_file_path (str, optional): The folder path for the locally\n",
    "            stored cruise data. Defaults to \"\".\n",
    "        ship_name (str, optional): The ship name that the cruise falls under.\n",
    "            Defaults to \"\".\n",
    "        survey_name (str, optional): The survey/cruise name. Defaults to \"\".\n",
    "        echosounder (str, optional): The specific echosounder you want to\n",
    "            check. Defaults to \"\".\n",
    "        save_to_local_path (bool, optional): If True, saves the detailed\n",
    "            dataframe to this local path as an excel file. Defaults to False.\n",
    "        debug (bool, optional): Whether or not to print out debug info.\n",
    "            Defaults to False.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"COMPARING LOCAL CRUISE FILES TO CLOUD FOR {ship_name}/{survey_name}/\"\n",
    "        f\"{echosounder}\"\n",
    "    )\n",
    "\n",
    "    # Create vars for use later\n",
    "    _, s3_resource, _ = create_s3_objs()\n",
    "\n",
    "    print(\"PARSING LOCAL FILES...\")\n",
    "    # Get all local files paths in cruise directory\n",
    "    all_raw_file_paths = glob.glob(local_cruise_file_path + \"/*.raw\")\n",
    "    all_idx_file_paths = glob.glob(local_cruise_file_path + \"/*.idx\")\n",
    "    all_bot_file_paths = glob.glob(local_cruise_file_path + \"/*.bot\")\n",
    "    # Check file numbers & types\n",
    "    num_local_raw_files = len(all_raw_file_paths)\n",
    "    num_local_idx_files = len(all_idx_file_paths)\n",
    "    num_local_bot_files = len(all_bot_file_paths)\n",
    "    num_local_files = (\n",
    "        num_local_raw_files + num_local_idx_files + num_local_bot_files\n",
    "    )\n",
    "    # Get file names along with file paths\n",
    "    # [(local_file_path, file_name_with_extension), (...)]\n",
    "    all_raw_file_paths = [\n",
    "        (file_path, file_path.split(os.path.sep)[-1])\n",
    "        for file_path in all_raw_file_paths\n",
    "    ]\n",
    "    all_idx_file_paths = [\n",
    "        (file_path, file_path.split(os.path.sep)[-1])\n",
    "        for file_path in all_idx_file_paths\n",
    "    ]\n",
    "    all_bot_file_paths = [\n",
    "        (file_path, file_path.split(os.path.sep)[-1])\n",
    "        for file_path in all_bot_file_paths\n",
    "    ]\n",
    "\n",
    "    print(\"PARSING S3 FILES...\")\n",
    "    # Compare number of files in cruise, local vs cloud\n",
    "    files_in_s3 = get_all_file_names_in_a_surveys_echosounder_folder(\n",
    "        ship_name=ship_name,\n",
    "        survey_name=survey_name,\n",
    "        echosounder=echosounder,\n",
    "        s3_resource=s3_resource,\n",
    "        return_full_paths=False,\n",
    "    )\n",
    "    num_s3_raw_files = len([x for x in files_in_s3 if x.endswith(\".raw\")])\n",
    "    num_s3_idx_files = len([x for x in files_in_s3 if x.endswith(\".idx\")])\n",
    "    num_s3_bot_files = len([x for x in files_in_s3 if x.endswith(\".bot\")])\n",
    "    num_files_in_s3 = len(files_in_s3)\n",
    "\n",
    "    # Create a dataframe to keep track of all files and their statuses\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"local_file_path\",\n",
    "            \"local_file_name\",\n",
    "            \"file_type\",\n",
    "            \"s3_object_key\",\n",
    "            \"exists_in_s3\",\n",
    "            \"local_file_size\",\n",
    "            \"s3_file_size\",\n",
    "            \"file_size_match\",\n",
    "            \"local_file_checksum\",\n",
    "            \"s3_checksum\",\n",
    "            \"checksum_match\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"COMPARING FILES...\")\n",
    "    # Go through each local file, and compare file existence, size, checksum\n",
    "    for local_file_path, file_name in all_raw_file_paths:\n",
    "        file_json = {\n",
    "            \"local_file_path\": local_file_path,\n",
    "            \"local_file_name\": file_name,\n",
    "            \"file_type\": file_name.split(\".\")[-1],\n",
    "            \"s3_object_key\": None,\n",
    "            \"exists_in_s3\": None,\n",
    "            \"local_file_size\": None,\n",
    "            \"s3_file_size\": None,\n",
    "            \"file_size_match\": False,\n",
    "            \"local_file_checksum\": None,\n",
    "            \"s3_checksum\": None,\n",
    "            \"checksum_match\": False,\n",
    "        }\n",
    "        # Create s3 object key\n",
    "        s3_object_key = (\n",
    "            f\"data/raw/{ship_name}/{survey_name}/{echosounder}/{file_name}\"\n",
    "        )\n",
    "        file_json[\"s3_object_key\"] = s3_object_key\n",
    "\n",
    "        # Get existence of file in s3\n",
    "        file_exists_in_s3 = check_if_file_exists_in_s3(\n",
    "            object_key=s3_object_key,\n",
    "            s3_resource=s3_resource,\n",
    "            s3_bucket_name=\"noaa-wcsd-pds\",\n",
    "        )\n",
    "        file_json[\"exists_in_s3\"] = file_exists_in_s3\n",
    "\n",
    "        # Compare existence\n",
    "        if file_exists_in_s3:\n",
    "            # Get file size for s3 object key\n",
    "            s3_file_size = get_file_size_from_s3(\n",
    "                object_key=s3_object_key, s3_resource=s3_resource\n",
    "            )\n",
    "            # Get checksum for object key\n",
    "            s3_checksum = get_checksum_sha256_from_s3(\n",
    "                object_key=s3_object_key, s3_resource=s3_resource\n",
    "            )\n",
    "            # Get local file size\n",
    "            local_file_size = get_local_file_size(local_file_path)\n",
    "            # Get local file checksum\n",
    "            local_file_checksum = get_local_sha256_checksum(local_file_path)\n",
    "            file_json[\"s3_file_size\"] = s3_file_size\n",
    "            file_json[\"local_file_size\"] = local_file_size\n",
    "            if local_file_size == s3_file_size:\n",
    "                file_json[\"file_size_match\"] = True\n",
    "\n",
    "            file_json[\"local_file_checksum\"] = local_file_checksum\n",
    "            file_json[\"s3_checksum\"] = s3_checksum\n",
    "            if local_file_checksum == s3_checksum:\n",
    "                file_json[\"checksum_match\"] = True\n",
    "\n",
    "        file_df = pd.json_normalize(file_json)\n",
    "        df = pd.concat([df, file_df], ignore_index=True)\n",
    "    print(\"\\n\\n\\n-------------------------\")\n",
    "    print(\"EXECUTIVE SUMMARY\")\n",
    "    print(\"-------------------------\")\n",
    "    print(\"LOCAL | S3\")\n",
    "    print(f\"TOTAL FILES: {num_local_files} | {num_files_in_s3}\")\n",
    "    print(f\"TOTAL .raw FILES: {num_local_raw_files} | {num_s3_raw_files}\")\n",
    "    print(f\"TOTAL .idx FILES: {num_local_idx_files} | {num_s3_idx_files}\")\n",
    "    print(f\"TOTAL .bot FILES: {num_local_bot_files} | {num_s3_bot_files}\")\n",
    "    print(f\"FILES NOT IN S3: {len(df[df['exists_in_s3'] == False])}\")\n",
    "    print(f\"FILE SIZE MISMATCHES: {len(df[df['file_size_match'] == False])}\")\n",
    "    print(f\"CHECKSUM MISMATCHES: {len(df[df['checksum_match'] == False])}\")\n",
    "    print(\"\\n-------------------------\")\n",
    "    print(\"FINDINGS\")\n",
    "    print(\"-------------------------\")\n",
    "\n",
    "    print(\n",
    "        f\"NUMBER OF FILES BETWEEN LOCAL AND S3 {\"DO NOT\" if num_files_in_s3 != (num_local_files) else \"\"} MATCH.\"\n",
    "    )\n",
    "\n",
    "    num_files_needing_reverification = len(\n",
    "        df[(df[[\"file_size_match\", \"checksum_match\"]].any(axis=1) & (df[\"exists_in_s3\"]==True))]\n",
    "    )\n",
    "    if num_files_needing_reverification > 0:\n",
    "        print(\n",
    "            f\"NUMBER OF FILES NEEDED FOR RE-VERIFICATION/CHECK: {num_files_needing_reverification}\"\n",
    "        )\n",
    "\n",
    "    if save_to_local_path:\n",
    "        df.to_excel(save_to_local_path, index=False)\n",
    "        print(f\"Detailed dataframe saved to {save_to_local_path}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"NOTE: The detailed Excel report can be saved by setting the \"\n",
    "            \"`save_to_local_path` parameter.\"\n",
    "        )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a67b36ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- IGNORE ---\n",
    "ship_name = get_closest_ncei_formatted_ship_name(ship_name)\n",
    "local_echosounder_directory_path = os.path.normpath(local_echosounder_directory_path)\n",
    "# --- IGNORE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6aed53",
   "metadata": {},
   "source": [
    "## Step 3: Let's Get The Executive Comparison Summary Of Our Local Files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "45c30710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMPARING LOCAL CRUISE FILES TO CLOUD FOR Reuben_Lasker/RL2107/EK80\n",
      "PARSING LOCAL FILES...\n",
      "PARSING S3 FILES...\n",
      "COMPARING FILES...\n",
      "\n",
      "\n",
      "\n",
      "-------------------------\n",
      "EXECUTIVE SUMMARY\n",
      "-------------------------\n",
      "LOCAL | S3\n",
      "TOTAL FILES: 7 | 39990\n",
      "TOTAL .raw FILES: 5 | 19995\n",
      "TOTAL .idx FILES: 2 | 19995\n",
      "TOTAL .bot FILES: 0 | 0\n",
      "FILES NOT IN S3: 1\n",
      "FILE SIZE MISMATCHES: 1\n",
      "CHECKSUM MISMATCHES: 5\n",
      "\n",
      "-------------------------\n",
      "FINDINGS\n",
      "-------------------------\n",
      "NUMBER OF FILES BETWEEN LOCAL AND S3 DO NOT MATCH.\n",
      "NOTE: The detailed Excel report can be saved by setting the `save_to_local_path` parameter.\n"
     ]
    }
   ],
   "source": [
    "df = compare_local_cruise_files_to_cloud(\n",
    "    local_cruise_file_path=local_echosounder_directory_path,\n",
    "    ship_name=ship_name,\n",
    "    survey_name=cruise_name,\n",
    "    echosounder=echosounder,\n",
    "    debug=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2fb854ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_file_path</th>\n",
       "      <th>local_file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>s3_object_key</th>\n",
       "      <th>exists_in_s3</th>\n",
       "      <th>local_file_size</th>\n",
       "      <th>s3_file_size</th>\n",
       "      <th>file_size_match</th>\n",
       "      <th>local_file_checksum</th>\n",
       "      <th>s3_checksum</th>\n",
       "      <th>checksum_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_CW-D202107...</td>\n",
       "      <td>2107RL_CW-D20210712-T201130.raw</td>\n",
       "      <td>raw</td>\n",
       "      <td>data/raw/Reuben_Lasker/RL2107/EK80/2107RL_CW-D...</td>\n",
       "      <td>True</td>\n",
       "      <td>1075268856</td>\n",
       "      <td>1075268856</td>\n",
       "      <td>True</td>\n",
       "      <td>45a90595ade62e01fe96791690f15e76afdb32dae9d7b5...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_CW-D202110...</td>\n",
       "      <td>2107RL_CW-D20211012-T224001.raw</td>\n",
       "      <td>raw</td>\n",
       "      <td>data/raw/Reuben_Lasker/RL2107/EK80/2107RL_CW-D...</td>\n",
       "      <td>True</td>\n",
       "      <td>24623276</td>\n",
       "      <td>24623276</td>\n",
       "      <td>True</td>\n",
       "      <td>7d976dce9e6ea1a2c9dd079d32bde1c75f4cd596521781...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_FM-D202108...</td>\n",
       "      <td>2107RL_FM-D20210804-T113532.raw</td>\n",
       "      <td>raw</td>\n",
       "      <td>data/raw/Reuben_Lasker/RL2107/EK80/2107RL_FM-D...</td>\n",
       "      <td>True</td>\n",
       "      <td>1093757948</td>\n",
       "      <td>1093757948</td>\n",
       "      <td>True</td>\n",
       "      <td>6adc6ecceb50ea79004caecb9a0016644d36d60749ea5e...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_FM-D202108...</td>\n",
       "      <td>2107RL_FM-D20210805-T041347.raw</td>\n",
       "      <td>raw</td>\n",
       "      <td>data/raw/Reuben_Lasker/RL2107/EK80/2107RL_FM-D...</td>\n",
       "      <td>True</td>\n",
       "      <td>1093758544</td>\n",
       "      <td>1093758544</td>\n",
       "      <td>True</td>\n",
       "      <td>7246570235b60ac6197a2a1ed45b4b032398e6b71a3d85...</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>..\\Reuben_Lasker\\RL2107\\EK80\\test_new_file.raw</td>\n",
       "      <td>test_new_file.raw</td>\n",
       "      <td>raw</td>\n",
       "      <td>data/raw/Reuben_Lasker/RL2107/EK80/test_new_fi...</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     local_file_path  \\\n",
       "0  ..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_CW-D202107...   \n",
       "1  ..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_CW-D202110...   \n",
       "2  ..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_FM-D202108...   \n",
       "3  ..\\Reuben_Lasker\\RL2107\\EK80\\2107RL_FM-D202108...   \n",
       "4     ..\\Reuben_Lasker\\RL2107\\EK80\\test_new_file.raw   \n",
       "\n",
       "                   local_file_name file_type  \\\n",
       "0  2107RL_CW-D20210712-T201130.raw       raw   \n",
       "1  2107RL_CW-D20211012-T224001.raw       raw   \n",
       "2  2107RL_FM-D20210804-T113532.raw       raw   \n",
       "3  2107RL_FM-D20210805-T041347.raw       raw   \n",
       "4                test_new_file.raw       raw   \n",
       "\n",
       "                                       s3_object_key exists_in_s3  \\\n",
       "0  data/raw/Reuben_Lasker/RL2107/EK80/2107RL_CW-D...         True   \n",
       "1  data/raw/Reuben_Lasker/RL2107/EK80/2107RL_CW-D...         True   \n",
       "2  data/raw/Reuben_Lasker/RL2107/EK80/2107RL_FM-D...         True   \n",
       "3  data/raw/Reuben_Lasker/RL2107/EK80/2107RL_FM-D...         True   \n",
       "4  data/raw/Reuben_Lasker/RL2107/EK80/test_new_fi...        False   \n",
       "\n",
       "  local_file_size s3_file_size file_size_match  \\\n",
       "0      1075268856   1075268856            True   \n",
       "1        24623276     24623276            True   \n",
       "2      1093757948   1093757948            True   \n",
       "3      1093758544   1093758544            True   \n",
       "4            None         None           False   \n",
       "\n",
       "                                 local_file_checksum s3_checksum  \\\n",
       "0  45a90595ade62e01fe96791690f15e76afdb32dae9d7b5...        None   \n",
       "1  7d976dce9e6ea1a2c9dd079d32bde1c75f4cd596521781...        None   \n",
       "2  6adc6ecceb50ea79004caecb9a0016644d36d60749ea5e...        None   \n",
       "3  7246570235b60ac6197a2a1ed45b4b032398e6b71a3d85...        None   \n",
       "4                                               None        None   \n",
       "\n",
       "  checksum_match  \n",
       "0          False  \n",
       "1          False  \n",
       "2          False  \n",
       "3          False  \n",
       "4          False  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: pip install requirements.txt\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bc6c0",
   "metadata": {},
   "source": [
    "## Let's See If We Can Upload Any Of The Files To GCP\n",
    "We can only upload the files that do not exist in NCEI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0d5f821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up GCP storage objects\n",
    "gcp_stor_client, gcp_bucket_name, gcp_bucket = setup_gcp_storage_objs(\n",
    "    project_id=gcp_project_id, gcp_bucket_name=gcp_bucket_name\n",
    ")\n",
    "# Iterate through the dataframe and upload each file one-by-one.\n",
    "df = df.sort_values(by=\"local_file_size\")\n",
    "if upload_non_s3_files_to_gcp:\n",
    "    for index, row in df.iterrows():\n",
    "        if not row[\"exists_in_s3\"]:\n",
    "            local_file_path = row[\"local_file_path\"]\n",
    "            file_name = row[\"local_file_name\"]\n",
    "            file_type = row[\"file_type\"]\n",
    "\n",
    "            # Parse the correct GCP storage bucket location\n",
    "            gcp_storage_bucket_location = (\n",
    "                parse_correct_gcp_storage_bucket_location(\n",
    "                    file_name=file_name,\n",
    "                    file_type=file_type,\n",
    "                    ship_name=ship_name,\n",
    "                    survey_name=cruise_name,\n",
    "                    echosounder=echosounder,\n",
    "                    data_source=\"NCEI\",\n",
    "                    is_metadata=False,\n",
    "                    is_survey_metadata=False,\n",
    "                    debug=False,\n",
    "                )\n",
    "            )\n",
    "            if not check_if_file_exists_in_gcp(\n",
    "                bucket=gcp_bucket, file_path=gcp_storage_bucket_location\n",
    "            ):\n",
    "                upload_file_to_gcp_bucket(\n",
    "                    bucket=gcp_bucket,\n",
    "                    blob_file_path=gcp_storage_bucket_location,\n",
    "                    local_file_path=local_file_path,\n",
    "                )\n",
    "                print(\n",
    "                    f\"UPLOADED {file_name} TO {gcp_storage_bucket_location}.\"\n",
    "                )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aalibrary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
