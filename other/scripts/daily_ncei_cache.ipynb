{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f7bb6d",
   "metadata": {},
   "source": [
    "# Daily NCEI Cache\n",
    "Used to calculate and update the cache in NCEI every single day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804d4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_gbq\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from aalibrary.utils.cloud_utils import list_all_objects_in_s3_bucket_location, create_s3_objs\n",
    "from aalibrary.utils.ncei_utils import get_file_size_and_checksum_from_s3, get_file_size_from_s3, get_checksum_sha256_from_s3, get_all_ship_names_in_ncei\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e169de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = 'noaa-wcsd-pds'\n",
    "s3_client, s3_resource, s3_bucket = create_s3_objs(bucket_name=BUCKET_NAME)\n",
    "\n",
    "def get_parsed_datetime_from_filename(file_name: str):\n",
    "    \n",
    "    # Get the parsed datetime of the file.\n",
    "    datetime_regex = r\"D\\d{8}-T\\d{6}\"\n",
    "    datetime_regex_match = re.search(\n",
    "        datetime_regex, file_name\n",
    "    )\n",
    "    if datetime_regex_match:\n",
    "        # ex. 2107RL_CW-D20211001-T132449.raw\n",
    "        # TODO: `telegram` within raw file has a time stamp, maybe extract\n",
    "        temp = datetime_regex_match.group()\n",
    "        year_str = temp[1:5]\n",
    "        month_str = temp[5:7]\n",
    "        date_str = temp[7:9]\n",
    "        year = int(year_str)\n",
    "        month = int(month_str)\n",
    "        date = int(date_str)\n",
    "        hour_str = temp[11:13]\n",
    "        minute_str = temp[13:15]\n",
    "        second_str = temp[15:]\n",
    "        hour = int(hour_str)\n",
    "        minute = int(minute_str)\n",
    "        second = int(second_str)\n",
    "        try:\n",
    "            datetime_str = (\n",
    "                f\"{year_str}-{month_str}-{date_str} \"\n",
    "                f\"{hour_str}:{minute_str}:{second_str}\"\n",
    "            )\n",
    "            return datetime_str\n",
    "        except AttributeError:\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f7c4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import concurrent.futures\n",
    "import multiprocessing\n",
    "from itertools import chain\n",
    "\n",
    "def _list_objects_v2_paged(bucket, prefix, s3_client):\n",
    "    \"\"\"Helper function to list objects for a specific prefix, handling pagination.\"\"\"\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "    \n",
    "    # Extract keys from pages\n",
    "    keys = []\n",
    "    for page in pages:\n",
    "        if 'Contents' in page:\n",
    "            for obj in page['Contents']:\n",
    "                keys.append((obj['Key'], obj['LastModified'], obj['Size']))\n",
    "    return keys\n",
    "\n",
    "def list_objects_parallel(bucket_name, prefixes, s3_client):\n",
    "    \"\"\"Lists objects in parallel using multiple prefixes.\"\"\"\n",
    "    # It's recommended to create a new session/client for each thread if possible\n",
    "    # or ensure the client is thread-safe (boto3 clients are generally thread-safe).\n",
    "    # Setting max_pool_connections can help with high concurrency.\n",
    "    \n",
    "    all_keys = []\n",
    "    # Determine the optimal number of workers (e.g., based on CPU count * 10)\n",
    "    parallelism = multiprocessing.cpu_count() * 40\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=parallelism) as executor:\n",
    "        # Submit listing tasks for each prefix\n",
    "        future_to_keys = {executor.submit(_list_objects_v2_paged, bucket_name, p, s3_client): p for p in prefixes}\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(future_to_keys):\n",
    "            prefix = future_to_keys[future]\n",
    "            try:\n",
    "                keys = future.result()\n",
    "                all_keys.extend(keys)\n",
    "            except Exception as exc:\n",
    "                print(f'{prefix} generated an exception: {exc}')\n",
    "                \n",
    "    return all_keys\n",
    "\n",
    "# Get all ship names in NCEI\n",
    "all_ncei_ship_paths = get_all_ship_names_in_ncei(return_full_paths=True)\n",
    "# all_ncei_ship_paths = ['data/raw/Reuben_Lasker/']\n",
    "\n",
    "objects = list_objects_parallel(BUCKET_NAME, all_ncei_ship_paths, s3_client)\n",
    "print(f\"Found {len(objects)} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e684c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(objects, columns=[\"s3_object_key\", \"last_modified_in_ncei\", \"size_bytes\"])\n",
    "df[\"file_name\"] = df[\"s3_object_key\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "df[\"file_type\"] = df[\"s3_object_key\"].apply(lambda x: x.split(\".\")[-1])\n",
    "df[\"file_datetime\"] = df[\"file_name\"].apply(get_parsed_datetime_from_filename)\n",
    "df[\"date_modified\"] = datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "df[\"date_modified\"] = pd.to_datetime(df[\"date_modified\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df.to_csv(\"ncei_daily_file_cache.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"checksum\"] = df[\"s3_object_key\"].apply(get_checksum_sha256_from_s3, s3_resource=s3_resource)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6149142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# from itertools import repeat\n",
    "\n",
    "# with mp.Pool(mp.cpu_count()) as pool:\n",
    "#     df['file_size_bytes'] = pool.map(get_file_size_and_checksum_from_s3, df['s3_object_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c1a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import multiprocessing as mp\n",
    "# import pandas as pd\n",
    "# from joblib import Parallel, delayed\n",
    "\n",
    "# df = pd.read_csv(\"./ncei_daily_file_cache.csv\")\n",
    "\n",
    "# sample_df = df[:10]\n",
    "# def run_joblib():\n",
    "#     results = Parallel(n_jobs=mp.cpu_count())(\n",
    "#             delayed(get_file_size_and_checksum_from_s3)(i) for i in sample_df['s3_object_key']\n",
    "#             )\n",
    "#     sample_df['file_size_bytes'] = results\n",
    "\n",
    "# run_joblib()\n",
    "# sample_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6087eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_gbq.to_gbq(df, destination_table=\"metadata.ncei_cache\", project_id=\"ggn-nmfs-aa-dev-1\", if_exists=\"replace\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aalibrary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
