{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"about/external_resources/","title":"External Resources","text":""},{"location":"about/external_resources/#other-strategic-initiatives","title":"Other Strategic Initiatives","text":"<p>NOAA has other strategic initiatives that are being implemented concurrently with Active Acoustics. You can find links to them here.</p> <p>Passive Acoustics Monitoring Strategic Initiative - NOAA Fisheries\u2019 PAM programs collect underwater sound recordings of the marine environment.</p>"},{"location":"documentation/aalibrary/","title":"Documentation for <code>aalibrary</code>","text":"<p>Modules:</p> Name Description <code>config</code> <p>Used for storing environment-specific settings such as database URIs and</p> <code>ices_ship_names</code> <p>This file contains the code to parse through the ICES API found here:</p> <code>ingestion</code> <p>This script contains functions used to ingest Active Acoustics data into GCP</p> <code>conversion</code> <p>This file is used to store conversion functions for the AALibrary.</p> <code>metadata</code> <p>This file contains functions that have to do with metadata.</p> <code>queries</code> <p>This script contains classes that have SQL queries used for interaction</p>"},{"location":"documentation/aalibrary/#aalibrary.config","title":"<code>config</code>","text":"<p>Used for storing environment-specific settings such as database URIs and such.</p>"},{"location":"documentation/aalibrary/#aalibrary.ices_ship_names","title":"<code>ices_ship_names</code>","text":"<p>This file contains the code to parse through the ICES API found here: https://vocab.ices.dk/?ref=315 Specifically the <code>SHIPC</code> platform code which refers to ship names.</p> <p>Functions:</p> Name Description <code>get_all_ices_ship_codes_and_names</code> <p>Gets all of the ices ship codes and their corresponding names in a</p> <code>get_all_ices_ship_names</code> <p>Gets all of the ICES ship names. You can normalize them to our standards</p> <code>get_all_ship_info</code> <p>Gets all of the ship's info from the following URL:</p> <code>get_ices_code_from_ship_name</code> <p>Gets the ICES Code for a ship given a ship's name.</p>"},{"location":"documentation/aalibrary/#aalibrary.ices_ship_names.get_all_ices_ship_codes_and_names","title":"<code>get_all_ices_ship_codes_and_names(normalize_ship_names=False)</code>","text":"<p>Gets all of the ices ship codes and their corresponding names in a dictionary format. The keys are the ICES code, and the name is the value.</p> <p>Parameters:</p> Name Type Description Default <code>normalize_ship_names</code> <code>bool</code> <p>Whether or not to format the ship name according to our own standards. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dict with all of the ICES ships. The keys are the ICES code, and the name is the value.</p> Source code in <code>src\\aalibrary\\ices_ship_names.py</code> <pre><code>def get_all_ices_ship_codes_and_names(\n    normalize_ship_names: bool = False,\n) -&gt; dict:\n    \"\"\"Gets all of the ices ship codes and their corresponding names in a\n    dictionary format. The keys are the ICES code, and the name is the value.\n\n    Args:\n        normalize_ship_names (bool, optional): Whether or not to format the\n            ship name according to our own standards. Defaults to False.\n\n    Returns:\n        dict: A dict with all of the ICES ships. The keys are the ICES code,\n            and the name is the value.\n    \"\"\"\n\n    all_ship_info = get_all_ship_info()\n    all_ship_codes_and_names = {}\n    for ship_info in all_ship_info:\n        all_ship_codes_and_names[ship_info[\"key\"]] = ship_info[\"description\"]\n\n    if normalize_ship_names:\n        all_ship_codes_and_names = {\n            code: normalize_ship_name(name)\n            for code, name in all_ship_codes_and_names.items()\n        }\n\n    return all_ship_codes_and_names\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ices_ship_names.get_all_ices_ship_names","title":"<code>get_all_ices_ship_names(normalize_ship_names=False)</code>","text":"<p>Gets all of the ICES ship names. You can normalize them to our standards if you wish.</p> <p>Parameters:</p> Name Type Description Default <code>normalize_ship_names</code> <code>bool</code> <p>Whether or not to format the ship name according to our own standards. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A list containing strings of all of the ship names.</p> Source code in <code>src\\aalibrary\\ices_ship_names.py</code> <pre><code>def get_all_ices_ship_names(normalize_ship_names: bool = False) -&gt; List:\n    \"\"\"Gets all of the ICES ship names. You can normalize them to our standards\n    if you wish.\n\n    Args:\n        normalize_ship_names (bool, optional): Whether or not to format the\n            ship name according to our own standards. Defaults to False.\n\n    Returns:\n        List: A list containing strings of all of the ship names.\n    \"\"\"\n\n    all_ship_info = get_all_ship_info()\n    all_ship_names = []\n    for ship_info in all_ship_info:\n        # Here `ship_info` is a dict\n        all_ship_names.append(ship_info[\"description\"])\n    if normalize_ship_names:\n        all_ship_names = [\n            normalize_ship_name(ship_name=ship_name)\n            for ship_name in all_ship_names\n        ]\n\n    return all_ship_names\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ices_ship_names.get_all_ship_info","title":"<code>get_all_ship_info()</code>","text":"<p>Gets all of the ship's info from the following URL: https:/vocab.ices.dk/services/api/Code/7f9a91e1-fb57-464a-8eb0-697e4b0235b5</p> <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A list with dicts of all the ships, including name, ices code, uuids and other fields.</p> Source code in <code>src\\aalibrary\\ices_ship_names.py</code> <pre><code>def get_all_ship_info() -&gt; List:\n    \"\"\"Gets all of the ship's info from the following URL:\n    https:/vocab.ices.dk/services/api/Code/7f9a91e1-fb57-464a-8eb0-697e4b0235b5\n\n\n    Returns:\n        List: A list with dicts of all the ships, including name, ices code,\n            uuids and other fields.\n    \"\"\"\n\n    response = requests.get(\n        url=(\n            \"https://vocab.ices.dk/services/api/Code/\"\n            \"7f9a91e1-fb57-464a-8eb0-697e4b0235b5\"\n        ),\n        timeout=10\n    )\n    all_ship_info = response.json()\n\n    return all_ship_info\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ices_ship_names.get_ices_code_from_ship_name","title":"<code>get_ices_code_from_ship_name(ship_name='', is_normalized=False)</code>","text":"<p>Gets the ICES Code for a ship given a ship's name.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship name string. Defaults to \"\".</p> <code>''</code> <code>is_normalized</code> <code>bool</code> <p>Whether or not the ship name is already normalized according to aalibrary standards. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The ICES Code if one has been found. Empty string if it has not.</p> Source code in <code>src\\aalibrary\\ices_ship_names.py</code> <pre><code>def get_ices_code_from_ship_name(\n    ship_name: str = \"\", is_normalized: bool = False\n) -&gt; str:\n    \"\"\"Gets the ICES Code for a ship given a ship's name.\n\n    Args:\n        ship_name (str, optional): The ship name string. Defaults to \"\".\n        is_normalized (bool, optional): Whether or not the ship name is already\n            normalized according to aalibrary standards. Defaults to False.\n\n    Returns:\n        str: The ICES Code if one has been found. Empty string if it has not.\n    \"\"\"\n\n    # Get all of the ship codes and names.\n    all_codes_and_names = get_all_ices_ship_codes_and_names(\n        normalize_ship_names=is_normalized\n    )\n    # Reverse it to make the ship names the keys.\n    all_codes_and_names = {v: k for k, v in all_codes_and_names.items()}\n    valid_ices_ship_names = list(all_codes_and_names.keys())\n    # Try to find the correct ICES code based on the ship name.\n    try:\n        return all_codes_and_names[ship_name]\n    except KeyError:\n        # Here the ship name does not exactly match any in the ICES DB.\n        # Check for spell check using custom list\n        spell_check_list = get_close_matches(\n            ship_name, valid_ices_ship_names, n=3, cutoff=0.6\n        )\n        if len(spell_check_list) &gt; 0:\n            print(\n                f\"This `ship_name` {ship_name} does not\"\n                \" exist in the ICES database. Did you mean one of the\"\n                f\" following?\\n{spell_check_list}\"\n            )\n        else:\n            print(\n                f\"This `ship_name` {ship_name} does not\"\n                \" exist in the ICES database. A close match could not be \"\n                \"found.\"\n            )\n        return \"\"\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion","title":"<code>ingestion</code>","text":"<p>This script contains functions used to ingest Active Acoustics data into GCP from various sources such as AWS buckets and Azure Data Lake.</p> <p>Functions:</p> Name Description <code>download_file_from_azure_directory</code> <p>Downloads a single file from an azure directory using the</p> <code>download_netcdf_file</code> <p>ENTRYPOINT FOR END-USERS</p> <code>download_raw_file</code> <p>ENTRYPOINT FOR END-USERS</p> <code>download_raw_file_from_azure</code> <p>ENTRYPOINT FOR END-USERS</p> <code>download_raw_file_from_ncei</code> <p>ENTRYPOINT FOR END-USERS</p> <code>download_specific_file_from_azure</code> <p>Creates a DataLakeFileClient and downloads a specific file from</p> <code>download_survey_from_ncei</code> <p>Downloads an entire survey from NCEI to a local directory while</p> <code>find_and_upload_survey_metadata_from_s3</code> <p>Finds the metadata that is associated with a particular survey in s3,</p> <code>find_data_source_for_file</code> <p>Finds the data source of a given filename by checking all possible data</p>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.download_file_from_azure_directory","title":"<code>download_file_from_azure_directory(directory_client, file_system='testcontainer', download_directory='./', file_path='')</code>","text":"<p>Downloads a single file from an azure directory using the DataLakeDirectoryClient. Useful for numerous operations, as authentication is only required once for the creation of each DataLakeDirectoryClient.</p> <p>Parameters:</p> Name Type Description Default <code>directory_client</code> <code>DataLakeDirectoryClient</code> <p>The DataLakeDirectoryClient that will be used to connect to a download from an azure file system in the data lake.</p> required <code>file_system</code> <code>str</code> <p>The file system (container) you wish to download your file from. Defaults to \"testcontainer\" for testing purposes.</p> <code>'testcontainer'</code> <code>download_directory</code> <code>str</code> <p>The local directory you want to download to. Defaults to \"./\".</p> <code>'./'</code> <code>file_path</code> <code>str</code> <p>The file path you want to download.</p> <code>''</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def download_file_from_azure_directory(\n    directory_client: DataLakeDirectoryClient,\n    file_system: str = \"testcontainer\",\n    download_directory: str = \"./\",\n    file_path: str = \"\",\n):\n    \"\"\"Downloads a single file from an azure directory using the\n    DataLakeDirectoryClient. Useful for numerous operations, as authentication\n    is only required once for the creation of each DataLakeDirectoryClient.\n\n    Args:\n        directory_client (DataLakeDirectoryClient): The\n            DataLakeDirectoryClient that will be used to connect to a\n            download from an azure file system in the data lake.\n        file_system (str): The file system (container) you wish to download\n            your file from. Defaults to \"testcontainer\" for testing purposes.\n        download_directory (str): The local directory you want to download to.\n            Defaults to \"./\".\n        file_path (str): The file path you want to download.\n    \"\"\"\n\n    # User-error-checking\n    check_for_assertion_errors(\n        data_lake_directory_client=directory_client,\n        file_download_directory=download_directory,\n    )\n\n    file_client = directory_client.get_file_client(\n        file_path=file_path, file_system=file_system\n    )\n\n    download_directory = os.path.normpath(download_directory)\n    file_name = os.path.normpath(file_path).split(os.path.sep)[-1]\n\n    with open(\n        file=os.sep.join([download_directory, file_name]), mode=\"wb\"\n    ) as local_file:\n        download = file_client.download_file()\n        local_file.write(download.readall())\n        local_file.close()\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.download_netcdf_file","title":"<code>download_netcdf_file(raw_file_name='', file_type='netcdf', ship_name='', survey_name='', echosounder='', data_source='', file_download_directory='', gcp_bucket=None, debug=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS Downloads a netcdf file from GCP storage bucket for use on your workstation. Works as follows:     1. Checks if the exact netcdf exists in gcp.         a. If it doesn't exists, prompts user to download it first.         b. If it exists, downloads to the <code>file_download_directory</code>.</p> <p>Parameters:</p> Name Type Description Default <code>raw_file_name</code> <code>str</code> <p>The raw file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (do not include the dot \".\"). Defaults to \"netcdf\".</p> <code>'netcdf'</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the file. Necessary due to the way the storage bucket is organized. Can be one of [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".</p> <code>''</code> <code>file_download_directory</code> <code>str</code> <p>The local directory you want to store your file in. Defaults to \"\".</p> <code>''</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP bucket object used to download the file. Defaults to None.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def download_netcdf_file(\n    raw_file_name: str = \"\",\n    file_type: str = \"netcdf\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"\",\n    file_download_directory: str = \"\",\n    gcp_bucket: storage.Client.bucket = None,\n    debug: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n    Downloads a netcdf file from GCP storage bucket for use on your\n    workstation.\n    Works as follows:\n        1. Checks if the exact netcdf exists in gcp.\n            a. If it doesn't exists, prompts user to download it first.\n            b. If it exists, downloads to the `file_download_directory`.\n\n    Args:\n        raw_file_name (str, optional): The raw file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (do not include the dot \".\").\n            Defaults to \"netcdf\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier.\n            Defaults to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the file. Necessary due to\n            the way the storage bucket is organized. Can be one of\n            [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".\n        file_download_directory (str, optional): The local directory you want\n            to store your file in. Defaults to \"\".\n        gcp_bucket (storage.Client.bucket, optional): The GCP bucket object\n            used to download the file. Defaults to None.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n\n    _, s3_resource, _ = utils.cloud_utils.create_s3_objs()\n\n    rf = RawFile(\n        file_name=raw_file_name,\n        file_type=file_type,\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        data_source=data_source,\n        file_download_directory=file_download_directory,\n        gcp_bucket=gcp_bucket,\n        debug=debug,\n        s3_resource=s3_resource,\n    )\n\n    if rf.netcdf_file_exists_in_gcp:\n        print(\n            (\n                f\"NETCDF FILE LOCATED IN GCP\"\n                f\": `{rf.netcdf_gcp_storage_bucket_location}`\\nDOWNLOADING...\"\n            )\n        )\n        utils.cloud_utils.download_file_from_gcp(\n            gcp_bucket=gcp_bucket,\n            blob_file_path=rf.netcdf_gcp_storage_bucket_location,\n            local_file_path=rf.netcdf_file_download_path,\n            debug=debug,\n        )\n        print(\n            f\"FILE `{raw_file_name}` DOWNLOADED \"\n            f\"TO `{rf.netcdf_file_download_path}`\"\n        )\n        return\n    else:\n        logging.error(\n            \"NETCDF FILE `%s` DOES NOT EXIST IN GCP AT THE LOCATION: `%s`.\",\n            raw_file_name,\n            rf.netcdf_gcp_storage_bucket_location,\n        )\n        logging.error(\n            \"PLEASE CONVERT AND UPLOAD THE RAW FILE FIRST VIA\"\n            \" `download_raw_file`.\"\n        )\n        raise FileNotFoundError\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.download_raw_file","title":"<code>download_raw_file(file_name='', file_type='raw', ship_name='', survey_name='', echosounder='', data_source='', file_download_directory='.', gcp_bucket=None, debug=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS Downloads a raw and idx file from NCEI for use on your workstation. Works as follows:     1. Checks if raw file exists in GCP.         a. If it exists,             checks if a netcdf version also exists or not             lets the user know.             i. If <code>force_download_from_ncei</code> is True                 downloads the raw and idx file from NCEI instead.         b. If it doesn't exist,             downloads .raw from NCEI and uploads to GCP for caching             downloads .idx from NCEI and uploads to GCP for caching</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (do not include the dot \".\"). Defaults to \"\".</p> <code>'raw'</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the file. Necessary due to the way the storage bucket is organized. Can be one of [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".</p> <code>''</code> <code>file_download_directory</code> <code>str</code> <p>The local file directory you want to store your file in. Defaults to current directory. Defaults to \".\".</p> <code>'.'</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP bucket object used to download the file. Defaults to None.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def download_raw_file(\n    file_name: str = \"\",\n    file_type: str = \"raw\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"\",\n    file_download_directory: str = \".\",\n    gcp_bucket: storage.Bucket = None,\n    debug: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n    Downloads a raw and idx file from NCEI for use on your workstation.\n    Works as follows:\n        1. Checks if raw file exists in GCP.\n            a. If it exists,\n                checks if a netcdf version also exists or not\n                lets the user know.\n                i. If `force_download_from_ncei` is True\n                    downloads the raw and idx file from NCEI instead.\n            b. If it doesn't exist,\n                downloads .raw from NCEI and uploads to GCP for caching\n                downloads .idx from NCEI and uploads to GCP for caching\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (do not include the dot \".\").\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier. Defaults\n            to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the file. Necessary due to\n            the way the storage bucket is organized. Can be one of\n            [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".\n        file_download_directory (str, optional): The local file directory you\n            want to store your file in. Defaults to current directory.\n            Defaults to \".\".\n        gcp_bucket (storage.Client.bucket, optional): The GCP bucket object\n            used to download the file. Defaults to None.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n\n    if gcp_bucket is None:\n        _, _, gcp_bucket = utils.cloud_utils.setup_gcp_storage_objs()\n    _, s3_resource, _ = utils.cloud_utils.create_s3_objs()\n\n    rf = RawFile(\n        file_name=file_name,\n        file_type=file_type,\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        data_source=data_source,\n        file_download_directory=file_download_directory,\n        debug=debug,\n        gcp_bucket=gcp_bucket,\n        s3_resource=s3_resource,\n    )\n\n    if rf.raw_file_exists_in_gcp:\n        # Inform user if file exists in GCP.\n        print(\n            f\"FILE `{rf.raw_file_name}` ALREADY EXISTS IN\"\n            \" GOOGLE STORAGE BUCKET.\"\n        )\n        # Here we download the raw file from GCP. We also check for a netcdf\n        # version and let the user know.\n        print(\"CHECKING FOR NETCDF VERSION...\")\n        if rf.netcdf_file_exists_in_gcp:\n            # Inform the user if a netcdf version exists in cache.\n            print(\n                (\n                    f\"FILE `{rf.raw_file_name}` EXISTS AS A NETCDF ALREADY.\"\n                    \" PLEASE DOWNLOAD THE NETCDF VERSION IF NEEDED.\"\n                )\n            )\n        else:\n            print(\n                (\n                    f\"FILE `{rf.raw_file_name}` DOES NOT EXIST AS NETCDF.\"\n                    \" CONSIDER RUNNING A CONVERSION FUNCTION\"\n                )\n            )\n\n        # Here we download the raw from GCP.\n        print(\n            (\n                f\"DOWNLOADING FILE `{rf.raw_file_name}` FROM GCP TO\"\n                f\" `{rf.raw_file_download_path}`\"\n            )\n        )\n        utils.cloud_utils.download_file_from_gcp(\n            gcp_bucket=rf.gcp_bucket,\n            blob_file_path=rf.raw_gcp_storage_bucket_location,\n            local_file_path=rf.raw_file_download_path,\n            debug=rf.debug,\n        )\n        print(\"DOWNLOADED.\")\n\n    elif rf.raw_file_exists_in_ncei and (\n        not rf.raw_file_exists_in_gcp\n    ):  # File does not exist in gcp and needs to be downloaded from NCEI\n        download_raw_file_from_ncei(\n            file_name=rf.raw_file_name,\n            file_type=\"raw\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            data_source=rf.data_source,\n            file_download_directory=rf.file_download_directory,\n            upload_to_gcp=True,\n            debug=rf.debug,\n        )\n\n    # Checking to make sure the idx exists in GCP...\n    if rf.idx_file_exists_in_gcp:\n        print(\"CORRESPONDING IDX FILE FOUND IN GCP. DOWNLOADING...\")\n        # Here we download the idx from GCP.\n        print(\n            (\n                f\"DOWNLOADING FILE `{rf.idx_file_name}` FROM GCP TO \"\n                f\"`{rf.idx_file_download_path}`\"\n            )\n        )\n        utils.cloud_utils.download_file_from_gcp(\n            gcp_bucket=rf.gcp_bucket,\n            blob_file_path=rf.idx_gcp_storage_bucket_location,\n            local_file_path=rf.idx_file_download_path,\n            debug=rf.debug,\n        )\n        print(\"DOWNLOADED.\")\n    elif rf.idx_file_exists_in_ncei and (not rf.idx_file_exists_in_gcp):\n        print(\n            (\n                \"CORRESPONDING IDX FILE NOT FOUND IN GCP.\"\n                \" DOWNLOADING FROM NCEI AND UPLOADING TO GCP...\"\n            )\n        )\n        # Safely download and upload the idx file.\n        download_single_file_from_aws(\n            file_url=rf.idx_file_ncei_url,\n            download_location=rf.idx_file_download_path,\n        )\n        # Upload to GCP at the correct storage bucket location.\n        upload_file_to_gcp_storage_bucket(\n            file_name=rf.idx_file_name,\n            file_type=\"idx\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            file_location=rf.idx_file_download_path,\n            gcp_bucket=rf.gcp_bucket,\n            data_source=rf.data_source,\n            debug=rf.debug,\n        )\n\n    # Checking to make sure the bot exists in GCP...\n    if rf.bot_file_exists_in_gcp:\n        print(\"CORRESPONDING BOT FILE FOUND IN GCP. DOWNLOADING...\")\n        # Here we download the bot from GCP.\n        print(\n            (\n                f\"DOWNLOADING FILE `{rf.bot_file_name}` FROM GCP\"\n                f\" TO `{rf.bot_file_download_path}`\"\n            )\n        )\n        utils.cloud_utils.download_file_from_gcp(\n            gcp_bucket=rf.gcp_bucket,\n            blob_file_path=rf.bot_gcp_storage_bucket_location,\n            local_file_path=rf.bot_file_download_path,\n            debug=rf.debug,\n        )\n        print(\"DOWNLOADED.\")\n    elif rf.bot_file_exists_in_ncei and (not rf.bot_file_exists_in_gcp):\n        print(\n            (\n                \"CORRESPONDING BOT FILE NOT FOUND IN GCP. TRYING TO \"\n                \"DOWNLOAD FROM NCEI AND UPLOADING TO GCP...\"\n            )\n        )\n        # Safely download and upload the bot file.\n        download_single_file_from_aws(\n            file_url=rf.bot_file_ncei_url,\n            download_location=rf.bot_file_download_path,\n        )\n        # Upload to GCP at the correct storage bucket location.\n        upload_file_to_gcp_storage_bucket(\n            file_name=rf.bot_file_name,\n            file_type=\"bot\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            file_location=rf.bot_file_download_path,\n            gcp_bucket=rf.gcp_bucket,\n            data_source=rf.data_source,\n            debug=rf.debug,\n        )\n\n    return\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.download_raw_file_from_azure","title":"<code>download_raw_file_from_azure(file_name='', file_type='raw', ship_name='', survey_name='', echosounder='', data_source='OMAO', file_download_directory='.', config_file_path='', upload_to_gcp=False, debug=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (do not include the dot \".\"). Defaults to \"\".</p> <code>'raw'</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the file. Necessary due to the way the storage bucket is organized. Can be one of [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".</p> <code>'OMAO'</code> <code>file_download_directory</code> <code>str</code> <p>The local directory you want to store your file in. Defaults to current directory. Defaults to \".\".</p> <code>'.'</code> <code>config_file_path</code> <code>str</code> <p>The location of the config file. Needs a <code>[DEFAULT]</code> section with a <code>azure_connection_string</code> variable defined. Defaults to \"\".</p> <code>''</code> <code>upload_to_gcp</code> <code>bool</code> <p>Whether or not you want to upload to GCP. Defaults to False.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def download_raw_file_from_azure(\n    file_name: str = \"\",\n    file_type: str = \"raw\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"OMAO\",\n    file_download_directory: str = \".\",\n    config_file_path: str = \"\",\n    upload_to_gcp: bool = False,\n    debug: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (do not include the dot \".\").\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier.\n            Defaults to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the file. Necessary due to\n            the way the storage bucket is organized. Can be one of\n            [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".\n        file_download_directory (str, optional): The local directory you want\n            to store your file in. Defaults to current directory. Defaults\n            to \".\".\n        config_file_path (str, optional): The location of the config file.\n            Needs a `[DEFAULT]` section with a `azure_connection_string`\n            variable defined. Defaults to \"\".\n        upload_to_gcp (bool, optional): Whether or not you want to upload to\n            GCP. Defaults to False.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n    # Create gcp bucket objects\n    _, _, gcp_bucket = utils.cloud_utils.setup_gcp_storage_objs()\n    try:\n        _, s3_resource, _ = utils.cloud_utils.create_s3_objs()\n    except Exception as e:\n        logging.error(\"CANNOT ESTABLISH CONNECTION TO S3 BUCKET..\\n %s\", e)\n        raise\n\n    rf = RawFile(\n        file_name=file_name,\n        file_type=file_type,\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        data_source=data_source,\n        file_download_directory=file_download_directory,\n        is_metadata=False,\n        upload_to_gcp=upload_to_gcp,\n        debug=debug,\n        gcp_bucket=gcp_bucket,\n        s3_resource=s3_resource,\n    )\n\n    # Location of temporary file in sandbox environment.\n    # https://contracttest4.blob.core.windows.net/testcontainer/Reuben_Lasker/RL_1601/EK_60/1601RL-D20160107-T074016.bot\n\n    # Create Azure Directory Client\n    azure_datalake_directory_client = get_data_lake_directory_client(\n        config_file_path=config_file_path\n    )\n\n    # TODO: check to see if you want to download from gcp instead.\n\n    # TODO: add if statement to check if the file exists in azure or not.\n    print(f\"DOWNLOADING FILE {rf.raw_file_name} FROM OMAO\")\n    download_file_from_azure_directory(\n        directory_client=azure_datalake_directory_client,\n        download_directory=rf.file_download_directory,\n        file_path=rf.raw_omao_file_path,\n    )\n\n    # Force download the idx file.\n    print(f\"DOWNLOADING IDX FILE {rf.idx_file_name} FROM OMAO\")\n    download_file_from_azure_directory(\n        directory_client=azure_datalake_directory_client,\n        download_directory=rf.file_download_directory,\n        file_path=rf.idx_omao_file_path,\n    )\n\n    # Force download the bot file.\n    print(f\"DOWNLOADING BOT FILE {rf.bot_file_name} FROM OMAO\")\n    download_file_from_azure_directory(\n        directory_client=azure_datalake_directory_client,\n        download_directory=rf.file_download_directory,\n        file_path=rf.bot_omao_file_path,\n    )\n\n    if upload_to_gcp:\n        if rf.raw_file_exists_in_gcp:\n            print(\n                (\n                    \"RAW FILE ALREADY EXISTS IN GCP AT \"\n                    f\"`{rf.raw_gcp_storage_bucket_location}`\"\n                )\n            )\n        else:\n            # TODO: try out a background process if possible -- file might\n            # have a lock. only async options, otherwise subprocess gsutil to\n            # upload it.\n            # Upload raw to GCP at the correct storage bucket location.\n            upload_file_to_gcp_storage_bucket(\n                file_name=file_name,\n                file_type=file_type,\n                ship_name=ship_name,\n                survey_name=survey_name,\n                echosounder=echosounder,\n                file_location=rf.raw_file_download_path,\n                gcp_bucket=gcp_bucket,\n                data_source=data_source,\n                debug=debug,\n            )\n            # Upload the metadata file as well.\n            metadata.create_and_upload_metadata_df(\n                rf=rf,\n                debug=debug,\n            )\n\n        if rf.idx_file_exists_in_gcp:\n            print(\n                (\n                    \"IDX FILE ALREADY EXISTS IN GCP AT \"\n                    f\"`{rf.idx_gcp_storage_bucket_location}`\"\n                )\n            )\n        else:\n            # Upload idx to GCP at the correct storage bucket location.\n            upload_file_to_gcp_storage_bucket(\n                file_name=rf.idx_file_name,\n                file_type=file_type,\n                ship_name=ship_name,\n                survey_name=survey_name,\n                echosounder=echosounder,\n                file_location=rf.idx_file_download_path,\n                gcp_bucket=gcp_bucket,\n                data_source=data_source,\n                debug=debug,\n            )\n\n        if rf.bot_file_exists_in_gcp:\n            print(\n                (\n                    \"BOT FILE ALREADY EXISTS IN GCP AT\"\n                    f\" `{rf.bot_gcp_storage_bucket_location}`\"\n                )\n            )\n        else:\n            # Upload bot to GCP at the correct storage bucket location.\n            upload_file_to_gcp_storage_bucket(\n                file_name=rf.bot_file_name,\n                file_type=file_type,\n                ship_name=ship_name,\n                survey_name=survey_name,\n                echosounder=echosounder,\n                file_location=rf.bot_file_download_path,\n                gcp_bucket=gcp_bucket,\n                data_source=data_source,\n                debug=debug,\n            )\n\n        return\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.download_raw_file_from_ncei","title":"<code>download_raw_file_from_ncei(file_name='', file_type='raw', ship_name='', survey_name='', echosounder='', data_source='NCEI', file_download_directory='.', upload_to_gcp=False, debug=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS Downloads a raw, idx, and bot file from NCEI. If <code>upload_to_gcp</code> is enabled, the downloaded files will also upload to the GCP storage bucket if they do not exist.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (do not include the dot \".\"). Defaults to \"\".</p> <code>'raw'</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the file. Necessary due to the way the storage bucket is organized. Can be one of [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".</p> <code>'NCEI'</code> <code>file_download_directory</code> <code>str</code> <p>The local file directory you want to store your file in. Defaults to current directory. Defaults to \".\".</p> <code>'.'</code> <code>upload_to_gcp</code> <code>bool</code> <p>Whether or not you want to upload to GCP. Defaults to False.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def download_raw_file_from_ncei(\n    file_name: str = \"\",\n    file_type: str = \"raw\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"NCEI\",\n    file_download_directory: str = \".\",\n    upload_to_gcp: bool = False,\n    debug: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n    Downloads a raw, idx, and bot file from NCEI. If `upload_to_gcp` is\n    enabled, the downloaded files will also upload to the GCP storage bucket\n    if they do not exist.\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (do not include the dot \".\").\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier.\n            Defaults to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the file. Necessary due to\n            the way the storage bucket is organized. Can be one of\n            [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".\n        file_download_directory (str, optional): The local file directory you\n            want to store your file in. Defaults to current directory.\n            Defaults to \".\".\n        upload_to_gcp (bool, optional): Whether or not you want to upload to\n            GCP. Defaults to False.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n    _, _, gcp_bucket = utils.cloud_utils.setup_gcp_storage_objs()\n    try:\n        _, s3_resource, _ = utils.cloud_utils.create_s3_objs()\n    except Exception as e:\n        logging.error(\"CANNOT ESTABLISH CONNECTION TO S3 BUCKET..\\n%s\", e)\n        raise\n\n    rf = RawFile(\n        file_name=file_name,\n        file_type=file_type,\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        data_source=data_source,\n        file_download_directory=file_download_directory,\n        upload_to_gcp=upload_to_gcp,\n        debug=debug,\n        gcp_bucket=gcp_bucket,\n        s3_resource=s3_resource,\n    )\n\n    if rf.raw_file_exists_in_ncei:\n        download_single_file_from_aws(\n            file_url=rf.raw_file_ncei_url,\n            download_location=rf.raw_file_download_path,\n        )\n    if rf.idx_file_exists_in_ncei:\n        # Force download the idx file.\n        download_single_file_from_aws(\n            file_url=rf.idx_file_ncei_url,\n            download_location=rf.idx_file_download_path,\n        )\n    if rf.bot_file_exists_in_ncei:\n        # Force download the bot file.\n        download_single_file_from_aws(\n            file_url=rf.bot_file_ncei_url,\n            download_location=rf.bot_file_download_path,\n        )\n\n    if upload_to_gcp:\n        if rf.raw_file_exists_in_gcp:\n            print(\n                (\n                    \"RAW FILE ALREADY EXISTS IN GCP AT \"\n                    f\"`{rf.raw_gcp_storage_bucket_location}`\"\n                )\n            )\n        else:\n            # TODO: try out a background process if possible -- file might\n            # have a lock. only async options, otherwise subprocess gsutil to\n            # upload it.\n\n            # Upload raw to GCP at the correct storage bucket location.\n            upload_file_to_gcp_storage_bucket(\n                file_name=rf.file_name,\n                file_type=\"raw\",\n                ship_name=rf.ship_name,\n                survey_name=rf.survey_name,\n                echosounder=rf.echosounder,\n                file_location=rf.raw_file_download_path,\n                gcp_bucket=rf.gcp_bucket,\n                data_source=rf.data_source,\n                debug=rf.debug,\n            )\n            # Upload the metadata file as well.\n            metadata.create_and_upload_metadata_df(\n                rf=rf,\n                debug=rf.debug,\n            )\n\n        if rf.idx_file_exists_in_gcp:\n            print(\n                (\n                    \"IDX FILE ALREADY EXISTS IN GCP AT \"\n                    f\"`{rf.idx_gcp_storage_bucket_location}`\"\n                )\n            )\n        elif rf.idx_file_exists_in_ncei and (not rf.idx_file_exists_in_gcp):\n            # Upload idx to GCP at the correct storage bucket location.\n            upload_file_to_gcp_storage_bucket(\n                file_name=rf.idx_file_name,\n                file_type=\"idx\",\n                ship_name=rf.ship_name,\n                survey_name=rf.survey_name,\n                echosounder=echosounder,\n                file_location=rf.idx_file_download_path,\n                gcp_bucket=rf.gcp_bucket,\n                data_source=rf.data_source,\n                is_metadata=False,\n                debug=rf.debug,\n            )\n\n        if rf.bot_file_exists_in_gcp:\n            print(\n                (\n                    \"BOT FILE ALREADY EXISTS IN GCP AT \"\n                    f\"`{rf.bot_gcp_storage_bucket_location}`\"\n                )\n            )\n        elif rf.bot_file_exists_in_ncei and (not rf.bot_file_exists_in_gcp):\n            # Upload bot to GCP at the correct storage bucket location.\n            upload_file_to_gcp_storage_bucket(\n                file_name=rf.bot_file_name,\n                file_type=\"bot\",\n                ship_name=rf.ship_name,\n                survey_name=rf.survey_name,\n                echosounder=rf.echosounder,\n                file_location=rf.bot_file_download_path,\n                gcp_bucket=rf.gcp_bucket,\n                data_source=rf.data_source,\n                is_metadata=False,\n                debug=rf.debug,\n            )\n\n        return\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.download_specific_file_from_azure","title":"<code>download_specific_file_from_azure(config_file_path='', container_name='testcontainer', file_path_in_container='')</code>","text":"<p>Creates a DataLakeFileClient and downloads a specific file from <code>container_name</code>.</p> <p>Parameters:</p> Name Type Description Default <code>config_file_path</code> <code>str</code> <p>The location of the config file. Needs a <code>[DEFAULT]</code> section with a <code>azure_connection_string</code> variable defined. Defaults to \"\".</p> <code>''</code> <code>container_name</code> <code>str</code> <p>The container within Azure Data Lake you are trying to access. Defaults to \"testcontainer\".</p> <code>'testcontainer'</code> <code>file_path_in_container</code> <code>str</code> <p>The file path of the file you would like downloaded. Defaults to \"\".</p> <code>''</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def download_specific_file_from_azure(\n    config_file_path: str = \"\",\n    container_name: str = \"testcontainer\",\n    file_path_in_container: str = \"\",\n):\n    \"\"\"Creates a DataLakeFileClient and downloads a specific file from\n    `container_name`.\n\n    Args:\n        config_file_path (str, optional): The location of the config file.\n            Needs a `[DEFAULT]` section with a `azure_connection_string`\n            variable defined. Defaults to \"\".\n        container_name (str, optional): The container within Azure Data Lake\n            you are trying to access. Defaults to \"testcontainer\".\n        file_path_in_container (str, optional): The file path of the file you\n            would like downloaded. Defaults to \"\".\n    \"\"\"\n\n    conf = configparser.ConfigParser()\n    conf.read(config_file_path)\n\n    file = DataLakeFileClient.from_connection_string(\n        conf[\"DEFAULT\"][\"azure_connection_string\"],\n        file_system_name=container_name,\n        file_path=file_path_in_container,\n    )\n\n    file_name = file_path_in_container.split(\"/\")[-1]\n\n    with open(f\"./{file_name}\", \"wb\") as my_file:\n        download = file.download_file()\n        download.readinto(my_file)\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.download_survey_from_ncei","title":"<code>download_survey_from_ncei(ship_name='', survey_name='', download_directory='', max_limit=None, debug=False)</code>","text":"<p>Downloads an entire survey from NCEI to a local directory while maintaining folder structure.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship name. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The name of the survey you would like to download. Defaults to \"\".</p> <code>''</code> <code>download_directory</code> <code>str</code> <p>The directory to which the files will be downloaded. Creates a directory in the cwd if not specified. Defaults to \"\". NOTE: The directory specified will have the <code>ship_name/survey_name</code> folders created within it.</p> <code>''</code> <code>max_limit</code> <code>int</code> <p>The maximum number of random files to download. Defaults to include all files.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether or not you want to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def download_survey_from_ncei(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    download_directory: str = \"\",\n    max_limit: int = None,\n    debug: bool = False,\n):\n    \"\"\"Downloads an entire survey from NCEI to a local directory while\n    maintaining folder structure.\n\n    Args:\n        ship_name (str, optional): The ship name. Defaults to \"\".\n        survey_name (str, optional): The name of the survey you would like to\n            download. Defaults to \"\".\n        download_directory (str, optional): The directory to which the files\n            will be downloaded. Creates a directory in the cwd if not\n            specified. Defaults to \"\".\n            NOTE: The directory specified will have the `ship_name/survey_name`\n            folders created within it.\n        max_limit (int, optional): The maximum number of random files to\n            download.\n            Defaults to include all files.\n        debug (bool, optional): Whether or not you want to print debug\n            statements. Defaults to False.\n    \"\"\"\n\n    if download_directory == \"\":\n        # Create a directory in the cwd\n        download_directory = os.sep.join(\n            [os.path.normpath(\"./\"), f\"{ship_name}\", f\"{survey_name}\"]\n        )\n\n    if debug:\n        logging.debug(\"FORMATTED DOWNLOAD DIRECTORY: %s\", download_directory)\n\n    # Get all s3 objects for the survey\n    print(f\"GETTING ALL S3 OBJECTS FOR SURVEY {survey_name}...\")\n    _, s3_resource, _ = utils.cloud_utils.create_s3_objs()\n    s3_objects = cloud_utils.list_all_objects_in_s3_bucket_location(\n        prefix=f\"data/raw/{ship_name}/{survey_name}/\",\n        s3_resource=s3_resource,\n        return_full_paths=True,\n    )\n    print(f\"FOUND {len(s3_objects)} FILES.\")\n\n    if max_limit is None:\n        max_limit = len(s3_objects)\n\n    subdirs = set()\n    # Get the subfolders from object keys\n    for s3_object in s3_objects:\n        # Skip folders\n        if s3_object.endswith(\"/\"):\n            continue\n        # Get the subfolder structure from the object key\n        subfolder_key = os.sep.join(\n            s3_object.replace(\"data/raw/\", \"\").split(\"/\")[:-1]\n        )\n        subdirs.add(subfolder_key)\n    for subdir in subdirs:\n        os.makedirs(os.sep.join([download_directory, subdir]), exist_ok=True)\n\n    # Create the directory if it doesn't exist.\n    if not os.path.isdir(download_directory):\n        print(f\"CREATING download_directory `{download_directory}`\")\n        os.makedirs(download_directory, exist_ok=True)\n    # normalize the path\n    download_directory = os.path.normpath(download_directory)\n    print(\"CREATED DOWNLOAD DIRECTORIES.\")\n\n    for _, object_key in enumerate(\n        tqdm(s3_objects[:max_limit], desc=\"Downloading\")\n    ):\n        # file_name = object_key.split(\"/\")[-1]\n        local_object_path = object_key.replace(\"data/raw/\", \"\")\n        download_location = os.path.normpath(\n            os.sep.join([download_directory, local_object_path])\n        )\n        download_single_file_from_aws(\n            file_url=object_key, download_location=download_location\n        )\n    print(f\"DOWNLOAD COMPLETE {os.path.abspath(download_directory)}.\")\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.find_and_upload_survey_metadata_from_s3","title":"<code>find_and_upload_survey_metadata_from_s3(ship_name='', survey_name='', gcp_bucket=None, debug=False)</code>","text":"<p>Finds the metadata that is associated with a particular survey in s3, then uploads all of those files into the correct gcp location.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP bucket object used to download the file. Defaults to None.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def find_and_upload_survey_metadata_from_s3(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    gcp_bucket: storage.Client.bucket = None,\n    debug: bool = False,\n):\n    \"\"\"Finds the metadata that is associated with a particular survey in s3,\n    then uploads all of those files into the correct gcp location.\n\n    Args:\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier. Defaults\n            to \"\".\n        gcp_bucket (storage.Client.bucket, optional): The GCP bucket object\n            used to download the file. Defaults to None.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n\n    metadata_location_in_s3 = f\"data/raw/{ship_name}/{survey_name}/metadata/\"\n\n    try:\n        _, _, s3_bucket = utils.cloud_utils.create_s3_objs()\n    except Exception as e:\n        logging.error(\"CANNOT ESTABLISH CONNECTION TO S3 BUCKET..\\n%s\", e)\n        raise\n\n    num_metadata_objects = cloud_utils.count_objects_in_s3_bucket_location(\n        prefix=metadata_location_in_s3, bucket=s3_bucket\n    )\n\n    if debug:\n        logging.debug(\n            \"%d num_metadata_objects FOUND IN S3 FOR %s - %s\",\n            num_metadata_objects,\n            ship_name,\n            survey_name,\n        )\n\n    if num_metadata_objects &gt;= 1:\n        # Get object keys\n        s3_objects = cloud_utils.list_all_objects_in_s3_bucket_location(\n            prefix=metadata_location_in_s3, s3_resource=s3_bucket\n        )\n        # Download and upload each object\n        for full_path, file_name in s3_objects:\n            # Get the correct full file download location\n            file_download_directory = os.sep.join(\n                [os.path.normpath(\"./\"), file_name]\n            )\n            # Download from aws\n            download_single_file_from_aws(\n                file_url=full_path, download_location=file_download_directory\n            )\n            # Upload to gcp\n            upload_file_to_gcp_storage_bucket(\n                file_name=file_name,\n                ship_name=ship_name,\n                survey_name=survey_name,\n                file_location=file_download_directory,\n                gcp_bucket=gcp_bucket,\n                data_source=\"NCEI\",\n                is_metadata=False,\n                is_survey_metadata=True,\n                debug=debug,\n            )\n            # Remove local file (it's temporary)\n            os.remove(file_download_directory)\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.ingestion.find_data_source_for_file","title":"<code>find_data_source_for_file()</code>","text":"<p>Finds the data source of a given filename by checking all possible data sources.</p> Source code in <code>src\\aalibrary\\ingestion.py</code> <pre><code>def find_data_source_for_file():\n    \"\"\"Finds the data source of a given filename by checking all possible data\n    sources.\"\"\"\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.conversion","title":"<code>conversion</code>","text":"<p>This file is used to store conversion functions for the AALibrary.</p> <p>Functions:</p> Name Description <code>convert_local_raw_to_ices_netcdf</code> <p>ENTRYPOINT FOR END-USERS</p> <code>convert_local_raw_to_netcdf</code> <p>ENTRYPOINT FOR END-USERS</p> <code>convert_raw_to_netcdf</code> <p>ENTRYPOINT FOR END-USERS</p> <code>convert_raw_to_netcdf_ices</code> <p>ENTRYPOINT FOR END-USERS</p>"},{"location":"documentation/aalibrary/#aalibrary.conversion.convert_local_raw_to_ices_netcdf","title":"<code>convert_local_raw_to_ices_netcdf(raw_file_location='', netcdf_file_download_directory='', echosounder='', delete_raw_after=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS Converts a local (on your computer) file from raw into netcdf using echopype.</p> <p>Parameters:</p> Name Type Description Default <code>raw_file_location</code> <code>str</code> <p>The location of the raw file. Defaults to \"\".</p> <code>''</code> <code>netcdf_file_download_directory</code> <code>str</code> <p>The location you want to download your netcdf file to. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used. Can be one of [\"EK80\", \"EK70\"]. Defaults to \"\".</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>Whether or not to overwrite the netcdf file. Defaults to False.</p> required <code>delete_raw_after</code> <code>bool</code> <p>Whether or not to delete the raw file after conversion is complete. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\conversion.py</code> <pre><code>def convert_local_raw_to_ices_netcdf(\n    raw_file_location: str = \"\",\n    netcdf_file_download_directory: str = \"\",\n    echosounder: str = \"\",\n    delete_raw_after: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n    Converts a local (on your computer) file from raw into netcdf using\n    echopype.\n\n    Args:\n        raw_file_location (str, optional): The location of the raw file.\n            Defaults to \"\".\n        netcdf_file_download_directory (str, optional): The location you want\n            to download your netcdf file to. Defaults to \"\".\n        echosounder (str, optional): The echosounder used. Can be one of\n            [\"EK80\", \"EK70\"]. Defaults to \"\".\n        overwrite (bool, optional): Whether or not to overwrite the netcdf\n            file. Defaults to False.\n        delete_raw_after (bool, optional): Whether or not to delete the raw\n            file after conversion is complete. Defaults to False.\n    \"\"\"\n\n    netcdf_file_download_directory = os.sep.join(\n        [os.path.normpath(netcdf_file_download_directory)]\n    )\n    print(f\"netcdf_file_download_directory {netcdf_file_download_directory}\")\n\n    # Create the download directory (path) if it doesn't exist\n    if not os.path.exists(netcdf_file_download_directory):\n        os.makedirs(netcdf_file_download_directory)\n\n    # Make sure the echosounder specified matches the raw file data.\n    if echosounder.lower() == \"ek80\":\n        assert sonar_checker.is_EK80(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n    elif echosounder.lower() == \"ek60\":\n        assert sonar_checker.is_EK60(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n    else:\n        print(\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` IS NOT SUPPORTED FOR \"\n            \"ICES NETCDF CONVERSION. PLEASE USE `EK80` OR `EK60`.\"\n        )\n\n    try:\n        print(\"CONVERTING RAW TO NETCDF...\")\n        raw_file_echopype = open_raw(\n            raw_file=raw_file_location, sonar_model=echosounder\n        )\n        if echosounder.lower() == \"ek80\":\n            echopype_ek80_raw_to_ices_netcdf(\n                echodata=raw_file_echopype,\n                export_file=netcdf_file_download_directory,\n            )\n        elif echosounder.lower() == \"ek60\":\n            echopype_ek60_raw_to_ices_netcdf(\n                echodata=raw_file_echopype,\n                export_file=netcdf_file_download_directory,\n            )\n        print(\"CONVERTED.\")\n        if delete_raw_after:\n            try:\n                print(\"DELETING RAW FILE...\")\n                os.remove(raw_file_location)\n                print(\"DELETED.\")\n            except Exception as e:\n                print(e)\n                print(\n                    \"THE RAW FILE COULD NOT BE DELETED DUE TO THE ERROR ABOVE.\"\n                )\n    except Exception as e:\n        logging.error(\n            \"COULD NOT CONVERT `%s` DUE TO ERROR %s\", raw_file_location, e\n        )\n        raise e\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.conversion.convert_local_raw_to_netcdf","title":"<code>convert_local_raw_to_netcdf(raw_file_location='', netcdf_file_download_directory='', echosounder='', overwrite=False, delete_raw_after=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS Converts a local (on your computer) file from raw into netcdf using echopype.</p> <p>Parameters:</p> Name Type Description Default <code>raw_file_location</code> <code>str</code> <p>The location of the raw file. Defaults to \"\".</p> <code>''</code> <code>netcdf_file_download_directory</code> <code>str</code> <p>The location you want to download your netcdf file to. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used. Can be one of [\"EK80\", \"EK70\"]. Defaults to \"\".</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>Whether or not to overwrite the netcdf file. Defaults to False.</p> <code>False</code> <code>delete_raw_after</code> <code>bool</code> <p>Whether or not to delete the raw file after conversion is complete. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\conversion.py</code> <pre><code>def convert_local_raw_to_netcdf(\n    raw_file_location: str = \"\",\n    netcdf_file_download_directory: str = \"\",\n    echosounder: str = \"\",\n    overwrite: bool = False,\n    delete_raw_after: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n    Converts a local (on your computer) file from raw into netcdf using\n    echopype.\n\n    Args:\n        raw_file_location (str, optional): The location of the raw file.\n            Defaults to \"\".\n        netcdf_file_download_directory (str, optional): The location you want\n            to download your netcdf file to. Defaults to \"\".\n        echosounder (str, optional): The echosounder used. Can be one of\n            [\"EK80\", \"EK70\"]. Defaults to \"\".\n        overwrite (bool, optional): Whether or not to overwrite the netcdf\n            file. Defaults to False.\n        delete_raw_after (bool, optional): Whether or not to delete the raw\n            file after conversion is complete. Defaults to False.\n    \"\"\"\n\n    netcdf_file_download_directory = os.sep.join(\n        [os.path.normpath(netcdf_file_download_directory)]\n    )\n    print(f\"netcdf_file_download_directory {netcdf_file_download_directory}\")\n\n    # Create the download directory (path) if it doesn't exist\n    if not os.path.exists(netcdf_file_download_directory):\n        os.makedirs(netcdf_file_download_directory)\n\n    # Make sure the echosounder specified matches the raw file data.\n    if echosounder.lower() == \"ek80\":\n        assert sonar_checker.is_EK80(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n    elif echosounder.lower() == \"ek60\":\n        assert sonar_checker.is_EK60(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n    elif echosounder.lower() == \"azfp6\":\n        assert sonar_checker.is_AZFP6(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n    elif echosounder.lower() == \"azfp\":\n        assert sonar_checker.is_AZFP(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n    elif echosounder.lower() == \"ad2cp\":\n        assert sonar_checker.is_AD2CP(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n    elif echosounder.lower() == \"er60\":\n        assert sonar_checker.is_ER60(\n            raw_file=raw_file_location, storage_options={}\n        ), (\n            f\"THE ECHOSOUNDER SPECIFIED `{echosounder}` DOES NOT MATCH THE \"\n            \"ECHOSOUNDER FOUND WITHIN THE RAW FILE.\"\n        )\n\n    try:\n        print(\"CONVERTING RAW TO NETCDF...\")\n        raw_file_echopype = open_raw(\n            raw_file=raw_file_location, sonar_model=echosounder\n        )\n        raw_file_echopype.to_netcdf(\n            save_path=netcdf_file_download_directory, overwrite=overwrite\n        )\n        print(\"CONVERTED.\")\n        if delete_raw_after:\n            try:\n                print(\"DELETING RAW FILE...\")\n                os.remove(raw_file_location)\n                print(\"DELETED.\")\n            except Exception as e:\n                print(e)\n                print(\n                    \"THE RAW FILE COULD NOT BE DELETED DUE TO THE ERROR ABOVE.\"\n                )\n    except Exception as e:\n        logging.error(\n            \"COULD NOT CONVERT `%s` DUE TO ERROR %s\", raw_file_location, e\n        )\n        raise e\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.conversion.convert_raw_to_netcdf","title":"<code>convert_raw_to_netcdf(file_name='', file_type='raw', ship_name='', survey_name='', echosounder='', data_source='', file_download_directory='', overwrite=False, delete_raw_after=False, gcp_bucket=None, is_metadata=False, debug=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS This function allows one to convert a file from raw to netcdf. Then uploads the file to GCP storage for caching.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (do not include the dot \".\"). Defaults to \"\".</p> <code>'raw'</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the file. Necessary due to the way the storage bucket is organized. Can be one of [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".</p> <code>''</code> <code>file_download_directory</code> <code>str</code> <p>The local directory you want to store your file in. Defaults to \"\".</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>Whether or not to overwrite the netcdf file. Defaults to False.</p> <code>False</code> <code>delete_raw_after</code> <code>bool</code> <p>Whether or not to delete the raw file after conversion is complete. Defaults to False.</p> <code>False</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP bucket object used to download the file. Defaults to None.</p> <code>None</code> <code>is_metadata</code> <code>bool</code> <p>Whether or not the file is a metadata file. Necessary since files that are considered metadata (metadata json, or readmes) are stored in a separate directory. Defaults to False.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\conversion.py</code> <pre><code>def convert_raw_to_netcdf(\n    file_name: str = \"\",\n    file_type: str = \"raw\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"\",\n    file_download_directory: str = \"\",\n    overwrite: bool = False,\n    delete_raw_after: bool = False,\n    gcp_bucket: storage.Client.bucket = None,\n    is_metadata: bool = False,\n    debug: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n    This function allows one to convert a file from raw to netcdf. Then uploads\n    the file to GCP storage for caching.\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (do not include the dot \".\").\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier. Defaults\n            to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the file. Necessary due to\n            the way the storage bucket is organized. Can be one of\n            [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".\n        file_download_directory (str, optional): The local directory you want\n            to store your file in. Defaults to \"\".\n        overwrite (bool, optional): Whether or not to overwrite the netcdf\n            file. Defaults to False.\n        delete_raw_after (bool, optional): Whether or not to delete the raw\n            file after conversion is complete. Defaults to False.\n        gcp_bucket (storage.Client.bucket, optional): The GCP bucket object\n            used to download the file. Defaults to None.\n        is_metadata (bool, optional): Whether or not the file is a metadata\n            file. Necessary since files that are considered metadata (metadata\n            json, or readmes) are stored in a separate directory. Defaults to\n            False.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n    # TODO: Implement an 'upload' param default to True.\n\n    _, _, gcp_bucket = utils.cloud_utils.setup_gcp_storage_objs()\n    _, s3_resource, _ = utils.cloud_utils.create_s3_objs()\n\n    rf = RawFile(\n        file_name=file_name,\n        file_type=file_type,\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        data_source=data_source,\n        file_download_directory=file_download_directory,\n        overwrite=overwrite,\n        gcp_bucket=gcp_bucket,\n        is_metadata=is_metadata,\n        debug=debug,\n        s3_resource=s3_resource,\n        s3_bucket_name=\"noaa-wcsd-pds\",\n    )\n\n    # Here we check for a netcdf version of the raw file on GCP\n    print(\"CHECKING FOR NETCDF VERSION ON GCP...\")\n    if rf.netcdf_file_exists_in_gcp:\n        # Inform the user if a netcdf version exists in cache.\n        download_netcdf_file(\n            raw_file_name=rf.netcdf_file_name,\n            file_type=\"netcdf\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            data_source=rf.data_source,\n            file_download_directory=rf.file_download_directory,\n            gcp_bucket=gcp_bucket,\n            debug=rf.debug,\n        )\n    else:\n        logging.info(\n            \"FILE `%s` DOES NOT EXIST AS NETCDF. DOWNLOADING/CONVERTING/\"\n            \"UPLOADING RAW...\",\n            rf.raw_file_name,\n        )\n\n        # Download the raw file.\n        # This function should take care of checking whether the raw file\n        # exists in any of the data sources, and fetching it.\n        download_raw_file(\n            file_name=rf.file_name,\n            file_type=rf.file_type,\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            data_source=rf.data_source,\n            file_download_directory=rf.file_download_directory,\n            debug=rf.debug,\n        )\n\n        # Convert the raw file to netcdf.\n        convert_local_raw_to_netcdf(\n            raw_file_location=rf.raw_file_download_path,\n            netcdf_file_download_directory=rf.file_download_directory,\n            echosounder=rf.echosounder,\n            overwrite=overwrite,\n            delete_raw_after=delete_raw_after,\n        )\n\n        # Upload the netcdf to the correct location for parsing.\n        upload_file_to_gcp_storage_bucket(\n            file_name=rf.netcdf_file_name,\n            file_type=\"netcdf\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            file_location=rf.netcdf_file_download_path,\n            gcp_bucket=gcp_bucket,\n            data_source=rf.data_source,\n            is_metadata=False,\n            debug=rf.debug,\n        )\n        # Upload the metadata file associated with this\n        metadata.create_and_upload_metadata_df_for_netcdf(\n            file_name=rf.netcdf_file_name,\n            file_type=\"netcdf\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            data_source=rf.data_source,\n            gcp_bucket=gcp_bucket,\n            netcdf_local_file_location=rf.netcdf_file_download_path,\n            debug=debug,\n        )\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.conversion.convert_raw_to_netcdf_ices","title":"<code>convert_raw_to_netcdf_ices(file_name='', file_type='raw', ship_name='', survey_name='', echosounder='', data_source='', file_download_directory='', overwrite=False, delete_raw_after=False, gcp_bucket=None, is_metadata=False, debug=False)</code>","text":"<p>ENTRYPOINT FOR END-USERS This function allows one to convert a file from raw to netcdf. Then uploads the file to GCP storage for caching.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (do not include the dot \".\"). Defaults to \"\".</p> <code>'raw'</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the file. Necessary due to the way the storage bucket is organized. Can be one of [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".</p> <code>''</code> <code>file_download_directory</code> <code>str</code> <p>The local directory you want to store your file in. Defaults to \"\".</p> <code>''</code> <code>overwrite</code> <code>bool</code> <p>Whether or not to overwrite the netcdf file. Defaults to False.</p> <code>False</code> <code>delete_raw_after</code> <code>bool</code> <p>Whether or not to delete the raw file after conversion is complete. Defaults to False.</p> <code>False</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP bucket object used to download the file. Defaults to None.</p> <code>None</code> <code>is_metadata</code> <code>bool</code> <p>Whether or not the file is a metadata file. Necessary since files that are considered metadata (metadata json, or readmes) are stored in a separate directory. Defaults to False.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\conversion.py</code> <pre><code>def convert_raw_to_netcdf_ices(\n    file_name: str = \"\",\n    file_type: str = \"raw\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"\",\n    file_download_directory: str = \"\",\n    overwrite: bool = False,\n    delete_raw_after: bool = False,\n    gcp_bucket: storage.Client.bucket = None,\n    is_metadata: bool = False,\n    debug: bool = False,\n):\n    \"\"\"ENTRYPOINT FOR END-USERS\n    This function allows one to convert a file from raw to netcdf. Then uploads\n    the file to GCP storage for caching.\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (do not include the dot \".\").\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier. Defaults\n            to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the file. Necessary due to\n            the way the storage bucket is organized. Can be one of\n            [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".\n        file_download_directory (str, optional): The local directory you want\n            to store your file in. Defaults to \"\".\n        overwrite (bool, optional): Whether or not to overwrite the netcdf\n            file. Defaults to False.\n        delete_raw_after (bool, optional): Whether or not to delete the raw\n            file after conversion is complete. Defaults to False.\n        gcp_bucket (storage.Client.bucket, optional): The GCP bucket object\n            used to download the file. Defaults to None.\n        is_metadata (bool, optional): Whether or not the file is a metadata\n            file. Necessary since files that are considered metadata (metadata\n            json, or readmes) are stored in a separate directory. Defaults to\n            False.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n\n    _, _, gcp_bucket = utils.cloud_utils.setup_gcp_storage_objs()\n    _, s3_resource, _ = utils.cloud_utils.create_s3_objs()\n\n    rf = RawFile(\n        file_name=file_name,\n        file_type=file_type,\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        data_source=data_source,\n        file_download_directory=file_download_directory,\n        overwrite=overwrite,\n        gcp_bucket=gcp_bucket,\n        is_metadata=is_metadata,\n        debug=debug,\n        s3_resource=s3_resource,\n        s3_bucket_name=\"noaa-wcsd-pds\",\n    )\n\n    # Here we check for a netcdf version of the raw file on GCP\n    print(\"CHECKING FOR NETCDF VERSION ON GCP...\")\n    if rf.netcdf_file_exists_in_gcp:\n        # Inform the user if a netcdf version exists in cache.\n        download_netcdf_file(\n            raw_file_name=rf.netcdf_file_name,\n            file_type=\"netcdf\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            data_source=rf.data_source,\n            file_download_directory=rf.file_download_directory,\n            gcp_bucket=gcp_bucket,\n            debug=rf.debug,\n        )\n    else:\n        logging.info(\n            \"FILE `%s` DOES NOT EXIST AS NETCDF. DOWNLOADING/CONVERTING/\"\n            \"UPLOADING RAW...\",\n            rf.raw_file_name,\n        )\n\n        # Download the raw file.\n        # This function should take care of checking whether the raw file\n        # exists in any of the data sources, and fetching it.\n        download_raw_file(\n            file_name=rf.file_name,\n            file_type=rf.file_type,\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            data_source=rf.data_source,\n            file_download_directory=rf.file_download_directory,\n            debug=rf.debug,\n        )\n\n        # Convert the raw file to netcdf.\n        convert_local_raw_to_ices_netcdf(\n            raw_file_location=rf.raw_file_download_path,\n            netcdf_file_download_directory=rf.file_download_directory,\n            echosounder=rf.echosounder,\n            delete_raw_after=delete_raw_after,\n        )\n\n        # Upload the netcdf to the correct location for parsing.\n        upload_file_to_gcp_storage_bucket(\n            file_name=rf.netcdf_file_name,\n            file_type=\"netcdf\",\n            ship_name=rf.ship_name,\n            survey_name=rf.survey_name,\n            echosounder=rf.echosounder,\n            file_location=rf.netcdf_file_download_path,\n            gcp_bucket=gcp_bucket,\n            data_source=rf.data_source,\n            is_metadata=False,\n            debug=rf.debug,\n        )\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.metadata","title":"<code>metadata</code>","text":"<p>This file contains functions that have to do with metadata.</p> <p>Functions:</p> Name Description <code>create_and_upload_metadata_df</code> <p>Creates a metadata file with appropriate information. Then uploads it</p> <code>create_and_upload_metadata_df_for_netcdf</code> <p>Creates a metadata file with appropriate information for netcdf files.</p> <code>create_metadata_json_for_raw_files</code> <p>Creates a JSON object containing metadata for the current user.</p> <code>get_metadata_in_df_format</code> <p>Retrieves the metadata associated with all objects in GCP in DataFrame</p> <code>upload_ncei_metadata_df_to_bigquery</code> <p>Finds the metadata obtained from a survey on NCEI, and uploads it to the</p>"},{"location":"documentation/aalibrary/#aalibrary.metadata.create_and_upload_metadata_df","title":"<code>create_and_upload_metadata_df(rf=None, debug=False)</code>","text":"<p>Creates a metadata file with appropriate information. Then uploads it to the correct table in GCP.</p> <p>Parameters:</p> Name Type Description Default <code>rf</code> <code>RawFile</code> <p>The RawFile object associated with this file. Defaults to None.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\metadata.py</code> <pre><code>def create_and_upload_metadata_df(\n    rf: RawFile = None,\n    debug: bool = False,\n):\n    \"\"\"Creates a metadata file with appropriate information. Then uploads it\n    to the correct table in GCP.\n\n    Args:\n        rf (RawFile, optional): The RawFile object associated with this file.\n            Defaults to None.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n    \"\"\"\n\n    # Create the metadata file to be uploaded.\n    metadata_df = create_metadata_json_for_raw_files(\n        rf=rf,\n        debug=debug,\n    )\n    # TODO: take care of netcdf files, possibly upload to another table with\\\n    # their metadata.\n    # If the file is a netcdf, we extract even more data from its headers.\n    # if netcdf_local_file_location:\n    #     # Extract the metadata\n    #     netcdf_metadata = nc_reader.get_netcdf_header(\n    #         file_path=netcdf_local_file_location\n    #     )\n    #     # Merge the netcdf metadata with the metadata we have created.\n    #     metadata_df.update(netcdf_metadata)\n    # # Extract the metadata string\n    # metadata_json_str = json.dumps(metadata_df)\n    # with open(f\"./{file_name}.json\", \"w\") as jf:\n    #     jf.write(metadata_json_str)\n\n    # Upload to GCP BigQuery\n    metadata_df.to_gbq(\n        destination_table=\"metadata.aalibrary_file_metadata\",\n        project_id=\"ggn-nmfs-aa-dev-1\",\n        if_exists=\"append\",\n    )\n\n    return\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.metadata.create_and_upload_metadata_df_for_netcdf","title":"<code>create_and_upload_metadata_df_for_netcdf()</code>","text":"<p>Creates a metadata file with appropriate information for netcdf files. Then uploads it to the correct table in GCP.</p> Source code in <code>src\\aalibrary\\metadata.py</code> <pre><code>def create_and_upload_metadata_df_for_netcdf():\n    \"\"\"Creates a metadata file with appropriate information for netcdf files.\n    Then uploads it to the correct table in GCP.\"\"\"\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.metadata.create_metadata_json_for_raw_files","title":"<code>create_metadata_json_for_raw_files(rf=None, debug=False)</code>","text":"<p>Creates a JSON object containing metadata for the current user.</p> <p>Parameters:</p> Name Type Description Default <code>rf</code> <code>RawFile</code> <p>The RawFile object associated with this file. Defaults to None.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether or not to print out the metadata json. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The metadata dataframe for the <code>aalibrary_file_metadata</code> database table.</p> Source code in <code>src\\aalibrary\\metadata.py</code> <pre><code>def create_metadata_json_for_raw_files(\n    rf: RawFile = None,\n    debug: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"Creates a JSON object containing metadata for the current user.\n\n    Args:\n        rf (RawFile, optional): The RawFile object associated with this file.\n            Defaults to None.\n        debug (bool, optional): Whether or not to print out the metadata json.\n            Defaults to False.\n\n    Returns:\n        pd.DataFrame: The metadata dataframe for the `aalibrary_file_metadata`\n            database table.\n    \"\"\"\n\n    # Gets the current gcloud user's email\n    get_curr_user_email_cmd = [\"gcloud\", \"config\", \"get-value\", \"account\"]\n    if platform.system() == \"Windows\":\n        email = subprocess.run(\n            get_curr_user_email_cmd,\n            shell=True,\n            capture_output=True,\n            text=True,\n            check=False,\n        ).stdout\n    else:\n        email = subprocess.run(\n            get_curr_user_email_cmd,\n            capture_output=True,\n            text=True,\n            check=False,\n        ).stdout\n    email = email.replace(\"\\n\", \"\")\n\n    # get the survey datetime.\n    file_datetime = datetime.strptime(\n        rf.get_file_datetime_str(), \"%Y-%m-%d %H:%M:%S\"\n    )\n\n    # calculate the deletion datetime\n    curr_datetime = datetime.now()\n    deletion_datetime = curr_datetime + timedelta(days=90)\n    deletion_datetime = deletion_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    metadata_json = {\n        \"FILE_NAME\": rf.raw_file_name,\n        \"DATE_CREATED\": datetime.now(timezone.utc).strftime(\n            \"%Y-%m-%d %H:%M:%S\"\n        ),\n        \"UPLOADED_BY\": email,\n        \"ECHOPYPE_VERSION\": echopype.__version__,\n        \"PYTHON_VERSION\": sys.version.split(\" \")[0],\n        \"NUMPY_VERSION\": np.version.version,\n        # maybe just add in echopype's reqs.\n        # pip lock file - for current environment\n        \"NCEI_CRUISE_ID\": rf.survey_name,\n        \"NCEI_URI\": rf.raw_file_s3_object_key,\n        \"GCP_URI\": rf.raw_gcp_storage_bucket_location,\n        \"FILE_DATETIME\": file_datetime,\n        \"DELETION_DATETIME\": deletion_datetime,\n        \"ICES_CODE\": rf.ices_code,\n    }\n\n    aalibrary_metadata_df = pd.json_normalize(metadata_json)\n    # make sure data types are conserved before upload to BigQuery.\n    aalibrary_metadata_df[\"DATE_CREATED\"] = pd.to_datetime(\n        aalibrary_metadata_df[\"DATE_CREATED\"], format=\"%Y-%m-%d %H:%M:%S\"\n    )\n    aalibrary_metadata_df[\"FILE_DATETIME\"] = pd.to_datetime(\n        aalibrary_metadata_df[\"FILE_DATETIME\"], format=\"%Y-%m-%d %H:%M:%S\"\n    )\n    aalibrary_metadata_df[\"DELETION_DATETIME\"] = pd.to_datetime(\n        aalibrary_metadata_df[\"DELETION_DATETIME\"], format=\"%Y-%m-%d %H:%M:%S\"\n    )\n\n    if debug:\n        print(aalibrary_metadata_df)\n        logging.debug(aalibrary_metadata_df)\n\n    return aalibrary_metadata_df\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.metadata.get_metadata_in_df_format","title":"<code>get_metadata_in_df_format()</code>","text":"<p>Retrieves the metadata associated with all objects in GCP in DataFrame format.</p> Source code in <code>src\\aalibrary\\metadata.py</code> <pre><code>def get_metadata_in_df_format():\n    \"\"\"Retrieves the metadata associated with all objects in GCP in DataFrame\n    format.\"\"\"\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.metadata.upload_ncei_metadata_df_to_bigquery","title":"<code>upload_ncei_metadata_df_to_bigquery(ship_name='', survey_name='', download_location='', s3_bucket=None)</code>","text":"<p>Finds the metadata obtained from a survey on NCEI, and uploads it to the <code>ncei_cruise_metadata</code> database table in bigquery. Also handles for extra database entries that are needed, such as uploading to the <code>ncei_instrument_metadata</code> when necessary.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>download_location</code> <code>str</code> <p>The local download location for the file. Defaults to \"\".</p> <code>''</code> <code>s3_bucket</code> <code>resource</code> <p>The bucket resource object. Defaults to None.</p> <code>None</code> Source code in <code>src\\aalibrary\\metadata.py</code> <pre><code>def upload_ncei_metadata_df_to_bigquery(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    download_location: str = \"\",\n    s3_bucket: boto3.resource = None,\n):\n    \"\"\"Finds the metadata obtained from a survey on NCEI, and uploads it to the\n    `ncei_cruise_metadata` database table in bigquery. Also handles for extra\n    database entries that are needed, such as uploading to the\n    `ncei_instrument_metadata` when necessary.\n\n    Args:\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier.\n            Defaults to \"\".\n        download_location (str, optional): The local download location for the\n            file. Defaults to \"\".\n        s3_bucket (boto3.resource, optional): The bucket resource object.\n            Defaults to None.\n    \"\"\"\n\n    # This var can either be a string with the file's location, or None.\n    metadata_file_exists = check_if_tugboat_metadata_json_exists_in_survey(\n        ship_name=ship_name, survey_name=survey_name, s3_bucket=s3_bucket\n    )\n\n    if metadata_file_exists:\n        # TODO: Download all metadata files to local for download? Even\n        # calibration files?\n        # Handle for main metadata file for upload to BigQuery.\n        s3_bucket.download_file(metadata_file_exists, download_location)\n        # Subroutine to parse this file and upload to gcp.\n        _parse_and_upload_ncei_survey_level_metadata(\n            survey_name=survey_name, file_location=download_location\n        )\n</code></pre>"},{"location":"documentation/aalibrary/#aalibrary.queries","title":"<code>queries</code>","text":"<p>This script contains classes that have SQL queries used for interaction with the metadata database in BigQuery.</p> <p>Classes:</p> Name Description <code>MetadataQueries</code> <p>This class contains queries related to the upload, alteration, and</p>"},{"location":"documentation/aalibrary/#aalibrary.queries.MetadataQueries","title":"<code>MetadataQueries</code>  <code>dataclass</code>","text":"<p>This class contains queries related to the upload, alteration, and retrieval of metadata from our BigQuery instance.</p> Source code in <code>src\\aalibrary\\queries.py</code> <pre><code>@dataclass\nclass MetadataQueries:\n    \"\"\"This class contains queries related to the upload, alteration, and\n    retrieval of metadata from our BigQuery instance.\n    \"\"\"\n\n    get_all_aalibrary_metadata_records: str = \"\"\"\n    SELECT * FROM `ggn-nmfs-aa-dev-1.metadata.aalibrary_file_metadata`\"\"\"\n\n    # TODO for mike ryan\n    get_all_possible_ship_names_from_database: str = \"\"\"\n    SELECT ship_name from `ggn-nmfs-aa-dev-1.metadata.aalibrary_file_metadata`\n    \"\"\"\n\n    def get_all_surveys_associated_with_a_ship_name(self, ship_name: str = \"\"):\n        get_all_surveys_associated_with_a_ship_name_query: str = \"\"\"\"\"\"\n        return get_all_surveys_associated_with_a_ship_name_query\n\n    def get_all_echosounders_used_in_a_survey(self, survey: str = \"\"): ...\n\n    def get_all_netcdf_files_in_database(self): ...\n</code></pre>"},{"location":"documentation/gcp_overview/","title":"Google Cloud Platform Overview","text":"<p>AALibrary serves as a data-fetching tool with many capabilities (such as Python implementation and console-based UI implementation). This library provides also provides advanced methods for interacting with the GCP cloud storage buckets/metadata database, to allow users to fetch specific data, perform analyses, and more\u2026</p>"},{"location":"documentation/gcp_overview/#current-environments-aka-projects-in-gcp","title":"Current Environments (aka \u2018Projects\u2019) in GCP","text":"<ul> <li>ggn-nmfs-aa-dev-1:<ul> <li>Used for development purposes for AALibrary.</li> <li>Good for testing.</li> <li>All data exists in one storage bucket.</li> <li>NOTE: NOT a stable environment. Data gets deleted often.</li> <li>No workstations available.</li> <li>Storage Bucket: ggn-nmfs-aa-dev-1-data</li> </ul> </li> <li>ggn-nmfs-aa-prod-1:<ul> <li>Production environment.</li> <li>Transfer Appliance files exist in separate bucket (ta_upload)</li> <li>No workstations available.</li> <li>Storage Buckets: ggn-nmfs-aa-prod-1-data, ta_upload (Files are stored until archival to NCEI)</li> </ul> </li> <li>ggn-nmfs-wsent-prod-1:<ul> <li>Used for our workstations.</li> </ul> </li> </ul>"},{"location":"documentation/gcp_overview/#storage-bucket-layouts","title":"Storage Bucket Layouts","text":""},{"location":"documentation/gcp_overview/#dev-environment-layout","title":"Dev Environment Layout","text":"<p>Below is the hierarchical layout for the development environment's storage bucket. As you can see, data is organized based on it's metadata. For example, any file relating to a specific survey will only exist within that survey's folder.</p> <p>This also means that this layout has a 1:1 mapping of each file to its location in the storage bucket. This makes files easily searchable using AALibrary.</p> <p></p>"},{"location":"documentation/gcp_overview/#prod-environment-layout","title":"Prod Environment Layout","text":"<p>The production environments storage bucket also utilizes a similar layout.</p> <p></p>"},{"location":"documentation/gcp_overview/#transfer-appliance-layout","title":"Transfer Appliance Layout","text":"<p>This storage bucket (ta_upload) exists within the production environment. The layout is different here compared to the other two storage buckets. There is a hierarchical order present, but instead of viewing <code>/data_source/</code> as a storage entity, it is viewed as a Science Center entity.</p> <p>There are two layouts possible here. The first refers to a similar layout that we have in the other storage buckets. This is ideally the layout we would like to reach as a strategic initiative.</p> <p></p> <p>The second layout is <code>dealer's choice</code>. This means that the organization of the folders within this layout is dependent on the FMC that is uploading data. Keep in mind, our goal is to upload all of the data to the cloud, so organization and file naming conventions can be corrected after the upload is done.</p> <p></p>"},{"location":"documentation/gcp_overview/#file-naming-conventions","title":"File Naming Conventions","text":"<p>There are certain file naming conventions that we are trying to normalize here at AASI. For reference, here are the file naming conventions that our storage buckets follow:</p> <ul> <li>Files should start with with the Survey_Name</li> <li>Files should end with the DateTime string:<ul> <li>Hours should be in Military time</li> </ul> </li> <li>Applies to Kongsberg echosounder files, other files may use other naming conventions</li> </ul>"},{"location":"documentation/gcp_overview/#metadata-database","title":"Metadata Database","text":"<p>We have a metadata database that can hold the same Tugboat metadata for enhanced usage through the AALibrary. (e.g. searching/retrieving files from March 26th, 2024). Coriolix Metadata is planned to be added, as more of it becomes available. If you would like access to the metadata DB, please see the permissions page.</p>"},{"location":"documentation/raw_file/","title":"Documentation for <code>aalibrary.raw_file.RawFile</code>","text":"<p>The <code>RawFile</code> class provides a way to access the meta-metadata of a raw data file. This includes automatic checking for whether a file exists in NCEI, OMAO, or GCP.</p> <p>A class used to represent a raw file, from given parameters.</p> <p>Methods:</p> Name Description <code>get_file_datetime_str</code> <p>Gets the datetime as a datetime formatted string.</p> <code>get_str_times</code> <p>Gets the parsed times of the current file in dict format</p> <code>print_times</code> <p>Prints the parsed times of the current file in dict format.</p> Source code in <code>src\\aalibrary\\raw_file.py</code> <pre><code>class RawFile:\n    \"\"\"A class used to represent a raw file, from given parameters.\"\"\"\n\n    file_name: str = None\n    file_type: str = None\n    ship_name: str = None\n    survey_name: str = None\n    echosounder: str = None\n    data_source: str = None\n    file_download_directory: str = None\n    is_metadata: bool = False\n    upload_to_gcp: bool = False\n    debug: bool = False\n    gcp_project_id: str = \"ggn-nmfs-aa-dev-1\"\n    gcp_bucket_name: str = \"ggn-nmfs-aa-dev-1-data\"\n    gcp_bucket: storage.Client.bucket = None\n    s3_resource: boto3.resource = None\n    # Get all valid and normalized ICES ship names\n    valid_ICES_ship_names = ices_ship_names.get_all_ices_ship_names(\n        normalize_ship_names=True\n    )\n\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n        self._handle_paths()\n        self._create_vars_for_use_later()\n        self._create_download_directories_if_not_exists()\n\n        self._check_for_assertion_errors()\n\n    def __repr__(self):\n        return pprint.pformat(self.__dict__, indent=4)\n\n    def __str__(self):\n        return pprint.pformat(self.__dict__, indent=4)\n\n    def _handle_paths(self):\n        \"\"\"Handles all minute functions and adjustments related to paths.\"\"\"\n\n        # Normalize paths\n        if \"file_download_directory\" in self.__dict__:\n            self.file_download_directory = os.path.normpath(\n                os.sep.join([os.path.abspath(self.file_download_directory)])\n            )\n\n    def _create_download_directories_if_not_exists(self):\n        \"\"\"Create the download directory (path) if it doesn't exist.\"\"\"\n\n        if \"file_download_directory\" in self.__dict__:\n            if not os.path.exists(self.file_download_directory):\n                os.makedirs(self.file_download_directory)\n\n    def _create_vars_for_use_later(self):\n        \"\"\"Creates vars that will add value and can be utilized later.\"\"\"\n\n        # Get the parsed datetime of the file.\n        # For NCEI - according to their format for naming files.\n        if (self.data_source == \"NCEI\") or (self.data_source == \"OMAO\"):\n            # ex. 2107RL_CW-D20211001-T132449.raw\n            # TODO: `telegram` within raw file has a time stamp, maybe extract\n\n            temp = self.file_name.lower().split(\"d\")[-1].split(\".\")[0]\n            self.year_str = temp[:4]\n            self.month_str = temp[4:6]\n            self.date_str = temp[6:8]\n            self.year = int(self.year_str)\n            self.month = int(self.month_str)\n            self.date = int(self.date_str)\n            temp = temp.split(\"t\")[-1]\n            self.hour_str = temp[:2]\n            self.minute_str = temp[2:4]\n            self.second_str = temp[4:]\n            self.hour = int(self.hour_str)\n            self.minute = int(self.minute_str)\n            self.second = int(self.second_str)\n\n        # Normalize ship name\n        if \"ship_name\" in self.__dict__:\n            self.ship_name_unnormalized = self.ship_name\n            self.ship_name = utils.helpers.normalize_ship_name(self.ship_name)\n            # Get the NCEI formatted name if the data source is NCEI.\n            # This is basically a spell checker for NCEI.\n            if self.data_source == \"NCEI\":\n                self.ship_name_unnormalized = (\n                    utils.ncei_utils.get_closest_ncei_formatted_ship_name(\n                        ship_name=self.ship_name\n                    )\n                )\n        # If the ship name exists in ICES, get the ICES code for it.\n        if self.ship_name in self.valid_ICES_ship_names:\n            self.ices_code = ices_ship_names.get_ices_code_from_ship_name(\n                ship_name=self.ship_name, is_normalized=True\n            )\n        else:\n            self.ices_code = \"\"\n\n        # Take care of an empty file_download_directory and treat it like the\n        # cwd.\n        if (self.__dict__[\"file_download_directory\"] == \"\") or (\n            \"file_download_directory\" not in self.__dict__\n        ):\n            self.file_download_directory = \".\"\n\n        # Create connection objects if they dont exist\n        self.s3_bucket_name = \"noaa-wcsd-pds\"\n        if (\n            (\"gcp_bucket\" not in self.__dict__)\n            or (\"gcp_bucket_name\" not in self.__dict__)\n            or (\"gcp_stor_client\" not in self.__dict__)\n        ):\n            self.gcp_stor_client, self.gcp_bucket_name, self.gcp_bucket = (\n                utils.cloud_utils.setup_gcp_storage_objs(\n                    project_id=self.gcp_project_id,\n                    gcp_bucket_name=self.gcp_bucket_name,\n                )\n            )\n        if (\n            (\"s3_resource\" not in self.__dict__)\n            or (\"s3_client\" not in self.__dict__)\n            or (\"s3_bucket\" not in self.__dict__)\n        ):\n            self.s3_client, self.s3_resource, self.s3_bucket = (\n                utils.cloud_utils.create_s3_objs()\n            )\n\n        # Create file names for all other files that can exist\n        self.raw_file_name = self.file_name\n        self.file_name_wo_extension = self.file_name.split(\".\")[0]\n        self.idx_file_name = \".\".join(self.file_name.split(\".\")[:-1]) + \".idx\"\n        self.bot_file_name = \".\".join(self.file_name.split(\".\")[:-1]) + \".bot\"\n        self.netcdf_file_name = (\n            \".\".join(self.file_name.split(\".\")[:-1]) + \".nc\"\n        )\n\n        # Create download paths for all four types of files\n        self.raw_file_download_path = os.path.normpath(\n            os.sep.join([self.file_download_directory, self.file_name])\n        )\n        self.idx_file_download_path = os.path.normpath(\n            os.sep.join([self.file_download_directory, self.idx_file_name])\n        )\n        self.bot_file_download_path = os.path.normpath(\n            os.sep.join([self.file_download_directory, self.bot_file_name])\n        )\n        self.netcdf_file_download_path = os.path.normpath(\n            os.sep.join([self.file_download_directory, self.netcdf_file_name])\n        )\n\n        # Create all possible NCEI urls that can exist\n        # We have to use the un-normalized version of the ship name since\n        # NCEI does not normalize it.\n        self.raw_file_ncei_url = utils.helpers.create_ncei_url_from_variables(\n            file_name=self.raw_file_name,\n            ship_name=self.ship_name_unnormalized,\n            survey_name=self.survey_name,\n            echosounder=self.echosounder,\n        )\n        self.idx_file_ncei_url = utils.helpers.create_ncei_url_from_variables(\n            file_name=self.idx_file_name,\n            ship_name=self.ship_name_unnormalized,\n            survey_name=self.survey_name,\n            echosounder=self.echosounder,\n        )\n        self.bot_file_ncei_url = utils.helpers.create_ncei_url_from_variables(\n            file_name=self.bot_file_name,\n            ship_name=self.ship_name_unnormalized,\n            survey_name=self.survey_name,\n            echosounder=self.echosounder,\n        )\n        # NCEI does not store netcdf files, so we will not be creating a url\n        # for them.\n\n        # Create all GCP Storage bucket locations for each possible file\n        self.raw_gcp_storage_bucket_location = (\n            utils.helpers.parse_correct_gcp_storage_bucket_location(\n                file_name=self.raw_file_name,\n                file_type=\"raw\",\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n                data_source=self.data_source,\n                is_metadata=self.is_metadata,\n                debug=self.debug,\n            )\n        )\n        self.idx_gcp_storage_bucket_location = (\n            utils.helpers.parse_correct_gcp_storage_bucket_location(\n                file_name=self.idx_file_name,\n                file_type=\"idx\",\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n                data_source=self.data_source,\n                is_metadata=self.is_metadata,\n                debug=self.debug,\n            )\n        )\n        self.bot_gcp_storage_bucket_location = (\n            utils.helpers.parse_correct_gcp_storage_bucket_location(\n                file_name=self.bot_file_name,\n                file_type=\"bot\",\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n                data_source=self.data_source,\n                is_metadata=self.is_metadata,\n                debug=self.debug,\n            )\n        )\n        self.netcdf_gcp_storage_bucket_location = (\n            utils.helpers.parse_correct_gcp_storage_bucket_location(\n                file_name=self.netcdf_file_name,\n                file_type=\"netcdf\",\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n                data_source=self.data_source,\n                is_metadata=self.is_metadata,\n                debug=self.debug,\n            )\n        )\n\n        # Create all OMAO storage locations for each file.\n        self.raw_omao_file_path = (\n            utils.helpers.create_omao_file_path_from_variables(\n                self.raw_file_name,\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n            )\n        )\n        self.idx_omao_file_path = (\n            utils.helpers.create_omao_file_path_from_variables(\n                file_name=self.idx_file_name,\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n            )\n        )\n        self.bot_omao_file_path = (\n            utils.helpers.create_omao_file_path_from_variables(\n                file_name=self.bot_file_name,\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n            )\n        )\n        self.netcdf_omao_file_path = (\n            utils.helpers.create_omao_file_path_from_variables(\n                file_name=self.netcdf_file_name,\n                ship_name=self.ship_name,\n                survey_name=self.survey_name,\n                echosounder=self.echosounder,\n            )\n        )\n\n        # Create object keys for NCEI\n        self.raw_file_s3_object_key = utils.cloud_utils.get_object_key_for_s3(\n            file_name=self.raw_file_name,\n            ship_name=self.ship_name_unnormalized,\n            survey_name=self.survey_name,\n            echosounder=self.echosounder,\n        )\n        self.idx_file_s3_object_key = utils.cloud_utils.get_object_key_for_s3(\n            file_name=self.idx_file_name,\n            ship_name=self.ship_name_unnormalized,\n            survey_name=self.survey_name,\n            echosounder=self.echosounder,\n        )\n        self.bot_file_s3_object_key = utils.cloud_utils.get_object_key_for_s3(\n            file_name=self.bot_file_name,\n            ship_name=self.ship_name_unnormalized,\n            survey_name=self.survey_name,\n            echosounder=self.echosounder,\n        )\n        # NCEI does not have netcdf files, so we will not create object keys.\n\n        # Check if the file(s) exist in NCEI\n        self.raw_file_exists_in_ncei = (\n            utils.cloud_utils.check_if_file_exists_in_s3(\n                object_key=self.raw_file_s3_object_key,\n                s3_resource=self.s3_resource,\n                s3_bucket_name=self.s3_bucket_name,\n            )\n        )\n        self.idx_file_exists_in_ncei = (\n            utils.cloud_utils.check_if_file_exists_in_s3(\n                object_key=self.idx_file_s3_object_key,\n                s3_resource=self.s3_resource,\n                s3_bucket_name=self.s3_bucket_name,\n            )\n        )\n        self.bot_file_exists_in_ncei = (\n            utils.cloud_utils.check_if_file_exists_in_s3(\n                object_key=self.bot_file_s3_object_key,\n                s3_resource=self.s3_resource,\n                s3_bucket_name=self.s3_bucket_name,\n            )\n        )\n        # NCEI does not store netcdf files, so we will not be checking.\n\n        # Check if the file(s) exist in GCP\n        self.raw_file_exists_in_gcp = (\n            utils.cloud_utils.check_if_file_exists_in_gcp(\n                bucket=self.gcp_bucket,\n                file_path=self.raw_gcp_storage_bucket_location,\n            )\n        )\n        self.idx_file_exists_in_gcp = (\n            utils.cloud_utils.check_if_file_exists_in_gcp(\n                bucket=self.gcp_bucket,\n                file_path=self.idx_gcp_storage_bucket_location,\n            )\n        )\n        self.bot_file_exists_in_gcp = (\n            utils.cloud_utils.check_if_file_exists_in_gcp(\n                bucket=self.gcp_bucket,\n                file_path=self.bot_gcp_storage_bucket_location,\n            )\n        )\n        self.netcdf_file_exists_in_gcp = (\n            utils.cloud_utils.check_if_file_exists_in_gcp(\n                bucket=self.gcp_bucket,\n                file_path=self.netcdf_gcp_storage_bucket_location,\n            )\n        )\n\n        # TODO: create vars for omao data lake existence.\n\n    def _check_for_assertion_errors(self):\n        \"\"\"Checks for errors in each variable in our self.__dict__.\"\"\"\n\n        if \"file_name\" in self.__dict__:\n            assert self.file_name != \"\", (\n                \"Please provide a valid file name with the file extension\"\n                \" (ex. `2107RL_CW-D20210813-T220732.raw`)\"\n            )\n        if \"file_type\" in self.__dict__:\n            assert self.file_type != \"\", \"Please provide a valid file type.\"\n            assert self.file_type in config.VALID_FILETYPES, (\n                \"Please provide a valid file type (extension) \"\n                f\"from the following: {config.VALID_FILETYPES}\"\n            )\n        if \"ship_name\" in self.__dict__:\n            assert self.ship_name != \"\", (\n                \"Please provide a valid ship name \"\n                \"(Title_Case_With_Underscores_As_Spaces).\"\n            )\n            assert \" \" not in self.ship_name, (\n                \"Please provide a valid ship name \"\n                \"(Title_Case_With_Underscores_As_Spaces).\"\n            )\n            # Check for spell check using custom list\n            spell_check_list = get_close_matches(\n                self.ship_name, self.valid_ICES_ship_names, n=3, cutoff=0.6\n            )\n            if len(spell_check_list) &gt; 0:\n                assert self.ship_name in self.valid_ICES_ship_names, (\n                    f\"This `ship_name` {self.ship_name} does not\"\n                    \" exist in the ICES database. Did you mean one of the\"\n                    f\" following?\\n{spell_check_list}\"\n                )\n            else:\n                assert self.ship_name in self.valid_ICES_ship_names, (\n                    f\"This `ship_name` {self.ship_name} does not\"\n                    \" exist in the ICES database.\"\n                )\n        if \"survey_name\" in self.__dict__:\n            assert (\n                self.survey_name != \"\"\n            ), \"Please provide a valid survey name.\"\n        if \"echosounder\" in self.__dict__:\n            assert (\n                self.echosounder != \"\"\n            ), \"Please provide a valid echosounder.\"\n            assert self.echosounder in config.VALID_ECHOSOUNDERS, (\n                \"Please provide a valid echosounder from the \"\n                f\"following: {config.VALID_ECHOSOUNDERS}\"\n            )\n        if \"data_source\" in self.__dict__:\n            assert self.data_source != \"\", (\n                \"Please provide a valid data source from the \"\n                f\"following: {config.VALID_DATA_SOURCES}\"\n            )\n            assert self.data_source in config.VALID_DATA_SOURCES, (\n                \"Please provide a valid data source from the \"\n                f\"following: {config.VALID_DATA_SOURCES}\"\n            )\n        if \"file_download_directory\" in self.__dict__:\n            assert (\n                self.file_download_directory != \"\"\n            ), \"Please provide a valid file download location (a directory).\"\n            assert os.path.isdir(self.file_download_directory), (\n                f\"File download location `{self.file_download_directory}` is\"\n                \" not found to be a valid dir, please reformat it.\"\n            )\n        if \"gcp_bucket\" in self.__dict__:\n            assert self.gcp_bucket is not None, (\n                \"Please provide a gcp_bucket object with\"\n                \" `utils.cloud_utils.setup_gcp_storage()`\"\n            )\n\n    # TODO:\n    def _raw_file_exists_in_azure_data_lake(self): ...\n    def _idx_file_exists_in_azure_data_lake(self): ...\n    def _bot_file_exists_in_azure_data_lake(self): ...\n    def _netcdf_file_exists_in_azure_data_lake(self): ...\n\n    def get_str_times(self) -&gt; dict:\n        \"\"\"Gets the parsed times of the current file in dict format\n\n        Returns:\n            dict: An OrderedDict containing all of the data collection times\n                (based on name), for this file.\n        \"\"\"\n\n        temp_dict = OrderedDict(\n            [\n                (\"year\", self.year_str),\n                (\"month\", self.month_str),\n                (\"date\", self.date_str),\n                (\"hour\", self.hour_str),\n                (\"minute\", self.minute_str),\n                (\"second\", self.second_str),\n            ]\n        )\n        return temp_dict\n\n    def print_times(self) -&gt; str:\n        \"\"\"Prints the parsed times of the current file in dict format.\n\n        Returns:\n            str: The pretty print version of a string of the current file's\n                data collection datetime (based on file name).\n        \"\"\"\n\n        temp_dict = OrderedDict(\n            [\n                (\"year\", self.year),\n                (\"month\", self.month),\n                (\"date\", self.date),\n                (\"hour\", self.hour),\n                (\"minute\", self.minute),\n                (\"second\", self.second),\n            ]\n        )\n\n        return pprint.pformat(temp_dict, indent=4)\n\n    def get_file_datetime_str(self) -&gt; str:\n        \"\"\"Gets the datetime as a datetime formatted string.\n        Format: \"%Y-%m-%d %H:%M:%S\"\n\n        Returns:\n            str: The datetime formatted string.\n        \"\"\"\n\n        datetime_str = (\n            f\"{self.year_str}-{self.month_str}-{self.date_str} \"\n            f\"{self.hour_str}:{self.minute_str}:{self.second_str}\"\n        )\n        return datetime_str\n</code></pre>"},{"location":"documentation/raw_file/#aalibrary.raw_file.RawFile.get_file_datetime_str","title":"<code>get_file_datetime_str()</code>","text":"<p>Gets the datetime as a datetime formatted string. Format: \"%Y-%m-%d %H:%M:%S\"</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The datetime formatted string.</p> Source code in <code>src\\aalibrary\\raw_file.py</code> <pre><code>def get_file_datetime_str(self) -&gt; str:\n    \"\"\"Gets the datetime as a datetime formatted string.\n    Format: \"%Y-%m-%d %H:%M:%S\"\n\n    Returns:\n        str: The datetime formatted string.\n    \"\"\"\n\n    datetime_str = (\n        f\"{self.year_str}-{self.month_str}-{self.date_str} \"\n        f\"{self.hour_str}:{self.minute_str}:{self.second_str}\"\n    )\n    return datetime_str\n</code></pre>"},{"location":"documentation/raw_file/#aalibrary.raw_file.RawFile.get_str_times","title":"<code>get_str_times()</code>","text":"<p>Gets the parsed times of the current file in dict format</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>An OrderedDict containing all of the data collection times (based on name), for this file.</p> Source code in <code>src\\aalibrary\\raw_file.py</code> <pre><code>def get_str_times(self) -&gt; dict:\n    \"\"\"Gets the parsed times of the current file in dict format\n\n    Returns:\n        dict: An OrderedDict containing all of the data collection times\n            (based on name), for this file.\n    \"\"\"\n\n    temp_dict = OrderedDict(\n        [\n            (\"year\", self.year_str),\n            (\"month\", self.month_str),\n            (\"date\", self.date_str),\n            (\"hour\", self.hour_str),\n            (\"minute\", self.minute_str),\n            (\"second\", self.second_str),\n        ]\n    )\n    return temp_dict\n</code></pre>"},{"location":"documentation/raw_file/#aalibrary.raw_file.RawFile.print_times","title":"<code>print_times()</code>","text":"<p>Prints the parsed times of the current file in dict format.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The pretty print version of a string of the current file's data collection datetime (based on file name).</p> Source code in <code>src\\aalibrary\\raw_file.py</code> <pre><code>def print_times(self) -&gt; str:\n    \"\"\"Prints the parsed times of the current file in dict format.\n\n    Returns:\n        str: The pretty print version of a string of the current file's\n            data collection datetime (based on file name).\n    \"\"\"\n\n    temp_dict = OrderedDict(\n        [\n            (\"year\", self.year),\n            (\"month\", self.month),\n            (\"date\", self.date),\n            (\"hour\", self.hour),\n            (\"minute\", self.minute),\n            (\"second\", self.second),\n        ]\n    )\n\n    return pprint.pformat(temp_dict, indent=4)\n</code></pre>"},{"location":"documentation/utils/","title":"Documentation for <code>aalibrary.utils</code>","text":"<p>The <code>utils</code> submodule provides powerful and necessary functions for interacting with cloud providers. These functions include obtaining meta-metadata about the data files, such as how many files exist in a particular cruise in NCEI and more...</p> <p>Modules:</p> Name Description <code>cloud_utils</code> <p>This file contains all utility functions for Active Acoustics.</p> <code>discrepancies</code> <p>This file is used to identify discrepancies between what data exists on</p> <code>frequency_data</code> <p>This module contains the FrequencyData class.</p> <code>gcp_utils</code> <p>This file contains code pertaining to auxiliary functions related to parsing</p> <code>helpers</code> <p>For helper functions.</p> <code>ices</code> <code>nc_reader</code> <p>This file is used to get header information out of a NetCDF file. The</p> <code>ncei_utils</code> <p>This file contains code pertaining to auxiliary functions related to parsing</p> <code>sonar_checker</code> <code>timings</code> <p>\"This script deals with the times associated with ingesting/preprocessing</p>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils","title":"<code>cloud_utils</code>","text":"<p>This file contains all utility functions for Active Acoustics.</p> <p>Functions:</p> Name Description <code>bq_query_to_pandas</code> <p>Takes a SQL query and returns the end result as a DataFrame.</p> <code>check_existence_of_supplemental_files</code> <p>Checks the existence of supplemental files (idx, bot, etc.) for a raw</p> <code>check_if_file_exists_in_gcp</code> <p>Checks whether a particular file exists in GCP using the file path</p> <code>check_if_file_exists_in_s3</code> <p>Checks to see if a file exists in an s3 bucket. Intended for use with</p> <code>check_if_netcdf_file_exists_in_gcp</code> <p>Checks if a netcdf file exists in GCP storage. If the bucket location is</p> <code>count_objects_in_s3_bucket_location</code> <p>Counts the number of objects within a bucket location.</p> <code>count_subdirectories_in_s3_bucket_location</code> <p>Counts the number of subdirectories within a bucket location.</p> <code>create_s3_objs</code> <p>Creates the s3 objects needed for using boto3 for a particular bucket.</p> <code>delete_file_from_gcp</code> <p>Deletes a file from the storage bucket.</p> <code>download_file_from_gcp</code> <p>Downloads a file from the blob storage bucket.</p> <code>download_file_from_gcp_as_string</code> <p>Downloads a file from the blob storage bucket as a text string.</p> <code>get_data_lake_directory_client</code> <p>Creates a data lake directory client. Returns an object of type</p> <code>get_object_key_for_s3</code> <p>Creates an object key for a file within s3 given the parameters above.</p> <code>get_service_client_sas</code> <p>Gets an azure service client using an SAS (shared access signature)</p> <code>get_subdirectories_in_s3_bucket_location</code> <p>Gets a list of all the subdirectories in a specific bucket location</p> <code>list_all_folders_in_gcp_bucket_location</code> <p>Lists all of the folders in a GCP storage bucket location.</p> <code>list_all_objects_in_gcp_bucket_location</code> <p>Gets all of the files within a GCP storage bucket location.</p> <code>list_all_objects_in_s3_bucket_location</code> <p>Lists all of the objects in a s3 bucket location denoted by <code>prefix</code>.</p> <code>setup_gbq_client_objs</code> <p>Sets up Google Big Query client objects used to execute queries and</p> <code>setup_gcp_storage_objs</code> <p>Sets up Google Cloud Platform storage objects for use in accessing and</p> <code>upload_file_to_gcp_bucket</code> <p>Uploads a file to the blob storage bucket.</p>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.bq_query_to_pandas","title":"<code>bq_query_to_pandas(client=None, query='')</code>","text":"<p>Takes a SQL query and returns the end result as a DataFrame.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def bq_query_to_pandas(client: bigquery.Client = None, query: str = \"\"):\n    \"\"\"Takes a SQL query and returns the end result as a DataFrame.\"\"\"\n\n    job = client.query(query)\n    return job.result().to_dataframe()\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.check_existence_of_supplemental_files","title":"<code>check_existence_of_supplemental_files(file_name='', file_type='raw', ship_name='', survey_name='', echosounder='', debug=False)</code>","text":"<p>Checks the existence of supplemental files (idx, bot, etc.) for a raw file. Will check for existence in all data sources.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (do not include the dot \".\"). Defaults to \"\".</p> <code>'raw'</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>RawFile</code> <code>RawFile</code> <p>Returns a RawFile object, existence can be accessed as a boolean via the variable within. Ex. rf.idx_file_exists_in_ncei</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def check_existence_of_supplemental_files(\n    file_name: str = \"\",\n    file_type: str = \"raw\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    debug: bool = False,\n) -&gt; RawFile:\n    \"\"\"Checks the existence of supplemental files (idx, bot, etc.) for a raw\n    file. Will check for existence in all data sources.\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (do not include the dot \".\").\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier. Defaults\n            to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n\n    Returns:\n        RawFile: Returns a RawFile object, existence can be accessed as a\n            boolean via the variable within.\n            Ex. rf.idx_file_exists_in_ncei\n    \"\"\"\n\n    # Create connection vars\n    gcp_stor_client, gcp_bucket_name, gcp_bucket = setup_gcp_storage_objs()\n    _, s3_resource, _ = create_s3_objs()\n\n    # Create the RawFile object.\n    rf = RawFile(\n        file_name=file_name,\n        file_type=file_type,\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        debug=debug,\n        gcp_bucket=gcp_bucket,\n        gcp_bucket_name=gcp_bucket_name,\n        gcp_stor_client=gcp_stor_client,\n        s3_resource=s3_resource,\n    )\n\n    return rf\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.check_if_file_exists_in_gcp","title":"<code>check_if_file_exists_in_gcp(bucket=None, file_path='')</code>","text":"<p>Checks whether a particular file exists in GCP using the file path (blob).</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>Bucket</code> <p>The bucket object used to check for the file. Defaults to None.</p> <code>None</code> <code>file_path</code> <code>str</code> <p>The blob file path within the bucket. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>Bool</code> <code>bool</code> <p>True if the file already exists, False otherwise.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def check_if_file_exists_in_gcp(\n    bucket: storage.Bucket = None, file_path: str = \"\"\n) -&gt; bool:\n    \"\"\"Checks whether a particular file exists in GCP using the file path\n    (blob).\n\n    Args:\n        bucket (storage.Bucket, optional): The bucket object used to check for\n            the file. Defaults to None.\n        file_path (str, optional): The blob file path within the bucket.\n            Defaults to \"\".\n\n    Returns:\n        Bool: True if the file already exists, False otherwise.\n    \"\"\"\n\n    return bucket.blob(file_path).exists()\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.check_if_file_exists_in_s3","title":"<code>check_if_file_exists_in_s3(object_key='', s3_resource=None, s3_bucket_name='')</code>","text":"<p>Checks to see if a file exists in an s3 bucket. Intended for use with NCEI, but will work with other s3 buckets as well.</p> <p>Parameters:</p> Name Type Description Default <code>object_key</code> <code>str</code> <p>The object key (location of the object). Defaults to \"\".</p> <code>''</code> <code>s3_resource</code> <code>resource</code> <p>The boto3 resource for this particular bucket. Defaults to None.</p> <code>None</code> <code>s3_bucket_name</code> <code>str</code> <p>The bucket name. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file exists within the bucket. False otherwise.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def check_if_file_exists_in_s3(\n    object_key: str = \"\",\n    s3_resource: boto3.resource = None,\n    s3_bucket_name: str = \"\",\n) -&gt; bool:\n    \"\"\"Checks to see if a file exists in an s3 bucket. Intended for use with\n    NCEI, but will work with other s3 buckets as well.\n\n    Args:\n        object_key (str, optional): The object key (location of the object).\n            Defaults to \"\".\n        s3_resource (boto3.resource, optional): The boto3 resource for this\n            particular bucket. Defaults to None.\n        s3_bucket_name (str, optional): The bucket name. Defaults to \"\".\n\n    Returns:\n        bool: True if the file exists within the bucket. False otherwise.\n    \"\"\"\n\n    try:\n        s3_resource.Object(s3_bucket_name, object_key).load()\n        return True\n    except Exception:\n        # object key does not exist.\n        # print(e)\n        return False\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.check_if_netcdf_file_exists_in_gcp","title":"<code>check_if_netcdf_file_exists_in_gcp(file_name='', ship_name='', survey_name='', echosounder='', data_source='', gcp_storage_bucket_location='', gcp_bucket=None, debug=False)</code>","text":"<p>Checks if a netcdf file exists in GCP storage. If the bucket location is not specified, it will use the helpers to parse the correct location.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the file. Necessary due to the way the storage bucket is organized. Can be one of [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".</p> <code>''</code> <code>gcp_storage_bucket_location</code> <code>str</code> <p>The string representing the blob's location within the storage bucket. Defaults to \"\".</p> <code>''</code> <code>gcp_bucket</code> <code>Bucket</code> <p>The bucket object used for downloading.</p> <code>None</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the file exists in GCP, False otherwise.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def check_if_netcdf_file_exists_in_gcp(\n    file_name: str = \"\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"\",\n    gcp_storage_bucket_location: str = \"\",\n    gcp_bucket: storage.Bucket = None,\n    debug: bool = False,\n) -&gt; bool:\n    \"\"\"Checks if a netcdf file exists in GCP storage. If the bucket location is\n    not specified, it will use the helpers to parse the correct location.\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier.\n            Defaults to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the file. Necessary due to\n            the way the storage bucket is organized. Can be one of\n            [\"NCEI\", \"OMAO\", \"HDD\"]. Defaults to \"\".\n        gcp_storage_bucket_location (str, optional): The string representing\n            the blob's location within the storage bucket. Defaults to \"\".\n        gcp_bucket (storage.Bucket): The bucket object used for downloading.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n\n    Returns:\n        bool: True if the file exists in GCP, False otherwise.\n    \"\"\"\n\n    if gcp_storage_bucket_location != \"\":\n        gcp_storage_bucket_location = (\n            helpers.parse_correct_gcp_storage_bucket_location(\n                file_name=file_name,\n                file_type=\"netcdf\",\n                survey_name=survey_name,\n                ship_name=ship_name,\n                echosounder=echosounder,\n                data_source=data_source,\n                is_metadata=False,\n                debug=debug,\n            )\n        )\n    netcdf_gcp_storage_bucket_location = (\n        get_netcdf_gcp_location_from_raw_gcp_location(\n            gcp_storage_bucket_location=gcp_storage_bucket_location\n        )\n    )\n    # check if the file exists in gcp\n    return check_if_file_exists_in_gcp(\n        bucket=gcp_bucket, file_path=netcdf_gcp_storage_bucket_location\n    )\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.count_objects_in_s3_bucket_location","title":"<code>count_objects_in_s3_bucket_location(prefix='', bucket=None)</code>","text":"<p>Counts the number of objects within a bucket location. NOTE: This DOES NOT include folders, as those do not count as objects.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The bucket location. Defaults to \"\".</p> <code>''</code> <code>bucket</code> <code>resource</code> <p>The bucket resource object. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of objects within the location.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def count_objects_in_s3_bucket_location(\n    prefix: str = \"\", bucket: boto3.resource = None\n) -&gt; int:\n    \"\"\"Counts the number of objects within a bucket location.\n    NOTE: This DOES NOT include folders, as those do not count as objects.\n\n    Args:\n        prefix (str, optional): The bucket location. Defaults to \"\".\n        bucket (boto3.resource, optional): The bucket resource object.\n            Defaults to None.\n\n    Returns:\n        int: The count of objects within the location.\n    \"\"\"\n\n    count = sum(1 for _ in bucket.objects.filter(Prefix=prefix).all())\n    return count\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.count_subdirectories_in_s3_bucket_location","title":"<code>count_subdirectories_in_s3_bucket_location(prefix='', bucket=None)</code>","text":"<p>Counts the number of subdirectories within a bucket location.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The bucket location. Defaults to \"\".</p> <code>''</code> <code>bucket</code> <code>resource</code> <p>The bucket resource object. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The count of subdirectories within the location.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def count_subdirectories_in_s3_bucket_location(\n    prefix: str = \"\", bucket: boto3.resource = None\n) -&gt; int:\n    \"\"\"Counts the number of subdirectories within a bucket location.\n\n    Args:\n        prefix (str, optional): The bucket location. Defaults to \"\".\n        bucket (boto3.resource, optional): The bucket resource object.\n            Defaults to None.\n\n    Returns:\n        int: The count of subdirectories within the location.\n    \"\"\"\n\n    subdirs = set()\n    for obj in bucket.objects.filter(Prefix=prefix):\n        prefix = \"/\".join(obj.key.split(\"/\")[:-1])\n        if prefix and prefix not in subdirs:\n            subdirs.add(prefix)\n            # print(prefix + \"/\")\n    return len(subdirs)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.create_s3_objs","title":"<code>create_s3_objs(bucket_name='noaa-wcsd-pds')</code>","text":"<p>Creates the s3 objects needed for using boto3 for a particular bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The bucket you want to refer to. The default points to the NCEI bucket. Defaults to \"noaa-wcsd-pds\".</p> <code>'noaa-wcsd-pds'</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple</code> <p>The s3 client (used for certain portions of the boto3 api), the s3 resource (newer, more used object for accessing s3 buckets), and the actual s3 bucket itself.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def create_s3_objs(bucket_name: str = \"noaa-wcsd-pds\") -&gt; Tuple:\n    \"\"\"Creates the s3 objects needed for using boto3 for a particular bucket.\n\n    Args:\n        bucket_name (str, optional): The bucket you want to refer to. The\n            default points to the NCEI bucket. Defaults to \"noaa-wcsd-pds\".\n\n    Returns:\n        Tuple: The s3 client (used for certain portions of the boto3 api), the\n            s3 resource (newer, more used object for accessing s3 buckets), and\n            the actual s3 bucket itself.\n    \"\"\"\n\n    # Setup access to S3 bucket as an anonymous user\n    s3_client = boto3.client(\n        \"s3\",\n        aws_access_key_id=\"\",\n        aws_secret_access_key=\"\",\n        config=Config(signature_version=UNSIGNED),\n    )\n    s3_resource = boto3.resource(\n        \"s3\",\n        aws_access_key_id=\"\",\n        aws_secret_access_key=\"\",\n        config=Config(signature_version=UNSIGNED),\n    )\n\n    s3_bucket = s3_resource.Bucket(bucket_name)\n\n    return s3_client, s3_resource, s3_bucket\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.delete_file_from_gcp","title":"<code>delete_file_from_gcp(gcp_bucket, blob_file_path)</code>","text":"<p>Deletes a file from the storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>gcp_bucket</code> <code>bucket</code> <p>The bucket object used for downloading from.</p> required <code>blob_file_path</code> <code>str</code> <p>The blob's file path. Ex. \"data/itds/logs/execute_rasp_ii/temp.csv\" NOTE: This must include the file name as well as the extension.</p> required <p>Raises:     AssertionError: If the file does not exist in GCP.     Exception: If there is an error deleting the file.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def delete_file_from_gcp(\n    gcp_bucket: storage.Client.bucket, blob_file_path: str\n):\n    \"\"\"Deletes a file from the storage bucket.\n\n    Args:\n        gcp_bucket (storage.Client.bucket): The bucket object used for\n            downloading from.\n        blob_file_path (str): The blob's file path.\n            Ex. \"data/itds/logs/execute_rasp_ii/temp.csv\"\n            NOTE: This must include the file name as well as the extension.\n    Raises:\n        AssertionError: If the file does not exist in GCP.\n        Exception: If there is an error deleting the file.\n    \"\"\"\n\n    file_exists_in_gcp = check_if_file_exists_in_gcp(\n        gcp_bucket, blob_file_path\n    )\n    assert (\n        file_exists_in_gcp\n    ), f\"File does not exist in GCP at `{blob_file_path}`.\"\n\n    blob = gcp_bucket.blob(blob_file_path)\n    try:\n        blob.delete()\n        return\n    except Exception:\n        print(traceback.format_exc())\n        raise\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.download_file_from_gcp","title":"<code>download_file_from_gcp(gcp_bucket, blob_file_path, local_file_path, debug=False)</code>","text":"<p>Downloads a file from the blob storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>gcp_bucket</code> <code>bucket</code> <p>The bucket object used for downloading from.</p> required <code>blob_file_path</code> <code>str</code> <p>The blob's file path. Ex. \"data/itds/logs/execute_rasp_ii/temp.csv\" NOTE: This must include the file name as well as the extension.</p> required <code>local_file_path</code> <code>str</code> <p>The local file path you wish to download the blob to.</p> required <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements.</p> <code>False</code> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def download_file_from_gcp(\n    gcp_bucket: storage.Client.bucket,\n    blob_file_path: str,\n    local_file_path: str,\n    debug: bool = False,\n):\n    \"\"\"Downloads a file from the blob storage bucket.\n\n    Args:\n        gcp_bucket (storage.Client.bucket): The bucket object used for\n            downloading from.\n        blob_file_path (str): The blob's file path.\n            Ex. \"data/itds/logs/execute_rasp_ii/temp.csv\"\n            NOTE: This must include the file name as well as the extension.\n        local_file_path (str): The local file path you wish to download the\n            blob to.\n        debug (bool): Whether or not to print debug statements.\n    \"\"\"\n\n    blob = gcp_bucket.blob(blob_file_path, chunk_size=1024 * 1024 * 1)\n    # Download from blob\n    try:\n        blob.download_to_filename(local_file_path)\n        if debug:\n            print(f\"New data downloaded to {local_file_path}\")\n    except Exception:\n        print(traceback.format_exc())\n        raise\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.download_file_from_gcp_as_string","title":"<code>download_file_from_gcp_as_string(gcp_bucket, blob_file_path)</code>","text":"<p>Downloads a file from the blob storage bucket as a text string.</p> <p>Parameters:</p> Name Type Description Default <code>gcp_bucket</code> <code>bucket</code> <p>The bucket object used for downloading from.</p> required <code>blob_file_path</code> <code>str</code> <p>The blob's file path. Ex. \"data/itds/logs/execute_rasp_ii/temp.csv\" NOTE: This must include the file name as well as the extension.</p> required Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def download_file_from_gcp_as_string(\n    gcp_bucket: storage.Client.bucket,\n    blob_file_path: str,\n):\n    \"\"\"Downloads a file from the blob storage bucket as a text string.\n\n    Args:\n        gcp_bucket (storage.Client.bucket): The bucket object used for\n            downloading from.\n        blob_file_path (str): The blob's file path.\n            Ex. \"data/itds/logs/execute_rasp_ii/temp.csv\"\n            NOTE: This must include the file name as well as the extension.\n    \"\"\"\n\n    blob = gcp_bucket.blob(blob_file_path, chunk_size=1024 * 1024 * 1)\n    # Download from blob\n    try:\n        return blob.download_as_text(encoding='utf-8')\n    except Exception:\n        print(traceback.format_exc())\n        raise\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.get_data_lake_directory_client","title":"<code>get_data_lake_directory_client(config_file_path='')</code>","text":"<p>Creates a data lake directory client. Returns an object of type DataLakeServiceClient.</p> <p>Parameters:</p> Name Type Description Default <code>config_file_path</code> <code>str</code> <p>The location of the config file. Needs a <code>[DEFAULT]</code> section with a <code>azure_connection_string</code> variable defined. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>DataLakeServiceClient</code> <code>DataLakeServiceClient</code> <p>An object of type DataLakeServiceClient, with connection to the connection string described in the config.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def get_data_lake_directory_client(\n    config_file_path: str = \"\",\n) -&gt; DataLakeServiceClient:\n    \"\"\"Creates a data lake directory client. Returns an object of type\n    DataLakeServiceClient.\n\n    Args:\n        config_file_path (str, optional): The location of the config file.\n            Needs a `[DEFAULT]` section with a `azure_connection_string`\n            variable defined. Defaults to \"\".\n\n    Returns:\n        DataLakeServiceClient: An object of type DataLakeServiceClient, with\n            connection to the connection string described in the config.\n    \"\"\"\n\n    config = configparser.ConfigParser()\n    config.read(config_file_path)\n\n    azure_service = DataLakeServiceClient.from_connection_string(\n        conn_str=config[\"DEFAULT\"][\"azure_connection_string\"]\n    )\n\n    return azure_service\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.get_object_key_for_s3","title":"<code>get_object_key_for_s3(file_url='', file_name='', ship_name='', survey_name='', echosounder='')</code>","text":"<p>Creates an object key for a file within s3 given the parameters above.</p> <p>Parameters:</p> Name Type Description Default <code>file_url</code> <code>str</code> <p>The entire url to the file resource in s3. Starts with \"https://\" or \"s3://\". Defaults to \"\". NOTE: If this is specified, there is no need to provide the other parameters.</p> <code>''</code> <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def get_object_key_for_s3(\n    file_url: str = \"\",\n    file_name: str = \"\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n):\n    \"\"\"Creates an object key for a file within s3 given the parameters above.\n\n    Args:\n        file_url (str, optional): The entire url to the file resource in s3.\n            Starts with \"https://\" or \"s3://\". Defaults to \"\".\n            NOTE: If this is specified, there is no need to provide the other\n            parameters.\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier.\n            Defaults to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n    \"\"\"\n\n    if file_url:\n        # We replace the beginning of common file paths\n        file_url = file_url.replace(\n            \"https://noaa-wcsd-pds.s3.amazonaws.com/\", \"\"\n        )\n        file_url = file_url.replace(\"s3://noaa-wcsd-pds/\", \"\")\n        return file_url\n    else:\n        # We default to using the parameters to create an object key according\n        # to NCEI standards.\n        object_key = (\n            f\"data/raw/{ship_name}/{survey_name}/{echosounder}/{file_name}\"\n        )\n        return object_key\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.get_service_client_sas","title":"<code>get_service_client_sas(account_name, sas_token)</code>","text":"<p>Gets an azure service client using an SAS (shared access signature) token. The token must be created in Azure.</p> <p>Parameters:</p> Name Type Description Default <code>account_name</code> <code>str</code> <p>The name of the account you are trying to create a service client with. This is usually a storage account that is attached to the container.</p> required <code>sas_token</code> <code>str</code> <p>The complete SAS token.</p> required <p>Returns:</p> Name Type Description <code>DataLakeServiceClient</code> <code>DataLakeServiceClient</code> <p>An object of type DataLakeServiceClient, with connection to the container/file the SAS allows access to.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def get_service_client_sas(\n    account_name: str, sas_token: str\n) -&gt; DataLakeServiceClient:\n    \"\"\"Gets an azure service client using an SAS (shared access signature)\n    token. The token must be created in Azure.\n\n    Args:\n        account_name (str): The name of the account you are trying to create a\n            service client with. This is usually a storage account that is\n            attached to the container.\n        sas_token (str): The complete SAS token.\n\n    Returns:\n        DataLakeServiceClient: An object of type DataLakeServiceClient, with\n            connection to the container/file the SAS allows access to.\n    \"\"\"\n    account_url = f\"https://{account_name}.dfs.core.windows.net\"\n\n    # The SAS token string can be passed in as credential param or appended to\n    # the account URL\n    service_client = DataLakeServiceClient(account_url, credential=sas_token)\n\n    return service_client\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.get_subdirectories_in_s3_bucket_location","title":"<code>get_subdirectories_in_s3_bucket_location(prefix='', s3_client=None, return_full_paths=False, bucket_name='noaa-wcsd-pds')</code>","text":"<p>Gets a list of all the subdirectories in a specific bucket location (called a prefix). The return can be with full paths (root to folder inclusive), or just the folder names.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The bucket folder location. Defaults to \"\".</p> <code>''</code> <code>s3_client</code> <code>client</code> <p>The bucket client object. Defaults to None.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <code>bucket_name</code> <code>str</code> <p>The bucket name. Defaults to \"noaa-wcsd-pds\".</p> <code>'noaa-wcsd-pds'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being the subdirectory. Whether these are full paths or just folder names are specified by the <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def get_subdirectories_in_s3_bucket_location(\n    prefix: str = \"\",\n    s3_client: boto3.client = None,\n    return_full_paths: bool = False,\n    bucket_name: str = \"noaa-wcsd-pds\",\n) -&gt; List[str]:\n    \"\"\"Gets a list of all the subdirectories in a specific bucket location\n    (called a prefix). The return can be with full paths (root to folder\n    inclusive), or just the folder names.\n\n    Args:\n        prefix (str, optional): The bucket folder location. Defaults to \"\".\n        s3_client (boto3.client, optional): The bucket client object.\n            Defaults to None.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n        bucket_name (str, optional): The bucket name. Defaults to\n            \"noaa-wcsd-pds\".\n\n    Returns:\n        List[str]: A list of strings, each being the subdirectory. Whether\n            these are full paths or just folder names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n    if not s3_client:\n        s3_client, _, _ = create_s3_objs(bucket_name)\n\n    subdirs = set()\n    result = s3_client.list_objects(\n        Bucket=bucket_name, Prefix=prefix, Delimiter=\"/\"\n    )\n    for o in result.get(\"CommonPrefixes\"):\n        subdir_full_path_from_prefix = o.get(\"Prefix\")\n        if return_full_paths:\n            subdir = subdir_full_path_from_prefix\n        else:\n            subdir = subdir_full_path_from_prefix.replace(prefix, \"\")\n            subdir = subdir.replace(\"/\", \"\")\n        subdirs.add(subdir)\n    return list(subdirs)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.list_all_folders_in_gcp_bucket_location","title":"<code>list_all_folders_in_gcp_bucket_location(location='', gcp_bucket=None, return_full_paths=True)</code>","text":"<p>Lists all of the folders in a GCP storage bucket location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>The blob location you would like to get the folders of. Defaults to \"\".</p> <code>''</code> <code>gcp_bucket</code> <code>bucket</code> <p>The gcp bucket to use. Defaults to None.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not to return full paths. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the folder names or full paths.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def list_all_folders_in_gcp_bucket_location(\n    location: str = \"\",\n    gcp_bucket: storage.Client.bucket = None,\n    return_full_paths: bool = True,\n) -&gt; List[str]:\n    \"\"\"Lists all of the folders in a GCP storage bucket location.\n\n    Args:\n        location (str, optional): The blob location you would like to get the\n            folders of. Defaults to \"\".\n        gcp_bucket (storage.Client.bucket, optional): The gcp bucket to use.\n            Defaults to None.\n        return_full_paths (bool, optional): Whether or not to return full\n            paths. Defaults to True.\n\n    Returns:\n        List[str]: A list of strings containing the folder names or full paths.\n    \"\"\"\n\n    if location and not location.endswith(\"/\"):\n        location += \"/\"\n\n    blobs_iterator = gcp_bucket.list_blobs(prefix=location, delimiter=\"/\")\n\n    folder_prefixes = []\n    # We MUST iterate through all blobs, since this is a lazy-loading iterator.\n    for _ in blobs_iterator:\n        ...\n\n    if blobs_iterator.prefixes:\n        for p in blobs_iterator.prefixes:\n            folder_prefixes.append(p)\n\n    if return_full_paths:\n        return folder_prefixes\n    else:\n        return [b.split(\"/\")[-2] for b in folder_prefixes]\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.list_all_objects_in_gcp_bucket_location","title":"<code>list_all_objects_in_gcp_bucket_location(location='', gcp_bucket=None)</code>","text":"<p>Gets all of the files within a GCP storage bucket location.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>The location to search for files. Defaults to \"\". Ex. \"NCEI/Reuben_Lasker/RL2107\"</p> <code>''</code> <code>gcp_bucket</code> <code>bucket</code> <p>The gcp bucket to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing all URIs for each file in the bucket.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def list_all_objects_in_gcp_bucket_location(\n    location: str = \"\", gcp_bucket: storage.Client.bucket = None\n) -&gt; List[str]:\n    \"\"\"Gets all of the files within a GCP storage bucket location.\n\n    Args:\n        location (str, optional): The location to search for files. Defaults\n            to \"\".\n            Ex. \"NCEI/Reuben_Lasker/RL2107\"\n        gcp_bucket (storage.Client.bucket, optional): The gcp bucket to use.\n            Defaults to None.\n\n    Returns:\n        List[str]: A list of strings containing all URIs for each file in the\n            bucket.\n    \"\"\"\n\n    all_blobs_in_this_location = []\n    for blob in gcp_bucket.list_blobs(prefix=location):\n        all_blobs_in_this_location.append(blob.name)\n    return all_blobs_in_this_location\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.list_all_objects_in_s3_bucket_location","title":"<code>list_all_objects_in_s3_bucket_location(prefix='', s3_resource=None, return_full_paths=False, bucket_name='noaa-wcsd-pds')</code>","text":"<p>Lists all of the objects in a s3 bucket location denoted by <code>prefix</code>. Returns a list containing str. You get full paths if you specify the <code>return_full_paths</code> parameter.</p> <p>Parameters:</p> Name Type Description Default <code>prefix</code> <code>str</code> <p>The bucket location. Defaults to \"\".</p> <code>''</code> <code>s3_resource</code> <code>resource</code> <p>The bucket resource object. Defaults to None.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <code>bucket_name</code> <code>str</code> <p>The bucket name. Defaults to \"noaa-wcsd-pds\".</p> <code>'noaa-wcsd-pds'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing either the objects name or path, dependent on the <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def list_all_objects_in_s3_bucket_location(\n    prefix: str = \"\",\n    s3_resource: boto3.resource = None,\n    return_full_paths: bool = False,\n    bucket_name: str = \"noaa-wcsd-pds\",\n) -&gt; List[str]:\n    \"\"\"Lists all of the objects in a s3 bucket location denoted by `prefix`.\n    Returns a list containing str. You get full paths if you specify the\n    `return_full_paths` parameter.\n\n    Args:\n        prefix (str, optional): The bucket location. Defaults to \"\".\n        s3_resource (boto3.resource, optional): The bucket resource object.\n            Defaults to None.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n        bucket_name (str, optional): The bucket name. Defaults to\n            \"noaa-wcsd-pds\".\n\n    Returns:\n        List[str]: A list of strings containing either the objects name or\n            path, dependent on the `return_full_paths` parameter.\n    \"\"\"\n    if not s3_resource:\n        _, s3_resource, _ = create_s3_objs(bucket_name)\n\n    object_keys = set()\n    bucket = s3_resource.Bucket(bucket_name)\n    for obj in bucket.objects.filter(Prefix=prefix):\n        if return_full_paths:\n            object_keys.add(obj.key)\n        else:\n            object_keys.add(obj.key.split(\"/\")[-1])\n\n    return list(object_keys)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.setup_gbq_client_objs","title":"<code>setup_gbq_client_objs(location='US', project_id='ggn-nmfs-aa-dev-1')</code>","text":"<p>Sets up Google Big Query client objects used to execute queries and such.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>str</code> <p>The location of the big-query tables/database. This is usually set when creating the database in big query. Defaults to \"US\".</p> <code>'US'</code> <code>project_id</code> <code>str</code> <p>The project id that the big query instance belongs to. Defaults to \"ggn-nmfs-aa-dev-1\".</p> <code>'ggn-nmfs-aa-dev-1'</code> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple[Client, GCSFileSystem]</code> <p>The big query client object, along with an object for the Google Cloud Storage file system.</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def setup_gbq_client_objs(\n    location: str = \"US\", project_id: str = \"ggn-nmfs-aa-dev-1\"\n) -&gt; Tuple[bigquery.Client, gcsfs.GCSFileSystem]:\n    \"\"\"Sets up Google Big Query client objects used to execute queries and\n    such.\n\n    Args:\n        location (str, optional): The location of the big-query\n            tables/database. This is usually set when creating the database in\n            big query. Defaults to \"US\".\n        project_id (str, optional): The project id that the big query instance\n            belongs to. Defaults to \"ggn-nmfs-aa-dev-1\".\n\n    Returns:\n        Tuple: The big query client object, along with an object for the Google\n            Cloud Storage file system.\n    \"\"\"\n\n    gcp_bq_client = bigquery.Client(location=location)\n\n    gcp_gcs_file_system = gcsfs.GCSFileSystem(project=project_id)\n\n    return gcp_bq_client, gcp_gcs_file_system\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.setup_gcp_storage_objs","title":"<code>setup_gcp_storage_objs(project_id='ggn-nmfs-aa-dev-1', gcp_bucket_name='ggn-nmfs-aa-dev-1-data')</code>","text":"<p>Sets up Google Cloud Platform storage objects for use in accessing and modifying storage buckets.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>The project id of the project you want to access. Defaults to \"ggn-nmfs-aa-dev-1\".</p> <code>'ggn-nmfs-aa-dev-1'</code> <code>gcp_bucket_name</code> <code>str</code> <p>The name of the exact bucket you want to access. Defaults to \"ggn-nmfs-aa-dev-1-data\".</p> <code>'ggn-nmfs-aa-dev-1-data'</code> <p>Returns:</p> Type Description <code>Tuple[Client, str, bucket]</code> <p>Tuple[storage.Client, str, storage.Client.bucket]: The storage client, followed by the GCP bucket name (str) and then the actual bucket object itself (which will be executing the commands used in this api).</p> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def setup_gcp_storage_objs(\n    project_id: str = \"ggn-nmfs-aa-dev-1\",\n    gcp_bucket_name: str = \"ggn-nmfs-aa-dev-1-data\",\n) -&gt; Tuple[storage.Client, str, storage.Client.bucket]:\n    \"\"\"Sets up Google Cloud Platform storage objects for use in accessing and\n    modifying storage buckets.\n\n    Args:\n        project_id (str, optional): The project id of the project you want to\n            access. Defaults to \"ggn-nmfs-aa-dev-1\".\n        gcp_bucket_name (str, optional): The name of the exact bucket you want\n            to access. Defaults to \"ggn-nmfs-aa-dev-1-data\".\n\n    Returns:\n        Tuple[storage.Client, str, storage.Client.bucket]: The storage client,\n            followed by the GCP bucket name (str) and then the actual bucket\n            object itself (which will be executing the commands used in this\n            api).\n    \"\"\"\n\n    gcp_stor_client = storage.Client(project=project_id)\n\n    gcp_bucket = gcp_stor_client.bucket(gcp_bucket_name)\n\n    return (gcp_stor_client, gcp_bucket_name, gcp_bucket)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.cloud_utils.upload_file_to_gcp_bucket","title":"<code>upload_file_to_gcp_bucket(bucket, blob_file_path, local_file_path, debug=False)</code>","text":"<p>Uploads a file to the blob storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>bucket</code> <p>The bucket object used for uploading.</p> required <code>blob_file_path</code> <code>str</code> <p>The blob's file path. Ex. \"data/itds/logs/execute_code_files/temp.csv\" NOTE: This must include the file name as well as the extension.</p> required <code>local_file_path</code> <code>str</code> <p>The local file path you wish to upload to the blob.</p> required <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements.</p> <code>False</code> Source code in <code>src\\aalibrary\\utils\\cloud_utils.py</code> <pre><code>def upload_file_to_gcp_bucket(\n    bucket: storage.Client.bucket,\n    blob_file_path: str,\n    local_file_path: str,\n    debug: bool = False,\n):\n    \"\"\"Uploads a file to the blob storage bucket.\n\n    Args:\n        bucket (storage.Client.bucket): The bucket object used for uploading.\n        blob_file_path (str): The blob's file path.\n            Ex. \"data/itds/logs/execute_code_files/temp.csv\"\n            NOTE: This must include the file name as well as the extension.\n        local_file_path (str): The local file path you wish to upload to the\n            blob.\n        debug (bool): Whether or not to print debug statements.\n    \"\"\"\n\n    if not bucket:\n        _, _, bucket = setup_gcp_storage_objs()\n\n    blob = bucket.blob(blob_file_path, chunk_size=1024 * 1024 * 1)\n    # Upload a new blob\n    try:\n        blob.upload_from_filename(local_file_path)\n        if debug:\n            print(f\"New data uploaded to {blob.name}\")\n    except Exception:\n        print(traceback.format_exc())\n        raise\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.discrepancies","title":"<code>discrepancies</code>","text":"<p>This file is used to identify discrepancies between what data exists on local versus what exists on the cloud. It considers the following things when comparing: * Number of files per cruise * File Name/Types * File Sizes * Checksum</p> <p>Functions:</p> Name Description <code>compare_local_cruise_files_to_cloud</code> <p>Compares the locally stored cruise files (per echosounder) to what</p> <code>get_local_file_size</code> <p>Gets the size of a local file in bytes.</p> <code>get_local_sha256_checksum</code> <p>Calculates the SHA256 checksum of a file.</p>"},{"location":"documentation/utils/#aalibrary.utils.discrepancies.compare_local_cruise_files_to_cloud","title":"<code>compare_local_cruise_files_to_cloud(local_cruise_file_path='', ship_name='', survey_name='', echosounder='')</code>","text":"<p>Compares the locally stored cruise files (per echosounder) to what exists on the cloud by number of files, file sizes, and checksums. Reports any discrepancies in the console.</p> <p>Parameters:</p> Name Type Description Default <code>local_cruise_file_path</code> <code>str</code> <p>The folder path for the locally stored cruise data. Defaults to \"\".</p> <code>''</code> <code>ship_name</code> <code>str</code> <p>The ship name that the cruise falls under. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey/cruise name. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The specific echosounder you want to check. Defaults to \"\".</p> <code>''</code> Source code in <code>src\\aalibrary\\utils\\discrepancies.py</code> <pre><code>def compare_local_cruise_files_to_cloud(\n    local_cruise_file_path: str = \"\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n):\n    \"\"\"Compares the locally stored cruise files (per echosounder) to what\n    exists on the cloud by number of files, file sizes, and\n    checksums. Reports any discrepancies in the console.\n\n    Args:\n        local_cruise_file_path (str, optional): The folder path for the locally\n            stored cruise data. Defaults to \"\".\n        ship_name (str, optional): The ship name that the cruise falls under.\n            Defaults to \"\".\n        survey_name (str, optional): The survey/cruise name. Defaults to \"\".\n        echosounder (str, optional): The specific echosounder you want to\n            check. Defaults to \"\".\n    \"\"\"\n\n    # Create vars for use later\n    _, s3_resource, _ = create_s3_objs()\n\n    # Get all local files paths in cruise directory\n    all_raw_file_paths = glob.glob(local_cruise_file_path + \"/*.raw\")\n    all_idx_file_paths = glob.glob(local_cruise_file_path + \"/*.idx\")\n    all_bot_file_paths = glob.glob(local_cruise_file_path + \"/*.bot\")\n    # Check file numbers &amp; types\n    num_local_raw_files = len(all_raw_file_paths)\n    num_local_idx_files = len(all_idx_file_paths)\n    num_local_bot_files = len(all_bot_file_paths)\n    num_local_files = (\n        num_local_raw_files + num_local_idx_files + num_local_bot_files\n    )\n    # Get file names along with file paths\n    # [(local_file_path, file_name_with_extension), (...)]\n    all_raw_file_paths = [\n        (file_path, file_path.split(\"/\")[-1])\n        for file_path in all_raw_file_paths\n    ]\n    all_idx_file_paths = [\n        (file_path, file_path.split(\"/\")[-1])\n        for file_path in all_idx_file_paths\n    ]\n    all_bot_file_paths = [\n        (file_path, file_path.split(\"/\")[-1])\n        for file_path in all_bot_file_paths\n    ]\n\n    # Compare number of files in cruise, local vs cloud\n    num_files_in_s3 = get_all_file_names_in_a_surveys_echosounder_folder(\n        ship_name=ship_name,\n        survey_name=survey_name,\n        echosounder=echosounder,\n        s3_resource=s3_resource,\n        return_full_paths=False,\n    )\n    if num_files_in_s3 == (num_local_files):\n        print(\n            \"NUMBER OF FILES MATCH FOR\"\n            f\" {ship_name}/{survey_name}/{echosounder}\"\n        )\n    elif num_files_in_s3 != (num_local_files):\n        print(\n            \"NUMBER OF FILES DO NOT MATCH FOR\"\n            f\" {ship_name}/{survey_name}/{echosounder}\"\n        )\n        print(\n            f\"NUMBER OF FILES IN S3: {num_files_in_s3} | NUMBER OF LOCAL \"\n            f\"FILES: {num_local_files}\"\n        )\n\n    # Go through each local file, and compare file existence, size, checksum\n    for local_file_path, file_name in all_raw_file_paths:\n        # Create s3 object key\n        s3_object_key = (\n            f\"data/raw/{ship_name}/{survey_name}/{echosounder}/{file_name}\"\n        )\n        # Get existence of file in s3\n        file_exists_in_s3 = check_if_file_exists_in_s3(\n            object_key=s3_object_key,\n            s3_resource=s3_resource,\n            s3_bucket_name=\"noaa-wcsd-pds\",\n        )\n        # If file exists in s3, get size and checksum\n        if file_exists_in_s3:\n            # Get file size for s3 object key\n            s3_file_size = get_file_size_from_s3(\n                object_key=s3_object_key, s3_resource=s3_resource\n            )\n            # Get checksum for object key\n            s3_checksum = get_checksum_sha256_from_s3(\n                object_key=s3_object_key, s3_resource=s3_resource\n            )\n\n        # Get local file size\n        local_file_size = get_local_file_size(local_file_path)\n        # Get local file checksum\n        local_file_checksum = get_local_sha256_checksum(local_file_path)\n\n        # Compare existence\n        if not file_exists_in_s3:\n            print(\n                f\"LOCAL FILE {local_file_path} DOES NOT EXIST IN S3:\"\n                f\" {s3_object_key}\"\n            )\n        elif file_exists_in_s3:\n            # Compare file sizes\n            if local_file_size != s3_file_size:\n                print(\n                    f\"FILE SIZE MISMATCH FOR {local_file_path} | LOCAL: \"\n                    f\"{local_file_size} | S3: {s3_file_size}\"\n                )\n            # Compare checksums\n            if local_file_checksum != s3_checksum:\n                print(\n                    f\"CHECKSUM MISMATCH FOR {local_file_path} | LOCAL: \"\n                    f\"{local_file_checksum} | S3: {s3_checksum}\"\n                )\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.discrepancies.get_local_file_size","title":"<code>get_local_file_size(local_file_path)</code>","text":"<p>Gets the size of a local file in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>local_file_path</code> <code>str</code> <p>The local file path.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The size of the file in bytes.</p> Source code in <code>src\\aalibrary\\utils\\discrepancies.py</code> <pre><code>def get_local_file_size(local_file_path: str) -&gt; int:\n    \"\"\"Gets the size of a local file in bytes.\n\n    Args:\n        local_file_path (str): The local file path.\n\n    Returns:\n        int: The size of the file in bytes.\n    \"\"\"\n    return os.path.getsize(local_file_path)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.discrepancies.get_local_sha256_checksum","title":"<code>get_local_sha256_checksum(local_file_path, chunk_size=65536)</code>","text":"<p>Calculates the SHA256 checksum of a file.</p> <p>Parameters:</p> Name Type Description Default <code>local_file_path</code> <code>str</code> <p>The path to the file.</p> required <code>chunk_size</code> <code>int</code> <p>The size of chunks to read the file in (in bytes).               Larger chunks can be more efficient for large files.</p> <code>65536</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The SHA256 checksum of the file as a hexadecimal string.</p> Source code in <code>src\\aalibrary\\utils\\discrepancies.py</code> <pre><code>def get_local_sha256_checksum(local_file_path, chunk_size=65536) -&gt; str:\n    \"\"\"\n    Calculates the SHA256 checksum of a file.\n\n    Args:\n        local_file_path (str): The path to the file.\n        chunk_size (int): The size of chunks to read the file in (in bytes).\n                          Larger chunks can be more efficient for large files.\n\n    Returns:\n        str: The SHA256 checksum of the file as a hexadecimal string.\n    \"\"\"\n\n    sha256_hash = hashlib.sha256()\n    try:\n        with open(local_file_path, \"rb\") as f:\n            # Read the file in chunks to handle large files efficiently\n            for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n                sha256_hash.update(chunk)\n        return sha256_hash.hexdigest()\n    except FileNotFoundError:\n        return \"File not found.\"\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data","title":"<code>frequency_data</code>","text":"<p>This module contains the FrequencyData class.</p> <p>Classes:</p> Name Description <code>FrequencyData</code> <p>Given some dataset 'Sv', list all frequencies available. This class</p> <p>Functions:</p> Name Description <code>main</code> <p>Opens a sample netCDF file and constructs a FrequencyData object to</p>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData","title":"<code>FrequencyData</code>","text":"<p>Given some dataset 'Sv', list all frequencies available. This class offers methods which help map out frequencies and channels plus additional utilities.</p> <p>Methods:</p> Name Description <code>__init__</code> <p>Initializes class object and parses the frequencies available</p> <code>construct_frequency_list</code> <p>Parses the frequencies available in the xarray 'Sv'</p> <code>construct_frequency_map</code> <p>Either using a channel_list or a frequency_list this function</p> <code>construct_frequency_pair_combination_list</code> <p>Returns a list of tuple elements containing frequency combinations</p> <code>construct_frequency_set_combination_list</code> <p>Constructs a list of available frequency set permutations.</p> <code>powerset</code> <p>Generates combinations of elements of iterables ;</p> <code>print_frequency_list</code> <p>Prints each frequency element available in Sv.</p> <code>print_frequency_pair_combination_list</code> <p>Prints frequency combination list one element at a time.</p> <code>print_frequency_set_combination_list</code> <p>Prints frequency combination list one element at a time.</p> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>class FrequencyData:\n    \"\"\"Given some dataset 'Sv', list all frequencies available. This class\n    offers methods which help map out frequencies and channels plus additional\n    utilities.\"\"\"\n\n    def __init__(self, Sv):\n        \"\"\"Initializes class object and parses the frequencies available\n        within the echodata object (xarray.Dataset) 'Sv'.\n\n        Args:\n            Sv (xarray.Dataset): The 'Sv' echodata object.\n        \"\"\"\n\n        self.Sv = Sv  # Create a self object.\n        self.frequency_list = []  # Declares a frequency list to be modified.\n\n        self.construct_frequency_list()  # Construct the frequency list.\n        # TODO : This string needs cleaning up ; remove unneeded commas and\n        # empty tuples.\n        # Constructs a list of available frequency set permutations.\n        # Example : [('18 kHz',), ('38 kHz',), ('120 kHz',), ('200 kHz',),\n        # ('18 kHz', '38 kHz'), ('18 kHz', '120 kHz'), ('18 kHz', '200 kHz'),\n        # ('38 kHz', '120 kHz'), ('38 kHz', '200 kHz'), ('120 kHz', '200 kHz'),\n        # ('18 kHz', '38 kHz', '120 kHz'), ('18 kHz', '38 kHz', '200 kHz'),\n        # ('18 kHz', '120 kHz', '200 kHz'), ('38 kHz', '120 kHz', '200 kHz'),\n        # ('18 kHz', '38 kHz', '120 kHz', '200 kHz')]\n        self.frequency_set_combination_list = (\n            self.construct_frequency_set_combination_list()\n        )\n        # print(self.frequency_set_combination_list)\n        # Constructs a list of all possible unequal permutation pairs of\n        # frequencies.\n        # Example : [('18 kHz', '38 kHz'), ('18 kHz', '120 kHz'),\n        # ('18 kHz', '200 kHz'), ('38 kHz', '120 kHz'), ('38 kHz', '200 kHz'),\n        # ('120 kHz', '200 kHz')]\n        self.frequency_pair_combination_list = (\n            self.construct_frequency_pair_combination_list()\n        )\n        # print(self.frequency_pair_combination_list)\n        self.construct_frequency_map()\n\n    def construct_frequency_list(self):\n        \"\"\"Parses the frequencies available in the xarray 'Sv'\"\"\"\n        # Iterate through the natural index associated with Sv.Sv\n        for i in range(len(self.Sv.Sv)):\n            # Extract frequency.\n            self.frequency_list.append(\n                str(self.Sv.Sv[i].coords.get(\"channel\"))\n                .split(\" kHz\")[0]\n                .split(\"GPT\")[1]\n                .strip()\n                + \" kHz\"\n            )\n        # Log the constructed frequency list.\n        logger.debug(f\"Constructed frequency list: {self.frequency_list}\")\n        # Return string array frequency list of the form [18kHz, 70kHz, 200kHz]\n        return self.frequency_list\n\n    def powerset(self, iterable):\n        \"\"\"Generates combinations of elements of iterables ;\n        powerset([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n\n        Args:\n            iterable (_type_): A list.\n\n        Returns combinations of elements of iterables.\n        \"\"\"\n        # Make a list from the iterable.\n        s = list(iterable)\n        # Returns a list of tuple elements containing combinations of elements\n        # which derived from the iterable object.\n        return chain.from_iterable(\n            combinations(s, r) for r in range(len(s) + 1)\n        )\n\n    def construct_frequency_set_combination_list(self) -&gt; List[Tuple]:\n        \"\"\"Constructs a list of available frequency set permutations.\n        Example : [\n            ('18 kHz',), ('38 kHz',), ('120 kHz',), ('200 kHz',),\n            ('18 kHz', '38 kHz'), ('18 kHz', '120 kHz'), ('18 kHz', '200 kHz'),\n            ('38 kHz', '120 kHz'), ('38 kHz', '200 kHz'),\n            ('120 kHz', '200 kHz'), ('18 kHz', '38 kHz', '120 kHz'),\n            ('18 kHz', '38 kHz', '200 kHz'),('18 kHz', '120 kHz', '200 kHz'),\n            ('38 kHz', '120 kHz', '200 kHz'),\n            ('18 kHz', '38 kHz', '120 kHz', '200 kHz')]\n\n\n        Returns:\n            list&lt;tuple&gt;: A list of tuple elements containing frequency\n                combinations which is useful for the KMeansOperator class.\n        \"\"\"\n        # Returns a list of tuple elements containing frequency combinations\n        # which is useful for the KMeansOperator class.\n        return list(self.powerset(self.frequency_list))\n\n    def print_frequency_set_combination_list(self):\n        \"\"\"Prints frequency combination list one element at a time.\"\"\"\n\n        for (\n            i\n        ) in (\n            self.frequency_set_combination_list\n        ):  # For each frequency combination associated with Sv.\n            print(i)  # Print out frequency combination tuple.\n\n    def construct_frequency_pair_combination_list(self) -&gt; List[Tuple]:\n        \"\"\"Returns a list of tuple elements containing frequency combinations\n        which is useful for the KMeansOperator class.\n\n        Returns:\n            list&lt;tuple&gt;: A list of tuple elements containing frequency\n                combinations which is useful for the KMeansOperator class.\n        \"\"\"\n        # Returns a list of tuple elements containing frequency combinations\n        # which is useful for the KMeansOperator class.\n        return list(itertools.combinations(self.frequency_list, 2))\n\n    def print_frequency_pair_combination_list(self):\n        \"\"\"Prints frequency combination list one element at a time.\"\"\"\n\n        # For each frequency combination associated with Sv.\n        for i in self.frequency_pair_combination_list:\n            # Print out frequency combination tuple.\n            print(i)\n\n    def print_frequency_list(self):\n        \"\"\"Prints each frequency element available in Sv.\"\"\"\n        # For each frequency in the frequency_list associated with Sv.\n        for i in self.frequency_list:\n            # Print out the associated frequency.\n            print(i)\n\n    def construct_frequency_map(self, frequencies_provided=True):\n        \"\"\"Either using a channel_list or a frequency_list this function\n        provides one which satisfies all requirements of this class structure.\n        In particular the channels and frequencies involved have to be known\n        and mapped to one another.\n\n        Args:\n            frequencies_provided (boolean): was a frequency_list provided at\n                object creation? If so then 'True' if a channel_list instead\n                was used then 'False'.\n        \"\"\"\n        if frequencies_provided is True:\n            self.simple_frequency_list = self.frequency_list\n            # Declare a frequency map to be populated with string frequencies\n            # of the form [[1,'38kHz'],[2,'120kHz'],[4,'200kHz']] where the\n            # first element is meant to be the channel representing the\n            # frequency. This is an internal object. Do not interfere.\n            self.frequency_map = []\n            # For each frequency 'j'.\n            for j in self.simple_frequency_list:\n                # Check each channel 'i'.\n                for i in range(len(self.Sv.Sv)):\n                    channel_desc = str(self.Sv.Sv[i].coords.get(\"channel\"))\n                    # If the channel description contains \"ES\" then it is an\n                    # ES channel.\n                    if \"ES\" in channel_desc:\n                        numeric_frequency_desc = (\n                            str(self.Sv.Sv[i].coords.get(\"channel\"))\n                            .split(\"ES\")[1]\n                            .split(\"-\")[0]\n                            .strip()\n                        )\n                        if numeric_frequency_desc == j.split(\"kHz\")[0].strip():\n                            self.frequency_map.append(\n                                [i, numeric_frequency_desc + \" kHz\"]\n                            )\n                    # If the channel description contains \"GPT\" then it is a\n                    # GPT channel.\n                    if \"GPT\" in channel_desc:\n                        numeric_frequency_desc = (\n                            str(self.Sv.Sv[i].coords.get(\"channel\"))\n                            .split(\" kHz\")[0]\n                            .split(\"GPT\")[1]\n                            .strip()\n                        )\n                        # To see if the channel associates with the\n                        # frequency 'j' .\n                        if numeric_frequency_desc == j.split(\"kHz\")[0].strip():\n                            # If so append it and the channel to the\n                            # 'frequency_list'.\n                            self.frequency_map.append(\n                                [i, numeric_frequency_desc + \" kHz\"]\n                            )\n        else:\n\n            channel_desc = str(self.Sv.Sv[i].coords.get(\"channel\"))\n            # If the channel description contains \"ES\" then it is an ES\n            # channel.\n            if \"ES\" in channel_desc:\n                for i in self.channel_list:\n                    self.frequency_map.append(\n                        [\n                            i,\n                            str(self.Sv.Sv[i].coords.get(\"channel\"))\n                            .split(\" kHz\")[0]\n                            .split(\"ES\")[1]\n                            .strip()\n                            + \" kHz\",\n                        ]\n                    )\n            # If the channel description contains \"GPT\" then it is a\n            # GPT channel.\n            if \"GPT\" in channel_desc:\n                for i in self.channel_list:\n                    self.frequency_map.append(\n                        [\n                            i,\n                            str(self.Sv.Sv[i].coords.get(\"channel\"))\n                            .split(\" kHz\")[0]\n                            .split(\"GPT\")[1]\n                            .strip()\n                            + \" kHz\",\n                        ]\n                    )\n\n        # Remove duplicates from frequency_list.\n        self.frequency_map = [\n            list(t) for t in set(tuple(item) for item in self.frequency_map)\n        ]\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.__init__","title":"<code>__init__(Sv)</code>","text":"<p>Initializes class object and parses the frequencies available within the echodata object (xarray.Dataset) 'Sv'.</p> <p>Parameters:</p> Name Type Description Default <code>Sv</code> <code>Dataset</code> <p>The 'Sv' echodata object.</p> required Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def __init__(self, Sv):\n    \"\"\"Initializes class object and parses the frequencies available\n    within the echodata object (xarray.Dataset) 'Sv'.\n\n    Args:\n        Sv (xarray.Dataset): The 'Sv' echodata object.\n    \"\"\"\n\n    self.Sv = Sv  # Create a self object.\n    self.frequency_list = []  # Declares a frequency list to be modified.\n\n    self.construct_frequency_list()  # Construct the frequency list.\n    # TODO : This string needs cleaning up ; remove unneeded commas and\n    # empty tuples.\n    # Constructs a list of available frequency set permutations.\n    # Example : [('18 kHz',), ('38 kHz',), ('120 kHz',), ('200 kHz',),\n    # ('18 kHz', '38 kHz'), ('18 kHz', '120 kHz'), ('18 kHz', '200 kHz'),\n    # ('38 kHz', '120 kHz'), ('38 kHz', '200 kHz'), ('120 kHz', '200 kHz'),\n    # ('18 kHz', '38 kHz', '120 kHz'), ('18 kHz', '38 kHz', '200 kHz'),\n    # ('18 kHz', '120 kHz', '200 kHz'), ('38 kHz', '120 kHz', '200 kHz'),\n    # ('18 kHz', '38 kHz', '120 kHz', '200 kHz')]\n    self.frequency_set_combination_list = (\n        self.construct_frequency_set_combination_list()\n    )\n    # print(self.frequency_set_combination_list)\n    # Constructs a list of all possible unequal permutation pairs of\n    # frequencies.\n    # Example : [('18 kHz', '38 kHz'), ('18 kHz', '120 kHz'),\n    # ('18 kHz', '200 kHz'), ('38 kHz', '120 kHz'), ('38 kHz', '200 kHz'),\n    # ('120 kHz', '200 kHz')]\n    self.frequency_pair_combination_list = (\n        self.construct_frequency_pair_combination_list()\n    )\n    # print(self.frequency_pair_combination_list)\n    self.construct_frequency_map()\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.construct_frequency_list","title":"<code>construct_frequency_list()</code>","text":"<p>Parses the frequencies available in the xarray 'Sv'</p> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def construct_frequency_list(self):\n    \"\"\"Parses the frequencies available in the xarray 'Sv'\"\"\"\n    # Iterate through the natural index associated with Sv.Sv\n    for i in range(len(self.Sv.Sv)):\n        # Extract frequency.\n        self.frequency_list.append(\n            str(self.Sv.Sv[i].coords.get(\"channel\"))\n            .split(\" kHz\")[0]\n            .split(\"GPT\")[1]\n            .strip()\n            + \" kHz\"\n        )\n    # Log the constructed frequency list.\n    logger.debug(f\"Constructed frequency list: {self.frequency_list}\")\n    # Return string array frequency list of the form [18kHz, 70kHz, 200kHz]\n    return self.frequency_list\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.construct_frequency_map","title":"<code>construct_frequency_map(frequencies_provided=True)</code>","text":"<p>Either using a channel_list or a frequency_list this function provides one which satisfies all requirements of this class structure. In particular the channels and frequencies involved have to be known and mapped to one another.</p> <p>Parameters:</p> Name Type Description Default <code>frequencies_provided</code> <code>boolean</code> <p>was a frequency_list provided at object creation? If so then 'True' if a channel_list instead was used then 'False'.</p> <code>True</code> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def construct_frequency_map(self, frequencies_provided=True):\n    \"\"\"Either using a channel_list or a frequency_list this function\n    provides one which satisfies all requirements of this class structure.\n    In particular the channels and frequencies involved have to be known\n    and mapped to one another.\n\n    Args:\n        frequencies_provided (boolean): was a frequency_list provided at\n            object creation? If so then 'True' if a channel_list instead\n            was used then 'False'.\n    \"\"\"\n    if frequencies_provided is True:\n        self.simple_frequency_list = self.frequency_list\n        # Declare a frequency map to be populated with string frequencies\n        # of the form [[1,'38kHz'],[2,'120kHz'],[4,'200kHz']] where the\n        # first element is meant to be the channel representing the\n        # frequency. This is an internal object. Do not interfere.\n        self.frequency_map = []\n        # For each frequency 'j'.\n        for j in self.simple_frequency_list:\n            # Check each channel 'i'.\n            for i in range(len(self.Sv.Sv)):\n                channel_desc = str(self.Sv.Sv[i].coords.get(\"channel\"))\n                # If the channel description contains \"ES\" then it is an\n                # ES channel.\n                if \"ES\" in channel_desc:\n                    numeric_frequency_desc = (\n                        str(self.Sv.Sv[i].coords.get(\"channel\"))\n                        .split(\"ES\")[1]\n                        .split(\"-\")[0]\n                        .strip()\n                    )\n                    if numeric_frequency_desc == j.split(\"kHz\")[0].strip():\n                        self.frequency_map.append(\n                            [i, numeric_frequency_desc + \" kHz\"]\n                        )\n                # If the channel description contains \"GPT\" then it is a\n                # GPT channel.\n                if \"GPT\" in channel_desc:\n                    numeric_frequency_desc = (\n                        str(self.Sv.Sv[i].coords.get(\"channel\"))\n                        .split(\" kHz\")[0]\n                        .split(\"GPT\")[1]\n                        .strip()\n                    )\n                    # To see if the channel associates with the\n                    # frequency 'j' .\n                    if numeric_frequency_desc == j.split(\"kHz\")[0].strip():\n                        # If so append it and the channel to the\n                        # 'frequency_list'.\n                        self.frequency_map.append(\n                            [i, numeric_frequency_desc + \" kHz\"]\n                        )\n    else:\n\n        channel_desc = str(self.Sv.Sv[i].coords.get(\"channel\"))\n        # If the channel description contains \"ES\" then it is an ES\n        # channel.\n        if \"ES\" in channel_desc:\n            for i in self.channel_list:\n                self.frequency_map.append(\n                    [\n                        i,\n                        str(self.Sv.Sv[i].coords.get(\"channel\"))\n                        .split(\" kHz\")[0]\n                        .split(\"ES\")[1]\n                        .strip()\n                        + \" kHz\",\n                    ]\n                )\n        # If the channel description contains \"GPT\" then it is a\n        # GPT channel.\n        if \"GPT\" in channel_desc:\n            for i in self.channel_list:\n                self.frequency_map.append(\n                    [\n                        i,\n                        str(self.Sv.Sv[i].coords.get(\"channel\"))\n                        .split(\" kHz\")[0]\n                        .split(\"GPT\")[1]\n                        .strip()\n                        + \" kHz\",\n                    ]\n                )\n\n    # Remove duplicates from frequency_list.\n    self.frequency_map = [\n        list(t) for t in set(tuple(item) for item in self.frequency_map)\n    ]\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.construct_frequency_pair_combination_list","title":"<code>construct_frequency_pair_combination_list()</code>","text":"<p>Returns a list of tuple elements containing frequency combinations which is useful for the KMeansOperator class.</p> <p>Returns:</p> Type Description <code>List[Tuple]</code> <p>list: A list of tuple elements containing frequency combinations which is useful for the KMeansOperator class. Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def construct_frequency_pair_combination_list(self) -&gt; List[Tuple]:\n    \"\"\"Returns a list of tuple elements containing frequency combinations\n    which is useful for the KMeansOperator class.\n\n    Returns:\n        list&lt;tuple&gt;: A list of tuple elements containing frequency\n            combinations which is useful for the KMeansOperator class.\n    \"\"\"\n    # Returns a list of tuple elements containing frequency combinations\n    # which is useful for the KMeansOperator class.\n    return list(itertools.combinations(self.frequency_list, 2))\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.construct_frequency_set_combination_list","title":"<code>construct_frequency_set_combination_list()</code>","text":"<p>Constructs a list of available frequency set permutations. Example : [     ('18 kHz',), ('38 kHz',), ('120 kHz',), ('200 kHz',),     ('18 kHz', '38 kHz'), ('18 kHz', '120 kHz'), ('18 kHz', '200 kHz'),     ('38 kHz', '120 kHz'), ('38 kHz', '200 kHz'),     ('120 kHz', '200 kHz'), ('18 kHz', '38 kHz', '120 kHz'),     ('18 kHz', '38 kHz', '200 kHz'),('18 kHz', '120 kHz', '200 kHz'),     ('38 kHz', '120 kHz', '200 kHz'),     ('18 kHz', '38 kHz', '120 kHz', '200 kHz')]</p> <p>Returns:</p> Type Description <code>List[Tuple]</code> <p>list: A list of tuple elements containing frequency combinations which is useful for the KMeansOperator class. Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def construct_frequency_set_combination_list(self) -&gt; List[Tuple]:\n    \"\"\"Constructs a list of available frequency set permutations.\n    Example : [\n        ('18 kHz',), ('38 kHz',), ('120 kHz',), ('200 kHz',),\n        ('18 kHz', '38 kHz'), ('18 kHz', '120 kHz'), ('18 kHz', '200 kHz'),\n        ('38 kHz', '120 kHz'), ('38 kHz', '200 kHz'),\n        ('120 kHz', '200 kHz'), ('18 kHz', '38 kHz', '120 kHz'),\n        ('18 kHz', '38 kHz', '200 kHz'),('18 kHz', '120 kHz', '200 kHz'),\n        ('38 kHz', '120 kHz', '200 kHz'),\n        ('18 kHz', '38 kHz', '120 kHz', '200 kHz')]\n\n\n    Returns:\n        list&lt;tuple&gt;: A list of tuple elements containing frequency\n            combinations which is useful for the KMeansOperator class.\n    \"\"\"\n    # Returns a list of tuple elements containing frequency combinations\n    # which is useful for the KMeansOperator class.\n    return list(self.powerset(self.frequency_list))\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.powerset","title":"<code>powerset(iterable)</code>","text":"<p>Generates combinations of elements of iterables ; powerset([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>_type_</code> <p>A list.</p> required <p>Returns combinations of elements of iterables.</p> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def powerset(self, iterable):\n    \"\"\"Generates combinations of elements of iterables ;\n    powerset([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\n\n    Args:\n        iterable (_type_): A list.\n\n    Returns combinations of elements of iterables.\n    \"\"\"\n    # Make a list from the iterable.\n    s = list(iterable)\n    # Returns a list of tuple elements containing combinations of elements\n    # which derived from the iterable object.\n    return chain.from_iterable(\n        combinations(s, r) for r in range(len(s) + 1)\n    )\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.print_frequency_list","title":"<code>print_frequency_list()</code>","text":"<p>Prints each frequency element available in Sv.</p> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def print_frequency_list(self):\n    \"\"\"Prints each frequency element available in Sv.\"\"\"\n    # For each frequency in the frequency_list associated with Sv.\n    for i in self.frequency_list:\n        # Print out the associated frequency.\n        print(i)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.print_frequency_pair_combination_list","title":"<code>print_frequency_pair_combination_list()</code>","text":"<p>Prints frequency combination list one element at a time.</p> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def print_frequency_pair_combination_list(self):\n    \"\"\"Prints frequency combination list one element at a time.\"\"\"\n\n    # For each frequency combination associated with Sv.\n    for i in self.frequency_pair_combination_list:\n        # Print out frequency combination tuple.\n        print(i)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.FrequencyData.print_frequency_set_combination_list","title":"<code>print_frequency_set_combination_list()</code>","text":"<p>Prints frequency combination list one element at a time.</p> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def print_frequency_set_combination_list(self):\n    \"\"\"Prints frequency combination list one element at a time.\"\"\"\n\n    for (\n        i\n    ) in (\n        self.frequency_set_combination_list\n    ):  # For each frequency combination associated with Sv.\n        print(i)  # Print out frequency combination tuple.\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.frequency_data.main","title":"<code>main()</code>","text":"<p>Opens a sample netCDF file and constructs a FrequencyData object to extract frequency information from it.</p> Source code in <code>src\\aalibrary\\utils\\frequency_data.py</code> <pre><code>def main():\n    \"\"\"Opens a sample netCDF file and constructs a FrequencyData object to\n    extract frequency information from it.\"\"\"\n\n    input_path = \"/home/mryan/Desktop/HB1603_L1-D20160707-T190150.nc\"\n    ed = ep.open_converted(input_path)\n    Sv = ep.calibrate.compute_Sv(ed)\n\n    freq_data = FrequencyData(Sv)\n    logger.debug(freq_data.frequency_map)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.gcp_utils","title":"<code>gcp_utils</code>","text":"<p>This file contains code pertaining to auxiliary functions related to parsing through our google storage bucket.</p> <p>Functions:</p> Name Description <code>get_all_echosounders_in_a_survey_in_storage_bucket</code> <p>Gets all of the echosounders in a survey in a GCP storage bucket.</p> <code>get_all_ship_names_in_gcp_bucket</code> <p>Gets all of the ship names within a GCP storage bucket.</p> <code>get_all_survey_names_from_a_ship_in_storage_bucket</code> <p>Gets all of the survey names from a particular ship in a GCP storage</p> <code>get_all_surveys_in_storage_bucket</code> <p>Gets all of the surveys in a GCP storage bucket.</p>"},{"location":"documentation/utils/#aalibrary.utils.gcp_utils.get_all_echosounders_in_a_survey_in_storage_bucket","title":"<code>get_all_echosounders_in_a_survey_in_storage_bucket(ship_name='', survey_name='', project_id='ggn-nmfs-aa-dev-1', gcp_bucket_name='ggn-nmfs-aa-dev-1-data', gcp_bucket=None, return_full_paths=False)</code>","text":"<p>Gets all of the echosounders in a survey in a GCP storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Will get normalized to GCP standards. Defaults to None.</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>project_id</code> <code>str</code> <p>The GCP project ID that the storage bucket resides in. Defaults to \"ggn-nmfs-aa-dev-1\".</p> <code>'ggn-nmfs-aa-dev-1'</code> <code>gcp_bucket_name</code> <code>str</code> <p>The GCP storage bucket name. Defaults to \"ggn-nmfs-aa-dev-1-data\".</p> <code>'ggn-nmfs-aa-dev-1-data'</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP storage bucket client object. If none, one will be created for you based on the <code>project_id</code> and <code>gcp_bucket_name</code>. Defaults to None.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the survey names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the echosounder names that exist in a survey.</p> Source code in <code>src\\aalibrary\\utils\\gcp_utils.py</code> <pre><code>def get_all_echosounders_in_a_survey_in_storage_bucket(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    project_id: str = \"ggn-nmfs-aa-dev-1\",\n    gcp_bucket_name: str = \"ggn-nmfs-aa-dev-1-data\",\n    gcp_bucket: storage.Client.bucket = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the echosounders in a survey in a GCP storage bucket.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Will get normalized to GCP standards. Defaults to None.\n        survey_name (str, optional): The survey name/identifier.\n            Defaults to \"\".\n        project_id (str, optional): The GCP project ID that the storage bucket\n            resides in.\n            Defaults to \"ggn-nmfs-aa-dev-1\".\n        gcp_bucket_name (str, optional): The GCP storage bucket name.\n            Defaults to \"ggn-nmfs-aa-dev-1-data\".\n        gcp_bucket (storage.Client.bucket, optional): The GCP storage bucket\n            client object.\n            If none, one will be created for you based on the `project_id` and\n            `gcp_bucket_name`. Defaults to None.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the survey names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings containing the echosounder names that\n            exist in a survey.\n    \"\"\"\n\n    if gcp_bucket is None:\n        _, _, gcp_bucket = setup_gcp_storage_objs(\n            project_id=project_id, gcp_bucket_name=gcp_bucket_name\n        )\n\n    # Normalize the ship name.\n    ship_name = normalize_ship_name(ship_name=ship_name)\n    # Search all possible directories for ship surveys\n    prefixes = [\n        f\"HDD/{ship_name}/{survey_name}/\",\n        f\"NCEI/{ship_name}/{survey_name}/\",\n        f\"OMAO/{ship_name}/{survey_name}/\",\n        f\"TEST/{ship_name}/{survey_name}/\",\n    ]\n    all_subfolder_names = set()\n    all_echosounders = set()\n    # Get all subfolders from this survey, whichever directory it resides in.\n    for prefix in prefixes:\n        subfolder_names = list_all_folders_in_gcp_bucket_location(\n            location=prefix,\n            gcp_bucket=gcp_bucket,\n            return_full_paths=return_full_paths,\n        )\n        all_subfolder_names.update(subfolder_names)\n    # Filter out any folder that is not an echosounder.\n    for folder_name in list(all_subfolder_names):\n        if (\n            (\"calibration\" not in folder_name.lower())\n            and (\"metadata\" not in folder_name.lower())\n            and (\"json\" not in folder_name.lower())\n            and (\"doc\" not in folder_name.lower())\n        ):\n            # Use 'add' since each 'folder_name' is a string.\n            all_echosounders.add(folder_name)\n\n    return list(all_echosounders)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.gcp_utils.get_all_ship_names_in_gcp_bucket","title":"<code>get_all_ship_names_in_gcp_bucket(project_id='ggn-nmfs-aa-dev-1', gcp_bucket_name='ggn-nmfs-aa-dev-1-data', gcp_bucket=None, return_full_paths=False)</code>","text":"<p>Gets all of the ship names within a GCP storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>The GCP project ID that the storage bucket resides in. Defaults to \"ggn-nmfs-aa-dev-1\".</p> <code>'ggn-nmfs-aa-dev-1'</code> <code>gcp_bucket_name</code> <code>str</code> <p>The GCP storage bucket name. Defaults to \"ggn-nmfs-aa-dev-1-data\".</p> <code>'ggn-nmfs-aa-dev-1-data'</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP storage bucket client object. If none, one will be created for you based on the <code>project_id</code> and <code>gcp_bucket_name</code>. Defaults to None.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False. NOTE: You can set this parameter to <code>True</code> if you would like to see which folders contain which ships. For example: Reuben Lasker can have data coming from both OMAO and local upload HDD. It will look like: {'OMAO/Reuben_Lasker/', 'HDD/Reuben_Lasker/'}</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the ship names.</p> Source code in <code>src\\aalibrary\\utils\\gcp_utils.py</code> <pre><code>def get_all_ship_names_in_gcp_bucket(\n    project_id: str = \"ggn-nmfs-aa-dev-1\",\n    gcp_bucket_name: str = \"ggn-nmfs-aa-dev-1-data\",\n    gcp_bucket: storage.Client.bucket = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the ship names within a GCP storage bucket.\n\n    Args:\n        project_id (str, optional): The GCP project ID that the storage bucket\n            resides in.\n            Defaults to \"ggn-nmfs-aa-dev-1\".\n        gcp_bucket_name (str, optional): The GCP storage bucket name.\n            Defaults to \"ggn-nmfs-aa-dev-1-data\".\n        gcp_bucket (storage.Client.bucket, optional): The GCP storage bucket\n            client object.\n            If none, one will be created for you based on the `project_id` and\n            `gcp_bucket_name`. Defaults to None.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n            NOTE: You can set this parameter to `True` if you would like to see\n            which folders contain which ships.\n            For example: Reuben Lasker can have data coming from both OMAO and\n            local upload HDD. It will look like:\n            {'OMAO/Reuben_Lasker/', 'HDD/Reuben_Lasker/'}\n\n    Returns:\n        List[str]: A list of strings containing the ship names.\n    \"\"\"\n\n    if gcp_bucket is None:\n        _, _, gcp_bucket = setup_gcp_storage_objs(\n            project_id=project_id, gcp_bucket_name=gcp_bucket_name\n        )\n    # Get the initial subdirs\n    prefixes = [\"HDD/\", \"NCEI/\", \"OMAO/\", \"TEST/\"]\n    all_ship_names = set()\n    for prefix in prefixes:\n        ship_names = list_all_folders_in_gcp_bucket_location(\n            location=prefix,\n            gcp_bucket=gcp_bucket,\n            return_full_paths=return_full_paths,\n        )\n        all_ship_names.update(ship_names)\n\n    return list(all_ship_names)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.gcp_utils.get_all_survey_names_from_a_ship_in_storage_bucket","title":"<code>get_all_survey_names_from_a_ship_in_storage_bucket(ship_name='', project_id='ggn-nmfs-aa-dev-1', gcp_bucket_name='ggn-nmfs-aa-dev-1-data', gcp_bucket=None, return_full_paths=False)</code>","text":"<p>Gets all of the survey names from a particular ship in a GCP storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Will get normalized to GCP standards. Defaults to None.</p> <code>''</code> <code>project_id</code> <code>str</code> <p>The GCP project ID that the storage bucket resides in. Defaults to \"ggn-nmfs-aa-dev-1\".</p> <code>'ggn-nmfs-aa-dev-1'</code> <code>gcp_bucket_name</code> <code>str</code> <p>The GCP storage bucket name. Defaults to \"ggn-nmfs-aa-dev-1-data\".</p> <code>'ggn-nmfs-aa-dev-1-data'</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP storage bucket client object. If none, one will be created for you based on the <code>project_id</code> and <code>gcp_bucket_name</code>. Defaults to None.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the survey names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the survey names.</p> Source code in <code>src\\aalibrary\\utils\\gcp_utils.py</code> <pre><code>def get_all_survey_names_from_a_ship_in_storage_bucket(\n    ship_name: str = \"\",\n    project_id: str = \"ggn-nmfs-aa-dev-1\",\n    gcp_bucket_name: str = \"ggn-nmfs-aa-dev-1-data\",\n    gcp_bucket: storage.Client.bucket = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the survey names from a particular ship in a GCP storage\n    bucket.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Will get normalized to GCP standards. Defaults to None.\n        project_id (str, optional): The GCP project ID that the storage bucket\n            resides in.\n            Defaults to \"ggn-nmfs-aa-dev-1\".\n        gcp_bucket_name (str, optional): The GCP storage bucket name.\n            Defaults to \"ggn-nmfs-aa-dev-1-data\".\n        gcp_bucket (storage.Client.bucket, optional): The GCP storage bucket\n            client object.\n            If none, one will be created for you based on the `project_id` and\n            `gcp_bucket_name`. Defaults to None.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the survey names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings containing the survey names.\n    \"\"\"\n\n    if gcp_bucket is None:\n        _, _, gcp_bucket = setup_gcp_storage_objs(\n            project_id=project_id, gcp_bucket_name=gcp_bucket_name\n        )\n\n    # Normalize the ship name.\n    ship_name = normalize_ship_name(ship_name=ship_name)\n    # Search all possible directories for ship surveys\n    prefixes = [\n        f\"HDD/{ship_name}/\",\n        f\"NCEI/{ship_name}/\",\n        f\"OMAO/{ship_name}/\",\n        f\"TEST/{ship_name}/\",\n    ]\n    all_survey_names = set()\n    for prefix in prefixes:\n        survey_names = list_all_folders_in_gcp_bucket_location(\n            location=prefix,\n            gcp_bucket=gcp_bucket,\n            return_full_paths=return_full_paths,\n        )\n        all_survey_names.update(survey_names)\n\n    return list(all_survey_names)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.gcp_utils.get_all_surveys_in_storage_bucket","title":"<code>get_all_surveys_in_storage_bucket(project_id='ggn-nmfs-aa-dev-1', gcp_bucket_name='ggn-nmfs-aa-dev-1-data', gcp_bucket=None, return_full_paths=False)</code>","text":"<p>Gets all of the surveys in a GCP storage bucket.</p> <p>Parameters:</p> Name Type Description Default <code>project_id</code> <code>str</code> <p>The GCP project ID that the storage bucket resides in. Defaults to \"ggn-nmfs-aa-dev-1\".</p> <code>'ggn-nmfs-aa-dev-1'</code> <code>gcp_bucket_name</code> <code>str</code> <p>The GCP storage bucket name. Defaults to \"ggn-nmfs-aa-dev-1-data\".</p> <code>'ggn-nmfs-aa-dev-1-data'</code> <code>gcp_bucket</code> <code>bucket</code> <p>The GCP storage bucket client object. If none, one will be created for you based on the <code>project_id</code> and <code>gcp_bucket_name</code>. Defaults to None.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the survey names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings containing the survey names.</p> Source code in <code>src\\aalibrary\\utils\\gcp_utils.py</code> <pre><code>def get_all_surveys_in_storage_bucket(\n    project_id: str = \"ggn-nmfs-aa-dev-1\",\n    gcp_bucket_name: str = \"ggn-nmfs-aa-dev-1-data\",\n    gcp_bucket: storage.Client.bucket = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the surveys in a GCP storage bucket.\n\n    Args:\n        project_id (str, optional): The GCP project ID that the storage bucket\n            resides in.\n            Defaults to \"ggn-nmfs-aa-dev-1\".\n        gcp_bucket_name (str, optional): The GCP storage bucket name.\n            Defaults to \"ggn-nmfs-aa-dev-1-data\".\n        gcp_bucket (storage.Client.bucket, optional): The GCP storage bucket\n            client object.\n            If none, one will be created for you based on the `project_id` and\n            `gcp_bucket_name`. Defaults to None.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the survey names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings containing the survey names.\n    \"\"\"\n\n    if gcp_bucket is None:\n        _, gcp_bucket_name, gcp_bucket = setup_gcp_storage_objs(\n            project_id=project_id, gcp_bucket_name=gcp_bucket_name\n        )\n\n    all_ship_prefixes = get_all_ship_names_in_gcp_bucket(\n        project_id=project_id,\n        gcp_bucket_name=gcp_bucket_name,\n        gcp_bucket=gcp_bucket,\n        return_full_paths=True,\n    )\n    all_surveys = set()\n    for ship_prefix in all_ship_prefixes:\n        # Get surveys from each ship prefix\n        ship_surveys = list_all_folders_in_gcp_bucket_location(\n            location=ship_prefix,\n            gcp_bucket=gcp_bucket,\n            return_full_paths=return_full_paths,\n        )\n        all_surveys.update(ship_surveys)\n\n    return list(all_surveys)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers","title":"<code>helpers</code>","text":"<p>For helper functions.</p> <p>Functions:</p> Name Description <code>check_for_assertion_errors</code> <p>Checks for errors in the kwargs provided.</p> <code>create_azure_config_file</code> <p>Creates a config file for azure storage keys.</p> <code>get_all_objects_in_survey_from_ncei</code> <p>Gets all of the object keys from a ship survey from the NCEI database.</p> <code>get_all_ship_objects_from_ncei</code> <p>Gets all of the object keys from a ship from the NCEI database.</p> <code>get_file_name_from_url</code> <p>Extracts the file name from a given storage bucket url. Includes the</p> <code>get_file_paths_via_json_link</code> <p>This function helps in getting the links from a json request, parsing</p> <code>get_netcdf_gcp_location_from_raw_gcp_location</code> <p>Gets the netcdf location of a raw file within GCP.</p> <code>normalize_ship_name</code> <p>Normalizes a ship's name. This is necessary for creating a deterministic</p> <code>parse_correct_gcp_storage_bucket_location</code> <p>Calculates the correct gcp storage location based on data source, file</p> <code>parse_variables_from_ncei_file_url</code> <p>Gets the file variables associated with a file url in NCEI.</p>"},{"location":"documentation/utils/#aalibrary.utils.helpers.check_for_assertion_errors","title":"<code>check_for_assertion_errors(**kwargs)</code>","text":"<p>Checks for errors in the kwargs provided.</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def check_for_assertion_errors(**kwargs):\n    \"\"\"Checks for errors in the kwargs provided.\"\"\"\n\n    if \"file_name\" in kwargs:\n        assert kwargs[\"file_name\"] != \"\", (\n            \"Please provide a valid file name with the file extension\"\n            \" (ex. `2107RL_CW-D20210813-T220732.raw`)\"\n        )\n    if \"file_type\" in kwargs:\n        assert kwargs[\"file_type\"] != \"\", \"Please provide a valid file type.\"\n        assert kwargs[\"file_type\"] in config.VALID_FILETYPES, (\n            \"Please provide a valid file type (extension) \"\n            f\"from the following: {config.VALID_FILETYPES}\"\n        )\n    if \"ship_name\" in kwargs:\n        assert kwargs[\"ship_name\"] != \"\", (\n            \"Please provide a valid ship name \"\n            \"(Title_Case_With_Underscores_As_Spaces).\"\n        )\n    if \"survey_name\" in kwargs:\n        assert (\n            kwargs[\"survey_name\"] != \"\"\n        ), \"Please provide a valid survey name.\"\n    if \"echosounder\" in kwargs:\n        assert (\n            kwargs[\"echosounder\"] != \"\"\n        ), \"Please provide a valid echosounder.\"\n        assert kwargs[\"echosounder\"] in config.VALID_ECHOSOUNDERS, (\n            \"Please provide a valid echosounder from the \"\n            f\"following: {config.VALID_ECHOSOUNDERS}\"\n        )\n    if \"data_source\" in kwargs:\n        assert kwargs[\"data_source\"] != \"\", (\n            \"Please provide a valid data source from the \"\n            f\"following: {config.VALID_DATA_SOURCES}\"\n        )\n        assert kwargs[\"data_source\"] in config.VALID_DATA_SOURCES, (\n            \"Please provide a valid data source from the \"\n            f\"following: {config.VALID_DATA_SOURCES}\"\n        )\n    if \"file_download_directory\" in kwargs:\n        assert (\n            kwargs[\"file_download_directory\"] != \"\"\n        ), \"Please provide a valid file download directory.\"\n        assert os.path.isdir(kwargs[\"file_download_directory\"]), (\n            f\"File download location `{kwargs['file_download_directory']}` is\"\n            \" not found to be a valid dir, please reformat it.\"\n        )\n    if \"gcp_bucket\" in kwargs:\n        assert kwargs[\"gcp_bucket\"] is not None, (\n            \"Please provide a gcp_bucket object with\"\n            \" `utils.cloud_utils.setup_gcp_storage()`\"\n        )\n    if \"directory\" in kwargs:\n        assert kwargs[\"directory\"] != \"\", \"Please provide a valid directory.\"\n        assert os.path.isdir(kwargs[\"directory\"]), (\n            f\"Directory location `{kwargs['directory']}` is not found to be a\"\n            \" valid dir, please reformat it.\"\n        )\n    if \"data_lake_directory_client\" in kwargs:\n        assert kwargs[\"data_lake_directory_client\"] is not None, (\n            f\"The data lake directory client cannot be a\"\n            f\" {type(kwargs['data_lake_directory_client'])} object. It needs \"\n            \"to be of the type `DataLakeDirectoryClient`.\"\n        )\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.create_azure_config_file","title":"<code>create_azure_config_file(download_directory='')</code>","text":"<p>Creates a config file for azure storage keys.</p> <p>Parameters:</p> Name Type Description Default <code>download_directory</code> <code>str</code> <p>The directory to store the azure config file. Defaults to \"\".</p> <code>''</code> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def create_azure_config_file(download_directory: str = \"\"):\n    \"\"\"Creates a config file for azure storage keys.\n\n    Args:\n        download_directory (str, optional): The directory to store the\n            azure config file. Defaults to \"\".\n    \"\"\"\n\n    assert (\n        download_directory != \"\"\n    ), \"Please provide a valid download directory.\"\n    download_directory = os.path.normpath(download_directory)\n    assert os.path.isdir(download_directory), (\n        f\"Directory location `{download_directory}` is not found to be a\"\n        \" valid dir, please reformat it.\"\n    )\n\n    azure_config_file_path = os.path.join(\n        download_directory, \"azure_config.ini\"\n    )\n\n    empty_config_str = \"\"\"[DEFAULT]\nazure_storage_account_name = \nazure_storage_account_key = \nazure_account_url = \nazure_connection_string = \"\"\"\n\n    with open(\n        azure_config_file_path, \"w\", encoding=\"utf-8\"\n    ) as azure_config_file:\n        azure_config_file.write(empty_config_str)\n\n    print(\n        f\"Please fill out the azure config file at: {azure_config_file_path}\"\n    )\n    return azure_config_file_path\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.get_all_objects_in_survey_from_ncei","title":"<code>get_all_objects_in_survey_from_ncei(ship_name='', survey_name='', s3_bucket=None)</code>","text":"<p>Gets all of the object keys from a ship survey from the NCEI database.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The name of the ship. Must be title-case and have spaces substituted for underscores. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The name of the survey. Must match what we have in the NCEI database. Defaults to \"\".</p> <code>''</code> <code>s3_bucket</code> <code>resource</code> <p>The boto3 bucket resource for the bucket that the ship data resides in. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings. Each one being an object key (path to the object inside of the bucket).</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def get_all_objects_in_survey_from_ncei(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    s3_bucket: boto3.resource = None,\n) -&gt; List[str]:\n    \"\"\"Gets all of the object keys from a ship survey from the NCEI database.\n\n    Args:\n        ship_name (str, optional): The name of the ship. Must be title-case\n            and have spaces substituted for underscores. Defaults to \"\".\n        survey_name (str, optional): The name of the survey. Must match what\n            we have in the NCEI database. Defaults to \"\".\n        s3_bucket (boto3.resource, optional): The boto3 bucket resource for\n            the bucket that the ship data resides in. Defaults to None.\n\n    Returns:\n        List[str]: A list of strings. Each one being an object key (path to\n            the object inside of the bucket).\n    \"\"\"\n\n    assert ship_name != \"\", (\n        \"Please provide a valid Titlecase\",\n        \" ship_name using underscores as spaces.\",\n    )\n    assert \" \" not in ship_name, (\n        \"Please provide a valid Titlecase\",\n        \" ship_name using underscores as spaces.\",\n    )\n    assert survey_name != \"\", \"Please provide a valid survey name.\"\n    assert s3_bucket is not None, \"Please pass in a boto3 bucket object.\"\n\n    survey_objects = []\n\n    for obj in s3_bucket.objects.filter(\n        Prefix=f\"data/raw/{ship_name}/{survey_name}\"\n    ):\n        survey_objects.append(obj.key)\n\n    return survey_objects\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.get_all_ship_objects_from_ncei","title":"<code>get_all_ship_objects_from_ncei(ship_name='', bucket=None)</code>","text":"<p>Gets all of the object keys from a ship from the NCEI database.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The name of the ship. Must be title-case and have spaces substituted for underscores. Defaults to \"\".</p> <code>''</code> <code>bucket</code> <code>resource</code> <p>The boto3 bucket resource for the bucket that the ship data resides in. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings. Each one being an object key (path to the object inside of the bucket).</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def get_all_ship_objects_from_ncei(\n    ship_name: str = \"\", bucket: boto3.resource = None\n) -&gt; List[str]:\n    \"\"\"Gets all of the object keys from a ship from the NCEI database.\n\n    Args:\n        ship_name (str, optional): The name of the ship. Must be title-case\n            and have spaces substituted for underscores. Defaults to \"\".\n        bucket (boto3.resource, optional): The boto3 bucket resource for the\n            bucket that the ship data resides in. Defaults to None.\n\n    Returns:\n        List[str]: A list of strings. Each one being an object key (path to\n            the object inside of the bucket).\n    \"\"\"\n\n    assert ship_name != \"\", (\n        \"Please provide a valid Titlecase\",\n        \" ship_name using underscores as spaces.\",\n    )\n    assert \" \" not in ship_name, (\n        \"Please provide a valid Titlecase\",\n        \" ship_name using underscores as spaces.\",\n    )\n    assert bucket is not None, \"Please pass in a boto3 bucket object.\"\n\n    ship_objects = []\n\n    for obj in bucket.objects.filter(Prefix=f\"data/raw/{ship_name}\"):\n        ship_objects.append(obj.key)\n\n    return ship_objects\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.get_file_name_from_url","title":"<code>get_file_name_from_url(url='')</code>","text":"<p>Extracts the file name from a given storage bucket url. Includes the file extension.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The full url of the storage object. Defaults to \"\". Example: \"https://noaa-wcsd-pds.s3.amazonaws.com/data/raw/Reuben_La           sker/RL2107/EK80/2107RL_CW-D20210813-T220732.raw\"</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The file name. Example: 2107RL_CW-D20210813-T220732.raw</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def get_file_name_from_url(url: str = \"\") -&gt; str:\n    \"\"\"Extracts the file name from a given storage bucket url. Includes the\n    file extension.\n\n    Args:\n        url (str, optional): The full url of the storage object.\n            Defaults to \"\".\n            Example: \"https://noaa-wcsd-pds.s3.amazonaws.com/data/raw/Reuben_La\n                      sker/RL2107/EK80/2107RL_CW-D20210813-T220732.raw\"\n\n    Returns:\n        str: The file name. Example: 2107RL_CW-D20210813-T220732.raw\n    \"\"\"\n\n    return url.split(\"/\")[-1]\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.get_file_paths_via_json_link","title":"<code>get_file_paths_via_json_link(link='')</code>","text":"<p>This function helps in getting the links from a json request, parsing the contents of that url into a json object. The output is a json of the filename, and the cloud path link (s3 bucket link). Code from: https://www.ngdc.noaa.gov/mgg/wcd/S3_download.html</p> <p>Parameters:</p> Name Type Description Default <code>link</code> <code>str</code> <p>The link to the json url. Defaults to \"\".</p> <code>''</code> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def get_file_paths_via_json_link(link: str = \"\"):\n    \"\"\"This function helps in getting the links from a json request, parsing\n    the contents of that url into a json object. The output is a json of the\n    filename, and the cloud path link (s3 bucket link).\n    Code from: https://www.ngdc.noaa.gov/mgg/wcd/S3_download.html\n\n    Args:\n        link (str, optional): The link to the json url. Defaults to \"\".\n    \"\"\"\n\n    url = requests.get(link, timeout=10)\n    text = url.text\n    contents = json.loads(text)\n    for k in contents.keys():\n        print(k)\n    for i in contents[\"features\"]:\n        file_name = i[\"attributes\"][\"FILE_NAME\"]\n        cloud_path = i[\"attributes\"][\"CLOUD_PATH\"]\n        if cloud_path:\n            print(f\"{file_name}, {cloud_path}\")\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.get_netcdf_gcp_location_from_raw_gcp_location","title":"<code>get_netcdf_gcp_location_from_raw_gcp_location(gcp_storage_bucket_location='')</code>","text":"<p>Gets the netcdf location of a raw file within GCP.</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def get_netcdf_gcp_location_from_raw_gcp_location(\n    gcp_storage_bucket_location: str = \"\",\n):\n    \"\"\"Gets the netcdf location of a raw file within GCP.\"\"\"\n\n    gcp_storage_bucket_location = gcp_storage_bucket_location.replace(\n        \"/raw/\", \"/netcdf/\"\n    )\n    # get rid of file extension and replace with netcdf\n    netcdf_gcp_storage_bucket_location = (\n        \".\".join(gcp_storage_bucket_location.split(\".\")[:-1]) + \".nc\"\n    )\n\n    return netcdf_gcp_storage_bucket_location\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.normalize_ship_name","title":"<code>normalize_ship_name(ship_name='')</code>","text":"<p>Normalizes a ship's name. This is necessary for creating a deterministic file structure within our GCP storage bucket. The ship name is returned as a Title_Cased_And_Snake_Cased ship name, with no punctuation. Ex. <code>HENRY B. BIGELOW</code> will return <code>Henry_B_Bigelow</code></p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship name string. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The formatted and normalized version of the ship name.</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def normalize_ship_name(ship_name: str = \"\") -&gt; str:\n    \"\"\"Normalizes a ship's name. This is necessary for creating a deterministic\n    file structure within our GCP storage bucket.\n    The ship name is returned as a Title_Cased_And_Snake_Cased ship name, with\n    no punctuation.\n    Ex. `HENRY B. BIGELOW` will return `Henry_B_Bigelow`\n\n    Args:\n        ship_name (str, optional): The ship name string. Defaults to \"\".\n\n    Returns:\n        str: The formatted and normalized version of the ship name.\n    \"\"\"\n\n    # Lower case the string\n    ship_name = ship_name.lower()\n    # Un-normalize (replace `_` with ` ` to help further processing)\n    # In the edge-case that users include an underscore.\n    ship_name = ship_name.replace(\"_\", \" \")\n    # Remove all punctuation.\n    ship_name = \"\".join(\n        [char for char in ship_name if char not in string.punctuation]\n    )\n    # Title-case it\n    ship_name = ship_name.title()\n    # Snake-case it\n    ship_name = ship_name.replace(\" \", \"_\")\n\n    return ship_name\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.parse_correct_gcp_storage_bucket_location","title":"<code>parse_correct_gcp_storage_bucket_location(file_name='', file_type='', ship_name='', survey_name='', echosounder='', data_source='', is_metadata=False, is_survey_metadata=False, debug=False)</code>","text":"<p>Calculates the correct gcp storage location based on data source, file type, and if the file is metadata or not.</p> <p>Parameters:</p> Name Type Description Default <code>file_name</code> <code>str</code> <p>The file name (includes extension). Defaults to \"\".</p> <code>''</code> <code>file_type</code> <code>str</code> <p>The file type (not include the dot \".\"). Defaults to \"\".</p> <code>''</code> <code>ship_name</code> <code>str</code> <p>The ship name associated with this survey. Defaults to \"\".</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name/identifier. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used to gather the data. Defaults to \"\".</p> <code>''</code> <code>data_source</code> <code>str</code> <p>The source of the data. Can be one of [\"NCEI\", \"OMAO\"]. Defaults to \"\".</p> <code>''</code> <code>is_metadata</code> <code>bool</code> <p>Whether or not the file is a metadata file. Necessary since files that are considered metadata (metadata json, or readmes) are stored in a separate directory. Defaults to False.</p> <code>False</code> <code>is_survey_metadata</code> <code>bool</code> <p>Whether or not the file is a metadata file associated with a survey. The files are stored at the survey level, in the <code>metadata/</code> folder. Defaults to False.</p> <code>False</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug statements. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The correctly parsed GCP storage bucket location.</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def parse_correct_gcp_storage_bucket_location(\n    file_name: str = \"\",\n    file_type: str = \"\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    data_source: str = \"\",\n    is_metadata: bool = False,\n    is_survey_metadata: bool = False,\n    debug: bool = False,\n) -&gt; str:\n    \"\"\"Calculates the correct gcp storage location based on data source, file\n    type, and if the file is metadata or not.\n\n    Args:\n        file_name (str, optional): The file name (includes extension).\n            Defaults to \"\".\n        file_type (str, optional): The file type (not include the dot \".\").\n            Defaults to \"\".\n        ship_name (str, optional): The ship name associated with this survey.\n            Defaults to \"\".\n        survey_name (str, optional): The survey name/identifier. Defaults\n            to \"\".\n        echosounder (str, optional): The echosounder used to gather the data.\n            Defaults to \"\".\n        data_source (str, optional): The source of the data. Can be one of\n            [\"NCEI\", \"OMAO\"]. Defaults to \"\".\n        is_metadata (bool, optional): Whether or not the file is a metadata\n            file. Necessary since files that are considered metadata (metadata\n            json, or readmes) are stored in a separate directory. Defaults to\n            False.\n        is_survey_metadata (bool, optional): Whether or not the file is a\n            metadata file associated with a survey. The files are stored at\n            the survey level, in the `metadata/` folder. Defaults to False.\n        debug (bool, optional): Whether or not to print debug statements.\n            Defaults to False.\n\n    Returns:\n        str: The correctly parsed GCP storage bucket location.\n    \"\"\"\n\n    assert (\n        (is_metadata and is_survey_metadata is False)\n        or (is_metadata is False and is_survey_metadata)\n        or (is_metadata is False and is_survey_metadata is False)\n    ), (\n        \"Please make sure that only one of `is_metadata` and\"\n        \" `is_survey_metadata` is True. Or you can set both to False.\"\n    )\n\n    # Creating the correct upload location\n    if is_survey_metadata:\n        gcp_storage_bucket_location = (\n            f\"{data_source}/{ship_name}/{survey_name}/metadata/{file_name}\"\n        )\n    elif is_metadata:\n        gcp_storage_bucket_location = (\n            f\"{data_source}/{ship_name}/{survey_name}/{echosounder}/metadata/\"\n        )\n        # Figure out if its a raw or idx file (belongs in raw folder)\n        if file_type.lower() in config.RAW_DATA_FILE_TYPES:\n            gcp_storage_bucket_location = (\n                gcp_storage_bucket_location + f\"raw/{file_name}.json\"\n            )\n        elif file_type.lower() in config.CONVERTED_DATA_FILE_TYPES:\n            gcp_storage_bucket_location = (\n                gcp_storage_bucket_location + f\"netcdf/{file_name}.json\"\n            )\n    else:\n        # Figure out if its a raw or idx file (belongs in raw folder)\n        if file_type.lower() in config.RAW_DATA_FILE_TYPES:\n            gcp_storage_bucket_location = (\n                f\"{data_source}/{ship_name}/\"\n                f\"{survey_name}/{echosounder}/data/raw/{file_name}\"\n            )\n        elif file_type.lower() in config.CONVERTED_DATA_FILE_TYPES:\n            gcp_storage_bucket_location = (\n                f\"{data_source}/{ship_name}/\"\n                f\"{survey_name}/{echosounder}/data/netcdf/{file_name}\"\n            )\n\n    if debug:\n        logging.debug(\n            \"PARSED GCP_STORAGE_BUCKET_LOCATION: %s\",\n            gcp_storage_bucket_location,\n        )\n\n    return gcp_storage_bucket_location\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.helpers.parse_variables_from_ncei_file_url","title":"<code>parse_variables_from_ncei_file_url(url='')</code>","text":"<p>Gets the file variables associated with a file url in NCEI. File urls in NCEI follow this template: data/raw/{ship_name}/{survey_name}/{echosounder}/{file_name}</p> <p>NOTE: file_name will include the extension.</p> Source code in <code>src\\aalibrary\\utils\\helpers.py</code> <pre><code>def parse_variables_from_ncei_file_url(url: str = \"\"):\n    \"\"\"Gets the file variables associated with a file url in NCEI.\n    File urls in NCEI follow this template:\n    data/raw/{ship_name}/{survey_name}/{echosounder}/{file_name}\n\n    NOTE: file_name will include the extension.\"\"\"\n\n    file_name = get_file_name_from_url(url=url)\n    file_type = file_name.split(\".\")[-1]\n    echosounder = url.split(\"/\")[-2]\n    survey_name = url.split(\"/\")[-3]\n    ship_name = url.split(\"/\")[-4]\n\n    return file_name, file_type, echosounder, survey_name, ship_name\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ices","title":"<code>ices</code>","text":"<p>Functions:</p> Name Description <code>correct_dimensions_ices</code> <p>Extracts angle data from echopype DataArray.</p> <code>echopype_ek60_raw_to_ices_netcdf</code> <p>Writes echodata Beam_group ds to a Beam_groupX netcdf file.</p> <code>echopype_ek80_raw_to_ices_netcdf</code> <p>Writes echodata Beam_group ds to a Beam_groupX netcdf file.</p> <code>ragged_data_type_ices</code> <p>Transforms a gridded 4 dimensional variable from an Echodata object</p> <code>write_ek60_beamgroup_to_netcdf</code> <p>Writes echopype Beam_group ds to a Beam_groupX netcdf file.</p> <code>write_ek80_beamgroup_to_netcdf</code> <p>Writes echodata Beam_group ds to a Beam_groupX netcdf file.</p>"},{"location":"documentation/utils/#aalibrary.utils.ices.correct_dimensions_ices","title":"<code>correct_dimensions_ices(echodata, variable_name='')</code>","text":"<p>Extracts angle data from echopype DataArray.</p> <p>Args: echodata (echopype.DataArray): Echopype echodata object containing data. variable_name (str): The name of the variable that needs to be transformed to a ragged array representation.</p> <p>Returns: np.array that returns array with correct dimension as specified by ICES netcdf convention.</p> Source code in <code>src\\aalibrary\\utils\\ices.py</code> <pre><code>def correct_dimensions_ices(echodata, variable_name: str = \"\") -&gt; np.ndarray:\n    \"\"\"Extracts angle data from echopype DataArray.\n\n    Args:\n    echodata (echopype.DataArray): Echopype echodata object containing data.\n    variable_name (str): The name of the variable that needs to be transformed to\n    a ragged array representation.\n\n    Returns:\n    np.array that returns array with correct dimension as specified by ICES netcdf convention.\n    \"\"\"\n    num_pings = echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"]\n    num_channels = echodata[\"Sonar/Beam_group1\"].sizes[\"channel\"]\n\n    compliant_np = np.empty((num_pings, num_channels))\n\n    for ping_time_val in range(num_pings):\n        compliant_np[ping_time_val, :] = (\n            echodata[\"Sonar/Beam_group1\"][variable_name]\n            .values.transpose()\n            .astype(np.float32)\n        )\n\n    return compliant_np\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ices.echopype_ek60_raw_to_ices_netcdf","title":"<code>echopype_ek60_raw_to_ices_netcdf(echodata, export_file)</code>","text":"<p>Writes echodata Beam_group ds to a Beam_groupX netcdf file.</p> <p>Args: echodata (echopype.echodata): Echopype echodata object containing beam_group_data. (echopype.DataArray): Echopype DataArray to be written. export_file (str or Path): Path to the NetCDF file.</p> Source code in <code>src\\aalibrary\\utils\\ices.py</code> <pre><code>def echopype_ek60_raw_to_ices_netcdf(echodata, export_file):\n    \"\"\"Writes echodata Beam_group ds to a Beam_groupX netcdf file.\n\n    Args:\n    echodata (echopype.echodata): Echopype echodata object containing beam_group_data.\n    (echopype.DataArray): Echopype DataArray to be written.\n    export_file (str or Path): Path to the NetCDF file.\n    \"\"\"\n\n    engine = \"netcdf4\"\n\n    output_file = validate_output_path(\n        source_file=echodata.source_file,\n        engine=engine,\n        save_path=export_file,\n        output_storage_options={},\n    )\n\n    save_file(\n        echodata[\"Top-level\"],\n        path=output_file,\n        mode=\"w\",\n        engine=engine,\n        compression_settings=COMPRESSION_SETTINGS[engine],\n    )\n    save_file(\n        echodata[\"Environment\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Environment\",\n        compression_settings=COMPRESSION_SETTINGS[engine],\n    )\n    save_file(\n        echodata[\"Platform\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Platform\",\n        compression_settings=COMPRESSION_SETTINGS[engine],\n    )\n\n    save_file(\n        echodata[\"Platform/NMEA\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Platform/NMEA\",\n        compression_settings=COMPRESSION_SETTINGS[engine],\n    )\n\n    save_file(\n        echodata[\"Sonar\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Sonar\",\n        compression_settings=COMPRESSION_SETTINGS[engine],\n    )\n\n    echopype_ek60_raw_to_ices_netcdf(echodata, output_file)\n\n    save_file(\n        echodata[\"Vendor_specific\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Vendor_specific\",\n        compression_settings=COMPRESSION_SETTINGS[engine],\n    )\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ices.echopype_ek80_raw_to_ices_netcdf","title":"<code>echopype_ek80_raw_to_ices_netcdf(echodata, export_file)</code>","text":"<p>Writes echodata Beam_group ds to a Beam_groupX netcdf file.</p> <p>Args: echodata (echopype.echodata): Echopype echodata object containing beam_group_data. (echopype.DataArray): Echopype DataArray to be written. export_file (str or Path): Path to the NetCDF file.</p> Source code in <code>src\\aalibrary\\utils\\ices.py</code> <pre><code>def echopype_ek80_raw_to_ices_netcdf(echodata, export_file):\n    \"\"\"Writes echodata Beam_group ds to a Beam_groupX netcdf file.\n\n    Args:\n    echodata (echopype.echodata): Echopype echodata object containing beam_group_data.\n    (echopype.DataArray): Echopype DataArray to be written.\n    export_file (str or Path): Path to the NetCDF file.\n    \"\"\"\n    engine = \"netcdf4\"\n\n    output_file = validate_output_path(\n        source_file=echodata.source_file,\n        engine=engine,\n        save_path=export_file,\n        output_storage_options={},\n    )\n\n    save_file(\n        echodata[\"Top-level\"],\n        path=output_file,\n        mode=\"w\",\n        engine=engine,\n        compression_settings=COMPRESSION_SETTINGS[engine]\n    )\n    save_file(\n        echodata[\"Environment\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Environment\",\n        compression_settings=COMPRESSION_SETTINGS[engine]\n    )\n    save_file(\n        echodata[\"Platform\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Platform\",\n        compression_settings=COMPRESSION_SETTINGS[engine]\n    )\n    save_file(\n        echodata[\"Platform/NMEA\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Platform/NMEA\",\n        compression_settings=COMPRESSION_SETTINGS[engine]\n    )\n    save_file(\n        echodata[\"Sonar\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Sonar\",\n        compression_settings=COMPRESSION_SETTINGS[engine]\n    )\n    write_ek80_beamgroup_to_netcdf(echodata, output_file)\n    save_file(\n        echodata[\"Vendor_specific\"],\n        path=output_file,\n        mode=\"a\",\n        engine=engine,\n        group=\"Vendor_specific\",\n        compression_settings=COMPRESSION_SETTINGS[engine]\n    )\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ices.ragged_data_type_ices","title":"<code>ragged_data_type_ices(echodata, variable_name='')</code>","text":"<p>Transforms a gridded 4 dimensional variable from an Echodata object into a ragged array representation.</p> <p>Args: echodata (echopype.Echodata): Echopype echodata object containing a variable in the Beam_group1. variable_name (str): The name of the variable that needs to be transformed to a ragged array representation.</p> <p>Returns: ICES complain np array of type object.</p> Source code in <code>src\\aalibrary\\utils\\ices.py</code> <pre><code>def ragged_data_type_ices(echodata, variable_name: str = \"\") -&gt; np.ndarray:\n    \"\"\"Transforms a gridded 4 dimensional variable from an Echodata object\n    into a ragged array representation.\n\n    Args:\n    echodata (echopype.Echodata): Echopype echodata object containing a variable in the Beam_group1.\n    variable_name (str): The name of the variable that needs to be transformed to\n    a ragged array representation.\n\n    Returns:\n    ICES complain np array of type object.\n    \"\"\"\n\n    num_pings = echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"]\n    num_channels = echodata[\"Sonar/Beam_group1\"].sizes[\"channel\"]\n    num_beam = echodata[\"Sonar/Beam_group1\"].sizes[\"beam\"]\n\n    compliant_np = np.empty((num_pings, num_channels, num_beam), object)\n\n    for c, channel in enumerate(\n        echodata[\"Sonar/Beam_group1\"][variable_name].coords[\"channel\"].values\n    ):\n\n        test = echodata[\"Sonar/Beam_group1\"][variable_name].sel(channel=channel)\n\n        # Find the first index along 'range_sample' where all values are NaN across 'beam'\n        is_nan_across_beam = test.isnull().all(dim=\"beam\")\n\n        # Find the first index along 'range_sample' where 'is_nan_across_beam' is True\n        first_nan_range_sample_indices = xr.apply_ufunc(\n            np.argmax,\n            is_nan_across_beam,\n            input_core_dims=[[\"range_sample\"]],\n            exclude_dims=set((\"range_sample\",)),\n            vectorize=True,  # Apply the function row-wise for each ping_time\n            dask=\"parallelized\",\n            output_dtypes=[int],\n        )\n\n        found_nan_block_mask = is_nan_across_beam.isel(\n            range_sample=first_nan_range_sample_indices.clip(min=0)\n        )\n\n        sample_t = []\n\n        # Iterate through ping_time to populate sample_t\n        for i, _ in enumerate(test[\"ping_time\"].values):\n            if found_nan_block_mask.isel(ping_time=i):\n                value_to_append = (\n                    test[\"range_sample\"].values[\n                        first_nan_range_sample_indices.isel(ping_time=i).item()\n                    ]\n                    - 1\n                )\n                sample_t.append(value_to_append)\n            else:\n                # If no all-NaN block was found, append the last range_sample index\n                sample_t.append(test[\"range_sample\"].values[-1])\n        sample_t = np.array(sample_t)\n\n        all_ping_segments = []\n\n        for i, ping_da in enumerate(test):\n            segment = ping_da.isel(range_sample=slice(sample_t[i])).values.transpose()\n            all_ping_segments.append(segment)\n\n        for i in range(len(compliant_np)):\n            for j in range(4):\n                compliant_np[i, c, j] = all_ping_segments[i][j].astype(np.float32)\n\n    return compliant_np\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ices.write_ek60_beamgroup_to_netcdf","title":"<code>write_ek60_beamgroup_to_netcdf(echodata, export_file)</code>","text":"<p>Writes echopype Beam_group ds to a Beam_groupX netcdf file.</p> <p>Parameters: ed (echopype.DataArray): Echopype DataArray to be written. export_file (str or Path): Path to the output NetCDF file.</p> Source code in <code>src\\aalibrary\\utils\\ices.py</code> <pre><code>def write_ek60_beamgroup_to_netcdf(echodata, export_file):\n    \"\"\"\n    Writes echopype Beam_group ds to a Beam_groupX netcdf file.\n\n    Parameters:\n    ed (echopype.DataArray): Echopype DataArray to be written.\n    export_file (str or Path): Path to the output NetCDF file.\n    \"\"\"\n    ragged_backscatter_r_data = ragged_data_type_ices(echodata, \"backscatter_r\")\n    beamwidth_receive_major_data = correct_dimensions_ices(\n        echodata, \"beamwidth_twoway_athwartship\"\n    )\n    beamwidth_receive_minor_data = correct_dimensions_ices(\n        echodata, \"beamwidth_twoway_alongship\"\n    )\n    echoangle_major_data = ragged_data_type_ices(echodata, \"angle_athwartship\")\n    echoangle_minor_data = ragged_data_type_ices(echodata, \"angle_alongship\")\n    equivalent_beam_angle_data = correct_dimensions_ices(\n        echodata, \"equivalent_beam_angle\"\n    )\n    rx_beam_rotation_phi_data = (\n        ragged_data_type_ices(echodata, \"angle_athwartship\") * -1\n    )\n    rx_beam_rotation_psi_data = np.zeros(\n        (echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1)\n    )\n    rx_beam_rotation_theta_data = ragged_data_type_ices(echodata, \"angle_alongship\")\n\n    for i in range(echodata[\"Sonar/Beam_group1\"].sizes[\"channel\"]):\n\n        with netCDF4.Dataset(export_file, \"a\", format=\"netcdf4\") as ncfile:\n            grp = ncfile.createGroup(f\"Sonar/Beam_group{i+1}\")\n            grp.setncattr(\"beam_mode\", echodata[\"Sonar/Beam_group1\"].attrs[\"beam_mode\"])\n            grp.setncattr(\n                \"conversion_equation_type\",\n                echodata[\"Sonar/Beam_group1\"].attrs[\"conversion_equation_t\"],\n            )\n            grp.setncattr(\n                \"long_name\", echodata[\"Sonar/Beam_group1\"].coords[\"channel\"].values[i]\n            )\n\n            # Create the VLEN type for 32-bit floats\n            sample_t = grp.createVLType(np.float32, \"sample_t\")\n            angle_t = grp.createVLType(np.float32, \"angle_t\")\n\n            # Create ping_time dimension and ping_time coordinate variable\n            grp.createDimension(\"ping_time\", None)\n\n            ping_time_var = grp.createVariable(\"ping_time\", np.int64, (\"ping_time\",))\n            ping_time_var.units = \"nanoseconds since 1970-01-01 00:00:00Z\"\n            ping_time_var.standard_name = \"time\"\n            ping_time_var.long_name = \"Time-stamp of each ping\"\n            ping_time_var.axis = \"T\"\n            ping_time_var.calendar = \"gregorian\"\n            ping_time_var[:] = echodata[\"Sonar/Beam_group1\"].coords[\n                \"ping_time\"\n            ].values - np.datetime64(\"1970-01-01T00:00:00Z\")\n\n            # Create beam dimension and coordinate variable\n            grp.createDimension(\"beam\", 1)\n\n            beam_var = grp.createVariable(\"beam\", \"S1\", (\"beam\",))\n            beam_var.long_name = \"Beam name\"\n            beam_var[:] = echodata[\"Sonar/Beam_group1\"].coords[\"channel\"].values[i]\n\n            # Create backscatter_r variable\n            backscatter_r = grp.createVariable(\n                \"backscatter_r\", sample_t, (\"ping_time\", \"beam\")\n            )\n            backscatter_r[:] = ragged_backscatter_r_data[:, i]\n            backscatter_r.setncattr(\n                \"long_name\", \"Raw backscatter measurements (real part)\"\n            )\n            backscatter_r.units = \"dB\"\n\n            # Create beam_stabilisation variable\n            beam_stablisation = grp.createVariable(\n                \"beam_stablisation\", int, (\"ping_time\", \"beam\")\n            )\n            beam_stablisation[:] = np.zeros(\n                (echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1)\n            )\n            beam_stablisation.setncattr(\n                \"long_name\", \"Beam stabilisation applied(or not)\"\n            )\n\n            # Create beam_type variable\n            beam_type = grp.createVariable(\"beam_type\", int, ())\n            beam_type[:] = echodata[\"Sonar/Beam_group1\"][\"beam_type\"].values[i]\n            beam_type.setncattr(\"long_name\", \"type of transducer (0-single, 1-split)\")\n\n            # Create beamwidth_receive_major variable\n            beamwidth_receive_major = grp.createVariable(\n                \"beamwidth_receive_major\", np.float32, (\"ping_time\", \"beam\")\n            )\n            beamwidth_receive_major[:] = beamwidth_receive_major_data[:, i]\n            beamwidth_receive_major.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along major (horizontal) axis of beam\",\n            )\n            beamwidth_receive_major.units = \"arc_degree\"\n            beamwidth_receive_major.valid_range = [0.0, 360.0]\n\n            # Create beamwidth_receive_minor variable\n            beamwidth_receive_minor = grp.createVariable(\n                \"beamwidth_receive_minor\", np.float32, (\"ping_time\", \"beam\")\n            )\n            beamwidth_receive_minor[:] = beamwidth_receive_minor_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            beamwidth_receive_minor.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along minor (vertical) axis of beam\",\n            )\n            beamwidth_receive_minor.units = \"arc_degree\"\n            beamwidth_receive_minor.valid_range = [0.0, 360.0]\n\n            beamwidth_transmit_major = grp.createVariable(\n                \"beamwidth_transmit_major\", np.float32, (\"ping_time\", \"beam\")\n            )\n            # Create beamwidth_transmit_major variable\n            beamwidth_transmit_major[:] = beamwidth_receive_major_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            beamwidth_transmit_major.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along major (horizontal) axis of beam\",\n            )\n            beamwidth_transmit_major.units = \"arc_degree\"\n            beamwidth_transmit_major.valid_range = [0.0, 360.0]\n\n            # Create beamwidth_transmit_minor variable\n            beamwidth_transmit_minor = grp.createVariable(\n                \"beamwidth_transmit_minor\", np.float32, (\"ping_time\", \"beam\")\n            )\n            beamwidth_transmit_minor[:] = beamwidth_receive_minor_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            beamwidth_transmit_minor.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along minor (vertical) axis of beam\",\n            )\n            beamwidth_transmit_minor.units = \"arc_degree\"\n            beamwidth_transmit_minor.valid_range = [0.0, 360.0]\n\n            # Create blanking_interval variable\n            blanking_interval = grp.createVariable(\n                \"blanking_interval\", float, (\"ping_time\", \"beam\")\n            )\n            blanking_interval[:] = np.zeros(\n                (echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1)\n            )\n            blanking_interval.setncattr(\n                \"long_name\", \"Beam stabilisation applied(or not)\"\n            )\n            blanking_interval.units = \"s\"\n            blanking_interval.valid_min = 0.0\n\n            # Create calibrated_frequency variable\n            calibrated_frequency = grp.createVariable(\n                \"calibrated_frequency\", np.float64, ()\n            )\n            calibrated_frequency[:] = echodata[\"Sonar/Beam_group1\"][\n                \"frequency_nominal\"\n            ].values[i]\n            calibrated_frequency.setncattr(\"long_name\", \"Calibration gain frequencies\")\n            calibrated_frequency.units = \"Hz\"\n            calibrated_frequency.valid_min = 0.0\n\n            # Create echoangle_major variable (talk to joe about this)\n            echoangle_major = grp.createVariable(\n                \"echoangle_major\", angle_t, (\"ping_time\", \"beam\")\n            )\n            echoangle_major[:] = echoangle_major_data[:, i]\n            echoangle_major.setncattr(\n                \"long_name\", \"Echo arrival angle in the major beam coordinate\"\n            )\n            echoangle_major.units = \"arc_degree\"\n            echoangle_major.valid_range = [-180.0, 180.0]\n\n            # Create echoangle_minor variable\n            echoangle_minor = grp.createVariable(\n                \"echoangle_minor\", angle_t, (\"ping_time\", \"beam\")\n            )\n            echoangle_minor[:] = echoangle_minor_data[:, i]\n            echoangle_minor.setncattr(\n                \"long_name\", \"Echo arrival angle in the minor beam coordinate\"\n            )\n            echoangle_minor.units = \"arc_degree\"\n            echoangle_minor.valid_range = [-180.0, 180.0]\n\n            # Create echoangle_major sensitivity variable\n            echoangle_major_sensitivity = grp.createVariable(\n                \"echoangle_major_sensitivityr\", np.float64, ()\n            )\n            echoangle_major_sensitivity[:] = echodata[\"Sonar/Beam_group1\"][\n                \"angle_sensitivity_athwartship\"\n            ].values[i]\n            echoangle_major_sensitivity.setncattr(\n                \"long_name\", \"Major angle scaling factor\"\n            )\n            echoangle_major_sensitivity.units = \"1\"\n            echoangle_major_sensitivity.valid_min = 0.0\n\n            # Create echoangle_minor sensitivity variable\n            echoangle_minor_sensitivity = grp.createVariable(\n                \"echoangle_minor_sensitivity\", np.float64, ()\n            )\n            echoangle_minor_sensitivity[:] = echodata[\"Sonar/Beam_group1\"][\n                \"angle_sensitivity_alongship\"\n            ].values[i]\n            echoangle_minor_sensitivity.setncattr(\n                \"long_name\", \"Minor angle scaling factor\"\n            )\n            echoangle_minor_sensitivity.units = \"1\"\n            echoangle_minor_sensitivity.valid_min = 0.0\n\n            # Create equivalent_beam_angle variable (weird angle values)\n            equivalent_beam_angle = grp.createVariable(\n                \"equivalent_beam_angle\", np.float64, (\"ping_time\", \"beam\")\n            )\n            equivalent_beam_angle[:] = equivalent_beam_angle_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            equivalent_beam_angle.setncattr(\"long_name\", \"Equivalent beam angle\")\n\n            # Create frequency variable\n            frequency = grp.createVariable(\"frequency\", np.float64, ())\n            frequency[:] = echodata[\"Sonar/Beam_group1\"][\"frequency_nominal\"].values[i]\n            frequency.setncattr(\"long_name\", \"Calibration gain frequencies\")\n            frequency.units = \"Hz\"\n            frequency.valid_min = 0.0\n\n            # Create non_quantitative_processing variable\n            non_quantitative_processing = grp.createVariable(\n                \"non_quantitative_processing\", int, (\"ping_time\")\n            )\n            non_quantitative_processing[:] = np.zeros(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"]\n            )\n            non_quantitative_processing.setncattr(\n                \"long_name\",\n                \"Presence or not of non-quantitative processing applied to the backscattering data (sonar specific)\",\n            )\n\n            # Create platoform_latitude variable\n            platoform_latitude = grp.createVariable(\n                \"platoform_latitude\", np.float64, (\"ping_time\")\n            )\n            platoform_latitude[:] = echodata[\"Platform\"][\"latitude\"].interp(\n                time1=echodata[\"Platform\"].coords[\"time2\"].values, method=\"nearest\"\n            )\n            platoform_latitude.setncattr(\n                \"long_name\", \"Heading of the platform at time of the ping\"\n            )\n            platoform_latitude.units = \"degrees_north\"\n            platoform_latitude.valid_range = [-180.0, 180.0]\n\n            # Create platoform_longitude variable\n            platoform_longitude = grp.createVariable(\n                \"platoform_longitude\", np.float64, (\"ping_time\")\n            )\n            platoform_longitude[:] = echodata[\"Platform\"][\"longitude\"].interp(\n                time1=echodata[\"Platform\"].coords[\"time2\"].values, method=\"nearest\"\n            )\n            platoform_longitude.setncattr(\"long_name\", \"longitude\")\n            platoform_longitude.units = \"degrees_east\"\n            platoform_longitude.valid_range = [-180.0, 180.0]\n\n            # Create platoform_pitch variable\n            platform_pitch = grp.createVariable(\n                \"platform_pitch\", np.float64, (\"ping_time\")\n            )\n            platform_pitch[:] = echodata[\"Platform\"][\"pitch\"].values\n            platform_pitch.setncattr(\"long_name\", \"pitch_angle\")\n            platform_pitch.units = \"arc_degree\"\n            platform_pitch.valid_range = [-90.0, 90.0]\n\n            # Create platoform_roll variable\n            platoform_roll = grp.createVariable(\n                \"platform_roll\", np.float64, (\"ping_time\")\n            )\n            platoform_roll[:] = echodata[\"Platform\"][\"roll\"].values\n            platoform_roll.setncattr(\"long_name\", \"roll angle\")\n            platoform_roll.units = \"arc_degree\"\n\n            # Create platoform_vertical_offset variable\n            platoform_vertical_offset = grp.createVariable(\n                \"platoform_vertical_offset\", np.float64, (\"ping_time\")\n            )\n            platoform_vertical_offset[:] = echodata[\"Platform\"][\n                \"vertical_offset\"\n            ].values\n            platoform_vertical_offset.setncattr(\n                \"long_name\",\n                \"Platform vertical distance from reference point to the water line\",\n            )\n            platoform_vertical_offset.units = \"m\"\n\n            # Create rx_beam_rotation_phi variable\n            rx_beam_rotation_phi = grp.createVariable(\n                \"rx_beam_rotation_phi\", angle_t, (\"ping_time\", \"beam\")\n            )\n            rx_beam_rotation_phi[:] = rx_beam_rotation_phi_data[:, i]\n            rx_beam_rotation_phi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the x axis\"\n            )\n            rx_beam_rotation_phi.units = \"arc_degree\"\n            rx_beam_rotation_phi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_psi variable\n            rx_beam_rotation_psi = grp.createVariable(\n                \"rx_beam_rotation_psi\", np.float64, (\"ping_time\", \"beam\")\n            )\n            rx_beam_rotation_psi[:] = rx_beam_rotation_psi_data\n            rx_beam_rotation_psi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the z axis\"\n            )\n            rx_beam_rotation_psi.units = \"arc_degree\"\n            rx_beam_rotation_psi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_theta variable\n            rx_beam_rotation_theta = grp.createVariable(\n                \"rx_beam_roation_theta\", angle_t, (\"ping_time\", \"beam\")\n            )\n            rx_beam_rotation_theta[:] = rx_beam_rotation_theta_data[:, i]\n            rx_beam_rotation_theta.setncattr(\n                \"long_name\", \"receive beam angular rotation about the y axis\"\n            )\n            rx_beam_rotation_theta.units = \"arc_degree\"\n            rx_beam_rotation_theta.valid_range = [-90.0, 90.0]\n\n            # Create sample_interval variable\n            sample_interval = grp.createVariable(\n                \"sample_interval\", np.float64, (\"ping_time\", \"beam\")\n            )\n            sample_interval[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"sample_interval\"]\n                .transpose()\n                .values[:, i]\n            )\n            sample_interval.setncattr(\"long_name\", \"Equivalent beam angle\")\n            sample_interval.units = \"s\"\n            sample_interval.valid_min = 0.0\n            sample_interval.coordinates = (\n                \"ping_time platform_latitude platform_longitude\"\n            )\n\n            # Create sample_time_offset variable\n            sample_time_offset = grp.createVariable(\n                \"sample_time_offset\", np.float64, (\"ping_time\", \"beam\")\n            )\n            sample_time_offset[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"sample_time_offset\"]\n                .transpose()\n                .values[:, i]\n            )\n            sample_time_offset.setncattr(\n                \"long_name\",\n                \"Time offset that is subtracted from the timestamp of each sample\",\n            )\n            sample_time_offset.units = \"s\"\n\n            # Create transmit_duration_nominal variable\n            transmit_duration_nominal = grp.createVariable(\n                \"transmit_duration_nominal\", np.float64, (\"ping_time\", \"beam\")\n            )\n            transmit_duration_nominal[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"transmit_duration_nominal\"]\n                .transpose()\n                .values[:, i]\n            )\n            transmit_duration_nominal.setncattr(\n                \"long_name\", \"Nominal duration of transmitted pulse\"\n            )\n            transmit_duration_nominal.units = \"Hz\"\n            transmit_duration_nominal.valid_min = 0.0\n\n            # Create transmit_frequency_start variable\n            transmit_frequency_start = grp.createVariable(\n                \"transmit_frequency_start\", np.float64, (\"ping_time\")\n            )\n            transmit_frequency_start[:] = echodata[\"Sonar/Beam_group1\"][\n                \"transmit_frequency_start\"\n            ].values[i]\n            transmit_frequency_start.setncattr(\n                \"long_name\", \"Start frequency in transmitted pulse\"\n            )\n            transmit_frequency_start.units = \"Hz\"\n            transmit_frequency_start.valid_min = 0.0\n\n            # Create transmit_frequency_stop variable\n            transmit_frequency_stop = grp.createVariable(\n                \"transmit_frequency_stop\", np.float64, (\"ping_time\")\n            )\n            transmit_frequency_stop[:] = echodata[\"Sonar/Beam_group1\"][\n                \"transmit_frequency_stop\"\n            ].values[i]\n            transmit_frequency_stop.setncattr(\n                \"long_name\", \"Stop frequency in transmitted pulse\"\n            )\n            transmit_frequency_stop.units = \"Hz\"\n            transmit_frequency_stop.valid_min = 0.0\n\n            # Create transmit_power variable\n            transmit_power = grp.createVariable(\n                \"transmit_power\", np.float64, (\"ping_time\", \"beam\")\n            )\n            transmit_power[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"transmit_power\"].transpose().values[:, i]\n            )\n            transmit_power.setncattr(\"long_name\", \"Nominal transmit power\")\n            transmit_power.units = \"W\"\n            transmit_power.valid_min = 0.0\n\n            # Create transmit_type\n            transmit_type = grp.createVariable(\"transmit_type\", np.float64, ())\n            transmit_type[:] = 0\n            transmit_type.setncattr(\"long_name\", \"Type of transmitted pulse\")\n\n            # Create tx_beam_rotation_phi variable\n            tx_beam_roation_phi = grp.createVariable(\n                \"tx_beam_roation_phi\", angle_t, (\"ping_time\", \"beam\")\n            )\n            tx_beam_roation_phi[:] = rx_beam_rotation_phi_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            tx_beam_roation_phi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the x axis\"\n            )\n            tx_beam_roation_phi.units = \"arc_degree\"\n            tx_beam_roation_phi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_psi variable\n            tx_beam_roation_psi = grp.createVariable(\n                \"tx_beam_roation_psi\", np.float32, (\"ping_time\", \"beam\")\n            )\n            tx_beam_roation_psi[:] = rx_beam_rotation_psi_data\n            tx_beam_roation_psi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the z axis\"\n            )\n            tx_beam_roation_psi.units = \"arc_degree\"\n            tx_beam_roation_psi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_theta variable\n            tx_beam_roation_theta = grp.createVariable(\n                \"tx_beam_roation_theta\", angle_t, (\"ping_time\", \"beam\")\n            )\n            tx_beam_roation_theta[:] = rx_beam_rotation_theta_data[:, i]\n            tx_beam_roation_theta.setncattr(\n                \"long_name\", \"receive beam angular rotation about the y axis\"\n            )\n            tx_beam_roation_theta.units = \"arc_degree\"\n            tx_beam_roation_theta.valid_range = [-90.0, 90.0]\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ices.write_ek80_beamgroup_to_netcdf","title":"<code>write_ek80_beamgroup_to_netcdf(echodata, export_file)</code>","text":"<p>Writes echodata Beam_group ds to a Beam_groupX netcdf file.</p> <p>Args: echodata (echopype.echodata): Echopype echodata object containing beam_group_data. (echopype.DataArray): Echopype DataArray to be written. export_file (str or Path): Path to the NetCDF file.</p> Source code in <code>src\\aalibrary\\utils\\ices.py</code> <pre><code>def write_ek80_beamgroup_to_netcdf(echodata, export_file):\n    \"\"\"Writes echodata Beam_group ds to a Beam_groupX netcdf file.\n\n    Args:\n    echodata (echopype.echodata): Echopype echodata object containing beam_group_data.\n    (echopype.DataArray): Echopype DataArray to be written.\n    export_file (str or Path): Path to the NetCDF file.\n    \"\"\"\n    ragged_backscatter_r_data = ragged_data_type_ices(echodata, \"backscatter_r\")\n    ragged_backscatter_i_data = ragged_data_type_ices(echodata, \"backscatter_i\")\n    beamwidth_receive_major_data = correct_dimensions_ices(\n        echodata, \"beamwidth_twoway_athwartship\"\n    )\n    beamwidth_receive_minor_data = correct_dimensions_ices(\n        echodata, \"beamwidth_twoway_alongship\"\n    )\n    echoangle_major_data = correct_dimensions_ices(echodata, \"angle_offset_athwartship\")\n    echoangle_minor_data = correct_dimensions_ices(echodata, \"angle_offset_alongship\")\n    equivalent_beam_angle_data = correct_dimensions_ices(\n        echodata, \"equivalent_beam_angle\"\n    )\n    rx_beam_rotation_phi_data = (\n        correct_dimensions_ices(echodata, \"angle_offset_athwartship\") * -1\n    )\n    rx_beam_rotation_psi_data = np.zeros(\n        (echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1)\n    )\n    rx_beam_rotation_theta_data = correct_dimensions_ices(\n        echodata, \"angle_offset_alongship\"\n    )\n\n    for i in range(echodata[\"Sonar/Beam_group1\"].sizes[\"channel\"]):\n\n        with netCDF4.Dataset(export_file, \"a\", format=\"netcdf4\") as ncfile:\n            grp = ncfile.createGroup(f\"Sonar/Beam_group{i+1}\")\n            grp.setncattr(\"beam_mode\", echodata[\"Sonar/Beam_group1\"].attrs[\"beam_mode\"])\n            grp.setncattr(\n                \"conversion_equation_type\",\n                echodata[\"Sonar/Beam_group1\"].attrs[\"conversion_equation_t\"],\n            )\n            grp.setncattr(\n                \"long_name\", echodata[\"Sonar/Beam_group1\"].coords[\"channel\"].values[i]\n            )\n\n            # Create the VLEN type for 32-bit floats\n            sample_t = grp.createVLType(np.float32, \"sample_t\")\n\n            # Create ping_time dimension and ping_time coordinate variable\n            grp.createDimension(\"ping_time\", None)\n\n            ping_time_var = grp.createVariable(\"ping_time\", np.int64, (\"ping_time\",))\n            ping_time_var.units = \"nanoseconds since 1970-01-01 00:00:00Z\"\n            ping_time_var.standard_name = \"time\"\n            ping_time_var.long_name = \"Time-stamp of each ping\"\n            ping_time_var.axis = \"T\"\n            ping_time_var.calendar = \"gregorian\"\n            ping_time_var[:] = echodata[\"Sonar/Beam_group1\"].coords[\n                \"ping_time\"\n            ].values - np.datetime64(\"1970-01-01T00:00:00Z\")\n\n            # Create beam dimension and coordinate variable\n            grp.createDimension(\"beam\", 1)\n\n            beam_var = grp.createVariable(\"beam\", \"S1\", (\"beam\",))\n            beam_var.long_name = \"Beam name\"\n            beam_var[:] = echodata[\"Sonar/Beam_group1\"].coords[\"channel\"].values[i]\n\n            # Create beam dimension and coordinate variable\n            grp.createDimension(\"sub_beam\", 4)\n\n            sub_beam_var = grp.createVariable(\"sub_beam\", np.int64, (\"sub_beam\",))\n            sub_beam_var.long_name = \"Beam quadrant number\"\n            sub_beam_var[:] = echodata[\"Sonar/Beam_group1\"].coords[\"beam\"].values\n\n            # Create backscatter_r variable\n            backscatter_r = grp.createVariable(\n                \"backscatter_r\",\n                sample_t,\n                (\"ping_time\", \"beam\", \"sub_beam\"),\n            )\n            backscatter_r[:] = ragged_backscatter_r_data[:, i, :]\n            backscatter_r.setncattr(\n                \"long_name\", \"Raw backscatter measurements (real part)\"\n            )\n            backscatter_r.units = \"dB\"\n\n            # Create backscatter_i variable\n            backscatter_i = grp.createVariable(\n                \"backscatter_i\", sample_t, (\"ping_time\", \"beam\", \"sub_beam\")\n            )\n            backscatter_i[:] = ragged_backscatter_i_data[:, i, :].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"],\n                1,\n                echodata[\"Sonar/Beam_group1\"].sizes[\"beam\"],\n            )\n            backscatter_i.setncattr(\n                \"long_name\", \"Raw backscatter measurements (imaginary part)\"\n            )\n            backscatter_i.units = \"dB\"\n\n            # Create beam_stabilisation variable\n            beam_stablisation = grp.createVariable(\n                \"beam_stablisation\", int, (\"ping_time\", \"beam\")\n            )\n            beam_stablisation[:] = np.zeros(\n                (echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1)\n            )\n            beam_stablisation.setncattr(\n                \"long_name\", \"Beam stabilisation applied(or not)\"\n            )\n\n            # Create beam_type variable\n            beam_type = grp.createVariable(\"beam_type\", int, ())\n            beam_type[:] = echodata[\"Sonar/Beam_group1\"][\"beam_type\"].values[i]\n            beam_type.setncattr(\"long_name\", \"type of transducer (0-single, 1-split)\")\n\n            # Create beamwidth_receive_major variable\n            beamwidth_receive_major = grp.createVariable(\n                \"beamwidth_receive_major\", np.float32, (\"ping_time\", \"beam\")\n            )\n            beamwidth_receive_major[:] = beamwidth_receive_major_data[:, i]\n            beamwidth_receive_major.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along major (horizontal) axis of beam\",\n            )\n            beamwidth_receive_major.units = \"arc_degree\"\n            beamwidth_receive_major.valid_range = [0.0, 360.0]\n\n            # stopped here\n            # Create beamwidth_receive_minor variable\n            beamwidth_receive_minor = grp.createVariable(\n                \"beamwidth_receive_minor\", np.float32, (\"ping_time\", \"beam\")\n            )\n            beamwidth_receive_minor[:] = beamwidth_receive_minor_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            beamwidth_receive_minor.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along minor (vertical) axis of beam\",\n            )\n            beamwidth_receive_minor.units = \"arc_degree\"\n            beamwidth_receive_minor.valid_range = [0.0, 360.0]\n\n            beamwidth_transmit_major = grp.createVariable(\n                \"beamwidth_transmit_major\", np.float32, (\"ping_time\", \"beam\")\n            )\n            # Create beamwidth_transmit_major variable\n            beamwidth_transmit_major[:] = beamwidth_receive_major_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            beamwidth_transmit_major.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along major (horizontal) axis of beam\",\n            )\n            beamwidth_transmit_major.units = \"arc_degree\"\n            beamwidth_transmit_major.valid_range = [0.0, 360.0]\n\n            # Create beamwidth_transmit_minor variable\n            beamwidth_transmit_minor = grp.createVariable(\n                \"beamwidth_transmit_minor\", np.float32, (\"ping_time\", \"beam\")\n            )\n            beamwidth_transmit_minor[:] = beamwidth_receive_minor_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            beamwidth_transmit_minor.setncattr(\n                \"long_name\",\n                \"Half power one-way receive beam width along minor (vertical) axis of beam\",\n            )\n            beamwidth_transmit_minor.units = \"arc_degree\"\n            beamwidth_transmit_minor.valid_range = [0.0, 360.0]\n\n            # Create blanking_interval variable\n            blanking_interval = grp.createVariable(\n                \"blanking_interval\", np.float32, (\"ping_time\", \"beam\")\n            )\n            blanking_interval[:] = np.zeros(\n                (echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1)\n            )\n            blanking_interval.setncattr(\n                \"long_name\", \"Beam stabilisation applied(or not)\"\n            )\n            blanking_interval.units = \"s\"\n            blanking_interval.valid_min = 0.0\n\n            # Create calibrated_frequency variable\n            calibrated_frequency = grp.createVariable(\n                \"calibrated_frequency\", np.float64, ()\n            )\n            calibrated_frequency[:] = echodata[\"Sonar/Beam_group1\"][\n                \"frequency_nominal\"\n            ].values[i]\n            calibrated_frequency.setncattr(\"long_name\", \"Calibration gain frequencies\")\n            calibrated_frequency.units = \"Hz\"\n            calibrated_frequency.valid_min = 0.0\n\n            # Create echoangle_major variable (talk to joe about this)\n            echoangle_major = grp.createVariable(\n                \"echoangle_major\", np.float32, (\"ping_time\", \"beam\")\n            )\n            echoangle_major[:] = echoangle_major_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            echoangle_major.setncattr(\n                \"long_name\", \"Echo arrival angle in the major beam coordinate\"\n            )\n            echoangle_major.units = \"arc_degree\"\n            echoangle_major.valid_range = [-180.0, 180.0]\n\n            # Create echoangle_minor variable\n            echoangle_minor = grp.createVariable(\n                \"echoangle_minor\", np.float32, (\"ping_time\", \"beam\")\n            )\n            echoangle_minor[:] = echoangle_minor_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            echoangle_minor.setncattr(\n                \"long_name\", \"Echo arrival angle in the minor beam coordinate\"\n            )\n            echoangle_minor.units = \"arc_degree\"\n            echoangle_minor.valid_range = [-180.0, 180.0]\n\n            # Create echoangle_major sensitivity variable\n            echoangle_major_sensitivity = grp.createVariable(\n                \"echoangle_major_sensitivityr\", np.float64, ()\n            )\n            echoangle_major_sensitivity[:] = echodata[\"Sonar/Beam_group1\"][\n                \"angle_sensitivity_athwartship\"\n            ].values[i]\n            echoangle_major_sensitivity.setncattr(\n                \"long_name\", \"Major angle scaling factor\"\n            )\n            echoangle_major_sensitivity.units = \"1\"\n            echoangle_major_sensitivity.valid_min = 0.0\n\n            # Create echoangle_minor sensitivity variable\n            echoangle_minor_sensitivity = grp.createVariable(\n                \"echoangle_minor_sensitivity\", np.float64, ()\n            )\n            echoangle_minor_sensitivity[:] = echodata[\"Sonar/Beam_group1\"][\n                \"angle_sensitivity_alongship\"\n            ].values[i]\n            echoangle_minor_sensitivity.setncattr(\n                \"long_name\", \"Minor angle scaling factor\"\n            )\n            echoangle_minor_sensitivity.units = \"1\"\n            echoangle_minor_sensitivity.valid_min = 0.0\n\n            # Create equivalent_beam_angle variable (weird angle values)\n            equivalent_beam_angle = grp.createVariable(\n                \"equivalent_beam_angle\", np.float32, (\"ping_time\", \"beam\")\n            )\n            equivalent_beam_angle[:] = equivalent_beam_angle_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            equivalent_beam_angle.setncattr(\"long_name\", \"Equivalent beam angle\")\n\n            # Create frequency variable\n            frequency = grp.createVariable(\"frequency\", np.float64, ())\n            frequency[:] = echodata[\"Sonar/Beam_group1\"][\"frequency_nominal\"].values[i]\n            frequency.setncattr(\"long_name\", \"Calibration gain frequencies\")\n            frequency.units = \"Hz\"\n            frequency.valid_min = 0.0\n\n            # Create non_quantitative_processing variable\n            non_quantitative_processing = grp.createVariable(\n                \"non_quantitative_processing\", int, (\"ping_time\")\n            )\n            non_quantitative_processing[:] = np.zeros(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"]\n            )\n            non_quantitative_processing.setncattr(\n                \"long_name\",\n                \"Presence or not of non-quantitative processing applied to the backscattering data (sonar specific)\",\n            )\n\n            # Create platform_heading variable\n            platform_heading = grp.createVariable(\n                \"platform_heading\", np.float32, (\"ping_time\")\n            )\n            platform_heading[:] = echodata[\"Platform\"][\"heading\"].values\n            platform_heading.setncattr(\"long_name\", \"Platform heading(true)\")\n            platform_heading.units = \"degrees_north\"\n            platform_heading.valid_range = [0, 360.0]\n\n            # Create platform_latitude variable\n            platform_latitude = grp.createVariable(\n                \"platform_latitude\", np.float32, (\"ping_time\")\n            )\n            platform_latitude[:] = echodata[\"Platform\"][\"latitude\"].interp(\n                time1=echodata[\"Platform\"].coords[\"time2\"].values, method=\"nearest\"\n            )\n            platform_latitude.setncattr(\n                \"long_name\", \"Heading of the platform at time of the ping\"\n            )\n            platform_latitude.units = \"degrees_north\"\n            platform_latitude.valid_range = [-180.0, 180.0]\n\n            # Create platform_longitude variable\n            platform_longitude = grp.createVariable(\n                \"platform_longitude\", np.float64, (\"ping_time\")\n            )\n            platform_longitude[:] = echodata[\"Platform\"][\"longitude\"].interp(\n                time1=echodata[\"Platform\"].coords[\"time2\"].values, method=\"nearest\"\n            )\n            platform_longitude.setncattr(\"long_name\", \"longitude\")\n            platform_longitude.units = \"degrees_east\"\n            platform_longitude.valid_range = [-180.0, 180.0]\n\n            # Create platform_pitch variable\n            platform_pitch = grp.createVariable(\n                \"platform_pitch\", np.float64, (\"ping_time\")\n            )\n            platform_pitch[:] = echodata[\"Platform\"][\"pitch\"].values\n            platform_pitch.setncattr(\"long_name\", \"pitch_angle\")\n            platform_pitch.units = \"arc_degree\"\n            platform_pitch.valid_range = [-90.0, 90.0]\n\n            # Create platform_roll variable\n            platform_roll = grp.createVariable(\n                \"platform_roll\", np.float64, (\"ping_time\")\n            )\n            platform_roll[:] = echodata[\"Platform\"][\"roll\"].values\n            platform_roll.setncattr(\"long_name\", \"roll angle\")\n            platform_roll.units = \"arc_degree\"\n\n            # Create platform_vertical_offset variable\n            platform_vertical_offset = grp.createVariable(\n                \"platform_vertical_offset\", np.float64, (\"ping_time\")\n            )\n            platform_vertical_offset[:] = echodata[\"Platform\"][\"vertical_offset\"].values\n            platform_vertical_offset.setncattr(\n                \"long_name\",\n                \"Platform vertical distance from reference point to the water line\",\n            )\n            platform_vertical_offset.units = \"m\"\n\n            # Create rx_beam_rotation_phi variable\n            rx_beam_rotation_phi = grp.createVariable(\n                \"rx_beam_rotation_phi\", np.float32, (\"ping_time\", \"beam\")\n            )\n            rx_beam_rotation_phi[:] = rx_beam_rotation_phi_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            rx_beam_rotation_phi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the x axis\"\n            )\n            rx_beam_rotation_phi.units = \"arc_degree\"\n            rx_beam_rotation_phi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_psi variable\n            rx_beam_rotation_psi = grp.createVariable(\n                \"rx_beam_rotation_psi\", np.float32, (\"ping_time\", \"beam\")\n            )\n            rx_beam_rotation_psi[:] = rx_beam_rotation_psi_data\n            rx_beam_rotation_psi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the z axis\"\n            )\n            rx_beam_rotation_psi.units = \"arc_degree\"\n            rx_beam_rotation_psi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_theta variable\n            rx_beam_rotation_theta = grp.createVariable(\n                \"rx_beam_roation_theta\", np.float32, (\"ping_time\", \"beam\")\n            )\n            rx_beam_rotation_theta[:] = rx_beam_rotation_theta_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            rx_beam_rotation_theta.setncattr(\n                \"long_name\", \"receive beam angular rotation about the y axis\"\n            )\n            rx_beam_rotation_theta.units = \"arc_degree\"\n            rx_beam_rotation_theta.valid_range = [-90.0, 90.0]\n\n            # Create sample_interval variable\n            sample_interval = grp.createVariable(\n                \"sample_interval\", np.float64, (\"ping_time\", \"beam\")\n            )\n            sample_interval[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"sample_interval\"]\n                .transpose()\n                .values[:, i]\n            )\n            sample_interval.setncattr(\"long_name\", \"Equivalent beam angle\")\n            sample_interval.units = \"s\"\n            sample_interval.valid_min = 0.0\n            sample_interval.coordinates = (\n                \"ping_time platform_latitude platform_longitude\"\n            )\n\n            # Create sample_time_offset variable\n            sample_time_offset = grp.createVariable(\n                \"sample_time_offset\", np.float32, (\"ping_time\", \"beam\")\n            )\n            sample_time_offset[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"sample_time_offset\"]\n                .transpose()\n                .values[:, i]\n            )\n            sample_time_offset.setncattr(\n                \"long_name\",\n                \"Time offset that is subtracted from the timestamp of each sample\",\n            )\n            sample_time_offset.units = \"s\"\n\n            # Create transmit_duration_nominal variable\n            transmit_duration_nominal = grp.createVariable(\n                \"transmit_duration_nominal\", np.float32, (\"ping_time\", \"beam\")\n            )\n            transmit_duration_nominal[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"transmit_duration_nominal\"]\n                .transpose()\n                .values[:, i]\n                .astype(np.float32)\n            )\n            transmit_duration_nominal.setncattr(\n                \"long_name\", \"Nominal duration of transmitted pulse\"\n            )\n            transmit_duration_nominal.units = \"Hz\"\n            transmit_duration_nominal.valid_min = 0.0\n\n            # Create transmit_frequency_start variable\n            transmit_frequency_start = grp.createVariable(\n                \"transmit_frequency_start\", np.float32, (\"ping_time\", \"beam\")\n            )\n            transmit_frequency_start[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"transmit_frequency_start\"]\n                .transpose()\n                .values[:, i]\n                .astype(np.float32)\n            )\n            transmit_frequency_start.setncattr(\n                \"long_name\", \"Start frequency in transmitted pulse\"\n            )\n            transmit_frequency_start.units = \"Hz\"\n            transmit_frequency_start.valid_min = 0.0\n\n            # Create transmit_frequency_stop variable\n            transmit_frequency_stop = grp.createVariable(\n                \"transmit_frequency_stop\", np.float32, (\"ping_time\", \"beam\")\n            )\n            transmit_frequency_stop[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"transmit_frequency_stop\"]\n                .transpose()\n                .values[:, i]\n                .astype(np.float32)\n            )\n            transmit_frequency_stop.setncattr(\n                \"long_name\", \"Stop frequency in transmitted pulse\"\n            )\n            transmit_frequency_stop.units = \"Hz\"\n            transmit_frequency_stop.valid_min = 0.0\n\n            # Create transmit_power variable\n            transmit_power = grp.createVariable(\n                \"transmit_power\", np.float32, (\"ping_time\", \"beam\")\n            )\n            transmit_power[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"transmit_power\"]\n                .transpose()\n                .values[:, i]\n                .astype(np.float32)\n            )\n            transmit_power.setncattr(\"long_name\", \"Nominal transmit power\")\n            transmit_power.units = \"W\"\n            transmit_power.valid_min = 0.0\n\n            # Create transmit_type\n            transmit_type = grp.createVariable(\n                \"transmit_type\", np.float32, (\"ping_time\", \"beam\")\n            )\n            transmit_type[:] = (\n                echodata[\"Sonar/Beam_group1\"][\"transmit_type\"]\n                .where(echodata[\"Sonar/Beam_group1\"][\"transmit_type\"] != \"CW\", 0)\n                .where(echodata[\"Sonar/Beam_group1\"][\"transmit_type\"] != \"LFM\", 1)\n                .transpose()\n                .values[:, i]\n                .astype(np.float32)\n            )\n            transmit_type.setncattr(\"long_name\", \"Type of transmitted pulse\")\n\n            # Create tx_beam_rotation_phi variable\n            tx_beam_roation_phi = grp.createVariable(\n                \"tx_beam_roation_phi\", np.float32, (\"ping_time\", \"beam\")\n            )\n            tx_beam_roation_phi[:] = rx_beam_rotation_phi_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            tx_beam_roation_phi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the x axis\"\n            )\n            tx_beam_roation_phi.units = \"arc_degree\"\n            tx_beam_roation_phi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_psi variable\n            tx_beam_roation_psi = grp.createVariable(\n                \"tx_beam_roation_psi\", np.float32, (\"ping_time\", \"beam\")\n            )\n            tx_beam_roation_psi[:] = rx_beam_rotation_psi_data\n            tx_beam_roation_psi.setncattr(\n                \"long_name\", \"receive beam angular rotation about the z axis\"\n            )\n            tx_beam_roation_psi.units = \"arc_degree\"\n            tx_beam_roation_psi.valid_range = [-180.0, 180.0]\n\n            # Create rx_beam_rotation_theta variable\n            tx_beam_roation_theta = grp.createVariable(\n                \"tx_beam_roation_theta\", np.float32, (\"ping_time\", \"beam\")\n            )\n            tx_beam_roation_theta[:] = rx_beam_rotation_theta_data[:, i].reshape(\n                echodata[\"Sonar/Beam_group1\"].sizes[\"ping_time\"], 1\n            )\n            tx_beam_roation_theta.setncattr(\n                \"long_name\", \"receive beam angular rotation about the y axis\"\n            )\n            tx_beam_roation_theta.units = \"arc_degree\"\n            tx_beam_roation_theta.valid_range = [-90.0, 90.0]\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.nc_reader","title":"<code>nc_reader</code>","text":"<p>This file is used to get header information out of a NetCDF file. The code reads a .nc file and returns a dict with all of the attributes gathered.</p> <p>Functions:</p> Name Description <code>get_netcdf_header</code> <p>Reads a NetCDF file and returns its header as a dictionary.</p>"},{"location":"documentation/utils/#aalibrary.utils.nc_reader.get_netcdf_header","title":"<code>get_netcdf_header(file_path)</code>","text":"<p>Reads a NetCDF file and returns its header as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the NetCDF file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing global attributes, dimensions, and</p> <code>dict</code> <p>variables.</p> Source code in <code>src\\aalibrary\\utils\\nc_reader.py</code> <pre><code>def get_netcdf_header(file_path: str) -&gt; dict:\n    \"\"\"Reads a NetCDF file and returns its header as a dictionary.\n\n    Args:\n        file_path (str): Path to the NetCDF file.\n\n    Returns:\n        dict: Dictionary containing global attributes, dimensions, and\n        variables.\n    \"\"\"\n    header_info = {}\n\n    with Dataset(file_path, \"r\") as nc_file:\n        # Extract global attributes\n        header_info[\"global_attributes\"] = {\n            attr: getattr(nc_file, attr) for attr in nc_file.ncattrs()\n        }\n\n        # Extract dimensions\n        header_info[\"dimensions\"] = {\n            dim: len(nc_file.dimensions[dim]) for dim in nc_file.dimensions\n        }\n\n        # Extract variable metadata\n        header_info[\"variables\"] = {\n            var: {\n                \"dimensions\": nc_file.variables[var].dimensions,\n                \"shape\": nc_file.variables[var].shape,\n                \"dtype\": str(nc_file.variables[var].dtype),\n                \"attributes\": {\n                    attr: getattr(nc_file.variables[var], attr)\n                    for attr in nc_file.variables[var].ncattrs()\n                },\n            }\n            for var in nc_file.variables\n        }\n\n    return header_info\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils","title":"<code>ncei_utils</code>","text":"<p>This file contains code pertaining to auxiliary functions related to parsing through NCEI's s3 bucket.</p> <p>Functions:</p> Name Description <code>check_if_tugboat_metadata_json_exists_in_survey</code> <p>Checks whether a Tugboat metadata JSON file exists within a survey.</p> <code>download_single_file_from_aws</code> <p>Safely downloads a file from AWS storage bucket, aka the NCEI</p> <code>download_specific_folder_from_ncei</code> <p>Downloads a specific folder and all of its contents from NCEI to a local</p> <code>get_all_echosounders_in_a_survey</code> <p>Gets all of the echosounders in a particular survey from NCEI.</p> <code>get_all_echosounders_that_exist_in_ncei</code> <p>Gets a list of all possible echosounders from NCEI.</p> <code>get_all_file_names_from_survey</code> <p>Gets all of the file names from a particular NCEI survey.</p> <code>get_all_file_names_in_a_surveys_echosounder_folder</code> <p>Gets all of the file names from a particular NCEI survey's echosounder</p> <code>get_all_metadata_files_in_survey</code> <p>Gets all of the metadata file names from a particular NCEI survey.</p> <code>get_all_raw_file_names_from_survey</code> <p>Gets all of the file names from a particular NCEI survey.</p> <code>get_all_ship_names_in_ncei</code> <p>Gets all of the ship names from NCEI. This is based on all of the</p> <code>get_all_survey_names_from_a_ship</code> <p>Gets a list of all of the survey names that exist under a ship name.</p> <code>get_all_surveys_in_ncei</code> <p>Gets a list of all of the possible survey names from NCEI.</p> <code>get_checksum_sha256_from_s3</code> <p>Gets the SHA-256 checksum of the s3 object.</p> <code>get_closest_ncei_formatted_ship_name</code> <p>Gets the closest NCEI formatted ship name to the given ship name.</p> <code>get_echosounder_from_raw_file</code> <p>Gets the echosounder used for a particular raw file.</p> <code>get_file_size_from_s3</code> <p>Gets the file size of an object in s3.</p> <code>get_folder_size_from_s3</code> <p>Gets the folder size in bytes from S3.</p> <code>get_random_raw_file_from_ncei</code> <p>Creates a test raw file for NCEI. This is used for testing purposes</p> <code>search_ncei_file_objects_for_string</code> <p>Searches NCEI for a file type's object keys that contain a particular</p> <code>search_ncei_objects_for_string</code> <p>Searches NCEI for object keys that contain a particular string. This</p>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.check_if_tugboat_metadata_json_exists_in_survey","title":"<code>check_if_tugboat_metadata_json_exists_in_survey(ship_name='', survey_name='', s3_bucket=None)</code>","text":"<p>Checks whether a Tugboat metadata JSON file exists within a survey. Returns the file's object key or None if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Defaults to None. NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use the <code>get_all_ship_names_in_ncei</code> function to see all possible NCEI ship names.</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name exactly as it is in NCEI. Defaults to \"\".</p> <code>''</code> <code>s3_bucket</code> <code>resource</code> <p>The bucket resource object. Defaults to None.</p> <code>None</code> <p>Returns:     Union[str, None]: Returns the file's object key string or None if it         does not exist.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def check_if_tugboat_metadata_json_exists_in_survey(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    s3_bucket: boto3.resource = None,\n) -&gt; Union[str, None]:\n    \"\"\"Checks whether a Tugboat metadata JSON file exists within a survey.\n    Returns the file's object key or None if it does not exist.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Defaults to None.\n            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n            ship names.\n        survey_name (str, optional): The survey name exactly as it is in NCEI.\n            Defaults to \"\".\n        s3_bucket (boto3.resource, optional): The bucket resource object.\n            Defaults to None.\n    Returns:\n        Union[str, None]: Returns the file's object key string or None if it\n            does not exist.\n    \"\"\"\n\n    # Find all metadata files within the metadata/ folder in NCEI\n    all_metadata_obj_keys = list_all_objects_in_s3_bucket_location(\n        prefix=f\"data/raw/{ship_name}/{survey_name}/metadata\",\n        s3_resource=s3_bucket,\n    )\n\n    for obj_key, file_name in all_metadata_obj_keys:\n        # Handle for main metadata file for upload to BigQuery.\n        if file_name.endswith(\"metadata.json\"):\n            return obj_key\n\n    return None\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.download_single_file_from_aws","title":"<code>download_single_file_from_aws(file_url='', download_location='')</code>","text":"<p>Safely downloads a file from AWS storage bucket, aka the NCEI repository.</p> <p>Parameters:</p> Name Type Description Default <code>file_url</code> <code>str</code> <p>The file url. Defaults to \"\".</p> <code>''</code> <code>download_location</code> <code>str</code> <p>The local download location for the file. Defaults to \"\".</p> <code>''</code> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def download_single_file_from_aws(\n    file_url: str = \"\",\n    download_location: str = \"\",\n):\n    \"\"\"Safely downloads a file from AWS storage bucket, aka the NCEI\n    repository.\n\n    Args:\n        file_url (str, optional): The file url. Defaults to \"\".\n        download_location (str, optional): The local download location for the\n            file. Defaults to \"\".\n    \"\"\"\n\n    try:\n        _, s3_resource, s3_bucket = create_s3_objs()\n    except Exception as e:\n        logging.error(\"CANNOT ESTABLISH CONNECTION TO S3 BUCKET..\\n{%s}\", e)\n        raise\n\n    # We replace the beginning of common file paths\n    file_url = get_object_key_for_s3(file_url=file_url)\n    file_name = get_file_name_from_url(file_url)\n\n    # Check if the file exists in s3\n    file_exists = check_if_file_exists_in_s3(\n        object_key=file_url,\n        s3_resource=s3_resource,\n        s3_bucket_name=s3_bucket.name,\n    )\n\n    if file_exists:\n        # Finally download the file.\n        try:\n            logging.info(\"DOWNLOADING `%s`...\", file_name)\n            s3_bucket.download_file(file_url, download_location)\n            logging.info(\n                \"DOWNLOADED `%s` TO `%s`\", file_name, download_location\n            )\n        except Exception as e:\n            logging.error(\n                \"ERROR DOWNLOADING FILE `%s` DUE TO\\n%s\", file_name, e\n            )\n            raise\n    else:\n        logging.error(\n            \"FILE %s DOES NOT EXIST IN NCEI S3 BUCKET. SKIPPING...\", file_name\n        )\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.download_specific_folder_from_ncei","title":"<code>download_specific_folder_from_ncei(folder_prefix='', download_directory='', debug=False)</code>","text":"<p>Downloads a specific folder and all of its contents from NCEI to a local directory.</p> <p>Parameters:</p> Name Type Description Default <code>folder_prefix</code> <code>str</code> <p>The folder's path in the s3 bucket. Ex. 'data/raw/Reuben_Lasker/' Defaults to \"\".</p> <code>''</code> <code>download_directory</code> <code>str</code> <p>The directory you want to download the folder and all of its contents to. Defaults to \"\".</p> <code>''</code> <code>debug</code> <code>bool</code> <p>Whether or not to print debug information. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def download_specific_folder_from_ncei(\n    folder_prefix: str = \"\", download_directory: str = \"\", debug: bool = False\n):\n    \"\"\"Downloads a specific folder and all of its contents from NCEI to a local\n    directory.\n\n    Args:\n        folder_prefix (str, optional): The folder's path in the s3 bucket.\n            Ex. 'data/raw/Reuben_Lasker/'\n            Defaults to \"\".\n        download_directory (str, optional): The directory you want to download\n            the folder and all of its contents to. Defaults to \"\".\n        debug (bool, optional): Whether or not to print debug information.\n            Defaults to False.\n    \"\"\"\n\n    if not folder_prefix.endswith(\"/\"):\n        folder_prefix += \"/\"\n\n    assert (download_directory is not None) and (\n        download_directory != \"\"\n    ), \"You must provide a download_directory to download the folder to.\"\n\n    if debug:\n        logging.debug(\"FORMATTED DOWNLOAD DIRECTORY: %s\", download_directory)\n\n    # Get all s3 objects for the survey\n    print(f\"GETTING ALL S3 OBJECTS FOR FOLDER `{folder_prefix}`...\")\n    _, s3_resource, _ = create_s3_objs()\n    s3_objects = list_all_objects_in_s3_bucket_location(\n        prefix=folder_prefix,\n        s3_resource=s3_resource,\n        return_full_paths=True,\n    )\n    print(f\"FOUND {len(s3_objects)} FILES.\")\n\n    subdirs = set()\n    # Get the subfolders from object keys\n    for s3_object in s3_objects:\n        # Skip folders\n        if s3_object.endswith(\"/\"):\n            continue\n        # Get the subfolder structure from the object key\n        subfolder_key = os.sep.join(\n            s3_object.replace(\"data/raw/\", \"\").split(\"/\")[:-1]\n        )\n        subdirs.add(subfolder_key)\n    for subdir in subdirs:\n        os.makedirs(os.sep.join([download_directory, subdir]), exist_ok=True)\n\n    # Create the directory if it doesn't exist.\n    if not os.path.isdir(download_directory):\n        print(f\"CREATING download_directory `{download_directory}`\")\n        os.makedirs(download_directory, exist_ok=True)\n    # normalize the path\n    download_directory = os.path.normpath(download_directory)\n    print(\"CREATED DOWNLOAD SUBDIRECTORIES.\")\n\n    for idx, object_key in enumerate(tqdm(s3_objects, desc=\"Downloading\")):\n        file_name = object_key.split(\"/\")[-1]\n        local_object_path = object_key.replace(\"data/raw/\", \"\")\n        download_location = os.path.normpath(\n            os.sep.join([download_directory, local_object_path])\n        )\n        download_single_file_from_aws(\n            file_url=object_key, download_location=download_location\n        )\n    print(f\"DOWNLOAD COMPLETE {os.path.abspath(download_directory)}.\")\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_echosounders_in_a_survey","title":"<code>get_all_echosounders_in_a_survey(ship_name='', survey_name='', s3_client=None, return_full_paths=False)</code>","text":"<p>Gets all of the echosounders in a particular survey from NCEI.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Defaults to None. NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use the <code>get_all_ship_names_in_ncei</code> function to see all possible NCEI ship names.</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name exactly as it is in NCEI. Defaults to \"\".</p> <code>''</code> <code>s3_client</code> <code>client</code> <p>The client used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being the echosounder name. Whether these are full paths or just folder names are specified by the <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_echosounders_in_a_survey(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    s3_client: boto3.client = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the echosounders in a particular survey from NCEI.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Defaults to None.\n            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n            ship names.\n        survey_name (str, optional): The survey name exactly as it is in NCEI.\n            Defaults to \"\".\n        s3_client (boto3.client, optional): The client used to perform this\n            operation. Defaults to None, but creates a client for you instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings, each being the echosounder name. Whether\n            these are full paths or just folder names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n\n    survey_prefix = f\"data/raw/{ship_name}/{survey_name}/\"\n    all_survey_folder_names = get_subdirectories_in_s3_bucket_location(\n        prefix=survey_prefix,\n        s3_client=s3_client,\n        return_full_paths=return_full_paths,\n        bucket_name=\"noaa-wcsd-pds\",\n    )\n    # Get echosounder folders by ignoring the other metadata folders\n    all_echosounders = []\n    for folder_name in all_survey_folder_names:\n        if (\n            (\"calibration\" not in folder_name.lower())\n            and (\"metadata\" not in folder_name.lower())\n            and (\"json\" not in folder_name.lower())\n            and (\"doc\" not in folder_name.lower())\n        ):\n            all_echosounders.append(folder_name)\n\n    return all_echosounders\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_echosounders_that_exist_in_ncei","title":"<code>get_all_echosounders_that_exist_in_ncei(s3_client=None)</code>","text":"<p>Gets a list of all possible echosounders from NCEI.</p> <p>Parameters:</p> Name Type Description Default <code>s3_client</code> <code>client</code> <p>The client used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being the echosounder name. Whether these are full paths or just folder names are specified by the <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_echosounders_that_exist_in_ncei(\n    s3_client: boto3.client = None,\n) -&gt; List[str]:\n    \"\"\"Gets a list of all possible echosounders from NCEI.\n\n    Args:\n        s3_client (boto3.client, optional): The client used to perform this\n            operation. Defaults to None, but creates a client for you instead.\n\n    Returns:\n        List[str]: A list of strings, each being the echosounder name. Whether\n            these are full paths or just folder names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n\n    # Create client objects if they dont exist.\n    if s3_client is None:\n        s3_client, _, _ = create_s3_objs()\n\n    # First we get all of the prefixes for each survey to exist in NCEI.\n    all_survey_prefixes = get_all_surveys_in_ncei(\n        s3_client=s3_client, return_full_paths=True\n    )\n    all_echosounders = set()\n    for survey_prefix in tqdm(\n        all_survey_prefixes, desc=\"Getting Echosounders\"\n    ):\n        # Remove trailing `/`\n        survey_prefix = survey_prefix.strip(\"/\")\n        survey_name = survey_prefix.split(\"/\")[-1]\n        ship_name = survey_prefix.split(\"/\")[-2]\n        survey_echosounders = get_all_echosounders_in_a_survey(\n            ship_name=ship_name,\n            survey_name=survey_name,\n            s3_client=s3_client,\n            return_full_paths=False,\n        )\n        all_echosounders.update(survey_echosounders)\n\n    return list(all_echosounders)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_file_names_from_survey","title":"<code>get_all_file_names_from_survey(ship_name='', survey_name='', s3_resource=None, return_full_paths=False)</code>","text":"<p>Gets all of the file names from a particular NCEI survey.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Defaults to None. NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use the <code>get_all_ship_names_in_ncei</code> function to see all possible NCEI ship names.</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name exactly as it is in NCEI. Defaults to \"\".</p> <code>''</code> <code>s3_resource</code> <code>resource</code> <p>The resource used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being the echosounder name. Whether these are full paths or just folder names are specified by the <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_file_names_from_survey(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    s3_resource: boto3.resource = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the file names from a particular NCEI survey.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Defaults to None.\n            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n            ship names.\n        survey_name (str, optional): The survey name exactly as it is in NCEI.\n            Defaults to \"\".\n        s3_resource (boto3.resource, optional): The resource used to perform\n            this operation. Defaults to None, but creates a client for you\n            instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings, each being the echosounder name. Whether\n            these are full paths or just folder names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n\n    survey_prefix = f\"data/raw/{ship_name}/{survey_name}/\"\n    all_files = list_all_objects_in_s3_bucket_location(\n        prefix=survey_prefix,\n        s3_resource=s3_resource,\n        return_full_paths=return_full_paths,\n    )\n    return all_files\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_file_names_in_a_surveys_echosounder_folder","title":"<code>get_all_file_names_in_a_surveys_echosounder_folder(ship_name='', survey_name='', echosounder='', s3_resource=None, return_full_paths=False)</code>","text":"<p>Gets all of the file names from a particular NCEI survey's echosounder folder.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Defaults to None. NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use the <code>get_all_ship_names_in_ncei</code> function to see all possible NCEI ship names.</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name exactly as it is in NCEI. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used. Defaults to \"\".</p> <code>''</code> <code>s3_resource</code> <code>resource</code> <p>The resource used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being the file name. Whether these are full paths or just file names are specified by the <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_file_names_in_a_surveys_echosounder_folder(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    s3_resource: boto3.resource = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the file names from a particular NCEI survey's echosounder\n    folder.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Defaults to None.\n            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n            ship names.\n        survey_name (str, optional): The survey name exactly as it is in NCEI.\n            Defaults to \"\".\n        echosounder (str, optional): The echosounder used. Defaults to \"\".\n        s3_resource (boto3.resource, optional): The resource used to perform\n            this operation. Defaults to None, but creates a client for you\n            instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings, each being the file name. Whether\n            these are full paths or just file names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n\n    survey_prefix = f\"data/raw/{ship_name}/{survey_name}/{echosounder}/\"\n    all_files = list_all_objects_in_s3_bucket_location(\n        prefix=survey_prefix,\n        s3_resource=s3_resource,\n        return_full_paths=return_full_paths,\n    )\n    return all_files\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_metadata_files_in_survey","title":"<code>get_all_metadata_files_in_survey(ship_name='', survey_name='', s3_resource=None, return_full_paths=False)</code>","text":"<p>Gets all of the metadata file names from a particular NCEI survey.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Defaults to None. NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use the <code>get_all_ship_names_in_ncei</code> function to see all possible NCEI ship names.</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name exactly as it is in NCEI. Defaults to \"\".</p> <code>''</code> <code>s3_resource</code> <code>resource</code> <p>The resource used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being the metadata file name. Whether these are full paths or just folder names are specified by the <code>return_full_paths</code> parameter. Returns empty list '[]' if no metadata files are present.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_metadata_files_in_survey(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    s3_resource: boto3.resource = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the metadata file names from a particular NCEI survey.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Defaults to None.\n            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n            ship names.\n        survey_name (str, optional): The survey name exactly as it is in NCEI.\n            Defaults to \"\".\n        s3_resource (boto3.resource, optional): The resource used to perform\n            this operation. Defaults to None, but creates a client for you\n            instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings, each being the metadata file name.\n            Whether these are full paths or just folder names are specified by\n            the `return_full_paths` parameter. Returns empty list '[]' if no\n            metadata files are present.\n    \"\"\"\n\n    survey_prefix = f\"data/raw/{ship_name}/{survey_name}/metadata/\"\n    all_metadata_files = list_all_objects_in_s3_bucket_location(\n        prefix=survey_prefix,\n        s3_resource=s3_resource,\n        return_full_paths=return_full_paths,\n    )\n    return all_metadata_files\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_raw_file_names_from_survey","title":"<code>get_all_raw_file_names_from_survey(ship_name='', survey_name='', echosounder='', s3_resource=None, return_full_paths=False)</code>","text":"<p>Gets all of the file names from a particular NCEI survey.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Defaults to None. NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use the <code>get_all_ship_names_in_ncei</code> function to see all possible NCEI ship names.</p> <code>''</code> <code>survey_name</code> <code>str</code> <p>The survey name exactly as it is in NCEI. Defaults to \"\".</p> <code>''</code> <code>echosounder</code> <code>str</code> <p>The echosounder used. Defaults to \"\".</p> <code>''</code> <code>s3_resource</code> <code>resource</code> <p>The resource used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being the raw file name. Whether these are full paths or just folder names are specified by the <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_raw_file_names_from_survey(\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounder: str = \"\",\n    s3_resource: boto3.resource = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets all of the file names from a particular NCEI survey.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Defaults to None.\n            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n            ship names.\n        survey_name (str, optional): The survey name exactly as it is in NCEI.\n            Defaults to \"\".\n        echosounder (str, optional): The echosounder used. Defaults to \"\".\n        s3_resource (boto3.resource, optional): The resource used to perform\n            this operation. Defaults to None, but creates a client for you\n            instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n\n    Returns:\n        List[str]: A list of strings, each being the raw file name. Whether\n            these are full paths or just folder names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n\n    survey_prefix = f\"data/raw/{ship_name}/{survey_name}/{echosounder}/\"\n    all_files = list_all_objects_in_s3_bucket_location(\n        prefix=survey_prefix,\n        s3_resource=s3_resource,\n        return_full_paths=return_full_paths,\n    )\n    all_files = [file for file in all_files if file.endswith(\".raw\")]\n    return all_files\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_ship_names_in_ncei","title":"<code>get_all_ship_names_in_ncei(normalize=False, s3_client=None, return_full_paths=False)</code>","text":"<p>Gets all of the ship names from NCEI. This is based on all of the folders listed under the <code>data/raw/</code> prefix.</p> <p>Parameters:</p> Name Type Description Default <code>normalize</code> <code>bool</code> <p>Whether or not to normalize the ship_name attribute to how GCP stores it. Defaults to False.</p> <code>False</code> <code>s3_client</code> <code>client</code> <p>The client used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_ship_names_in_ncei(\n    normalize: bool = False,\n    s3_client: boto3.client = None,\n    return_full_paths: bool = False,\n):\n    \"\"\"Gets all of the ship names from NCEI. This is based on all of the\n    folders listed under the `data/raw/` prefix.\n\n    Args:\n        normalize (bool, optional): Whether or not to normalize the ship_name\n            attribute to how GCP stores it. Defaults to False.\n        s3_client (boto3.client, optional): The client used to perform this\n            operation. Defaults to None, but creates a client for you instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n    \"\"\"\n\n    # Create client objects if they dont exist.\n    if s3_client is None:\n        s3_client, _, _ = create_s3_objs()\n\n    # Get the initial subdirs\n    prefix = \"data/raw/\"\n    subdirs = get_subdirectories_in_s3_bucket_location(\n        prefix=prefix, s3_client=s3_client, return_full_paths=return_full_paths\n    )\n    if normalize:\n        subdirs = [normalize_ship_name(ship_name=subdir) for subdir in subdirs]\n    return subdirs\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_survey_names_from_a_ship","title":"<code>get_all_survey_names_from_a_ship(ship_name='', s3_client=None, return_full_paths=False)</code>","text":"<p>Gets a list of all of the survey names that exist under a ship name.</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship's name you want to get all surveys from. Defaults to None. NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use the <code>get_all_ship_names_in_ncei</code> function to see all possible NCEI ship names.</p> <code>''</code> <code>s3_client</code> <code>client</code> <p>The client used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <p>Returns:     List[str]: A list of strings, each being the survey name. Whether         these are full paths or just folder names are specified by the         <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_survey_names_from_a_ship(\n    ship_name: str = \"\",\n    s3_client: boto3.client = None,\n    return_full_paths: bool = False,\n) -&gt; List[str]:\n    \"\"\"Gets a list of all of the survey names that exist under a ship name.\n\n    Args:\n        ship_name (str, optional): The ship's name you want to get all surveys\n            from. Defaults to None.\n            NOTE: The ship's name MUST be spelled exactly as it is in NCEI. Use\n            the `get_all_ship_names_in_ncei` function to see all possible NCEI\n            ship names.\n        s3_client (boto3.client, optional): The client used to perform this\n            operation. Defaults to None, but creates a client for you instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n    Returns:\n        List[str]: A list of strings, each being the survey name. Whether\n            these are full paths or just folder names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n    # Create client objects if they dont exist.\n    if s3_client is None:\n        s3_client, _, _ = create_s3_objs()\n\n    # Make sure the ship name is valid\n    all_ship_names = get_all_ship_names_in_ncei(\n        normalize=False, s3_client=s3_client, return_full_paths=False\n    )\n    if ship_name not in all_ship_names:\n        close_matches = get_close_matches(\n            ship_name, all_ship_names, n=3, cutoff=0.6\n        )\n    assert ship_name in all_ship_names, (\n        f\"The ship name provided `{ship_name}` \"\n        \"needs to be spelled exactly like in NCEI.\\n\"\n        \"Use the `get_all_ship_names_in_ncei` function to see all possible \"\n        \"NCEI ship names.\\n\"\n        f\"Did you mean one of these possible ship names?\\n{close_matches}\"\n    )\n\n    ship_prefix = f\"data/raw/{ship_name}/\"\n    all_surveys = set()\n    # Get a list of all of this ship's survey names\n    all_ship_survey_names = get_subdirectories_in_s3_bucket_location(\n        prefix=ship_prefix,\n        s3_client=s3_client,\n        return_full_paths=return_full_paths,\n        bucket_name=\"noaa-wcsd-pds\",\n    )\n    all_surveys.update(all_ship_survey_names)\n    return list(all_surveys)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_all_surveys_in_ncei","title":"<code>get_all_surveys_in_ncei(s3_client=None, return_full_paths=False)</code>","text":"<p>Gets a list of all of the possible survey names from NCEI.</p> <p>Parameters:</p> Name Type Description Default <code>s3_client</code> <code>client</code> <p>The client used to perform this operation. Defaults to None, but creates a client for you instead.</p> <code>None</code> <code>return_full_paths</code> <code>bool</code> <p>Whether or not you want a full path from bucket root to the subdirectory returned. Set to false if you only want the subdirectory names listed. Defaults to False.</p> <code>False</code> <p>Returns:     List[str]: A list of strings, each being the survey name. Whether         these are full paths or just folder names are specified by the         <code>return_full_paths</code> parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_all_surveys_in_ncei(\n    s3_client: boto3.client = None, return_full_paths: bool = False\n) -&gt; List[str]:\n    \"\"\"Gets a list of all of the possible survey names from NCEI.\n\n    Args:\n        s3_client (boto3.client, optional): The client used to perform this\n            operation. Defaults to None, but creates a client for you instead.\n        return_full_paths (bool, optional): Whether or not you want a full\n            path from bucket root to the subdirectory returned. Set to false\n            if you only want the subdirectory names listed. Defaults to False.\n    Returns:\n        List[str]: A list of strings, each being the survey name. Whether\n            these are full paths or just folder names are specified by the\n            `return_full_paths` parameter.\n    \"\"\"\n\n    # Create client objects if they dont exist.\n    if s3_client is None:\n        s3_client, _, _ = create_s3_objs()\n\n    # First we get all of the prefixes for each ship.\n    all_ship_prefixes = get_all_ship_names_in_ncei(\n        normalize=False, s3_client=s3_client, return_full_paths=True\n    )\n    all_surveys = set()\n    for ship_prefix in tqdm(all_ship_prefixes, desc=\"Getting Surveys\"):\n        # Get a list of all of this ship's survey names\n        all_ship_survey_names = get_subdirectories_in_s3_bucket_location(\n            prefix=ship_prefix,\n            s3_client=s3_client,\n            return_full_paths=return_full_paths,\n            bucket_name=\"noaa-wcsd-pds\",\n        )\n        all_surveys.update(all_ship_survey_names)\n    return list(all_surveys)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_checksum_sha256_from_s3","title":"<code>get_checksum_sha256_from_s3(object_key, s3_resource)</code>","text":"<p>Gets the SHA-256 checksum of the s3 object.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_checksum_sha256_from_s3(object_key, s3_resource):\n    \"\"\"Gets the SHA-256 checksum of the s3 object.\"\"\"\n    obj = s3_resource.Object(\"noaa-wcsd-pds\", object_key)\n    checksum = obj.checksum_sha256\n    return checksum\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_closest_ncei_formatted_ship_name","title":"<code>get_closest_ncei_formatted_ship_name(ship_name='')</code>","text":"<p>Gets the closest NCEI formatted ship name to the given ship name. NOTE: Only use if the <code>data_source</code>==\"NCEI\".</p> <p>Parameters:</p> Name Type Description Default <code>ship_name</code> <code>str</code> <p>The ship name to search the closest match for. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Union[str, None]: The NCEI formatted ship name or None, if none matched.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_closest_ncei_formatted_ship_name(\n    ship_name: str = \"\",\n) -&gt; Union[str, None]:\n    \"\"\"Gets the closest NCEI formatted ship name to the given ship name.\n    NOTE: Only use if the `data_source`==\"NCEI\".\n\n    Args:\n        ship_name (str, optional): The ship name to search the closest match\n            for.\n            Defaults to \"\".\n\n    Returns:\n        Union[str, None]: The NCEI formatted ship name or None, if none\n            matched.\n    \"\"\"\n\n    all_ship_names = get_all_ship_names_in_ncei(\n        normalize=False, return_full_paths=False\n    )\n    close_matches = get_close_matches(\n        ship_name, all_ship_names, n=3, cutoff=0.85\n    )\n    if len(close_matches) &gt;= 1:\n        return close_matches[0]\n    else:\n        return None\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_echosounder_from_raw_file","title":"<code>get_echosounder_from_raw_file(file_name='', ship_name='', survey_name='', echosounders=None, s3_client=None, s3_resource=None, s3_bucket=None)</code>","text":"<p>Gets the echosounder used for a particular raw file.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_echosounder_from_raw_file(\n    file_name: str = \"\",\n    ship_name: str = \"\",\n    survey_name: str = \"\",\n    echosounders: List[str] = None,\n    s3_client: boto3.client = None,\n    s3_resource: boto3.resource = None,\n    s3_bucket: boto3.resource = None,\n):\n    \"\"\"Gets the echosounder used for a particular raw file.\"\"\"\n\n    if (s3_client is None) or (s3_resource is None) or (s3_bucket is None):\n        s3_client, s3_resource, s3_bucket = create_s3_objs()\n\n    if echosounders is None:\n        echosounders = get_all_echosounders_in_a_survey(\n            ship_name=ship_name,\n            survey_name=survey_name,\n            s3_client=s3_client,\n            return_full_paths=False,\n        )\n\n    for echosounder in echosounders:\n        raw_file_location = (\n            f\"data/raw/{ship_name}/{survey_name}/{echosounder}/{file_name}\"\n        )\n        raw_file_exists = check_if_file_exists_in_s3(\n            object_key=raw_file_location,\n            s3_resource=s3_resource,\n            s3_bucket_name=s3_bucket.name,\n        )\n        if raw_file_exists:\n            return echosounder\n\n    return ValueError(\"An echosounder could not be found for this raw file.\")\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_file_size_from_s3","title":"<code>get_file_size_from_s3(object_key, s3_resource)</code>","text":"<p>Gets the file size of an object in s3.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_file_size_from_s3(object_key, s3_resource):\n    \"\"\"Gets the file size of an object in s3.\"\"\"\n    obj = s3_resource.Object(\"noaa-wcsd-pds\", object_key)\n    file_size = obj.content_length\n    return file_size\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_folder_size_from_s3","title":"<code>get_folder_size_from_s3(folder_prefix, s3_resource)</code>","text":"<p>Gets the folder size in bytes from S3.</p> <p>Parameters:</p> Name Type Description Default <code>folder_prefix</code> <code>str</code> <p>The object key prefix of the folder in S3.</p> required <code>s3_resource</code> <code>resource</code> <p>The resource used to perform this operation. Defaults to None, but creates a client for you instead.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total size of the folder in bytes.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_folder_size_from_s3(\n    folder_prefix: str, s3_resource: boto3.resource\n) -&gt; int:\n    \"\"\"Gets the folder size in bytes from S3.\n\n    Args:\n        folder_prefix (str): The object key prefix of the folder in S3.\n        s3_resource (boto3.resource, optional): The resource used to perform\n            this operation. Defaults to None, but creates a client for you\n            instead.\n\n    Returns:\n        int: The total size of the folder in bytes.\n    \"\"\"\n    if s3_resource is None:\n        _, s3_resource, _ = create_s3_objs()\n\n    # Initialize total size\n    total_size = 0\n\n    # Get all objects' keys in the folder\n    all_files_object_keys = list_all_objects_in_s3_bucket_location(\n        prefix=folder_prefix,\n        s3_resource=s3_resource,\n        return_full_paths=True,\n    )\n\n    for file_object_key in tqdm(\n        all_files_object_keys, desc=\"Calculating Folder Size\"\n    ):\n        total_size += get_file_size_from_s3(\n            object_key=file_object_key, s3_resource=s3_resource\n        )\n\n    return total_size\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.get_random_raw_file_from_ncei","title":"<code>get_random_raw_file_from_ncei()</code>","text":"<p>Creates a test raw file for NCEI. This is used for testing purposes only. Retries automatically if an error occurs.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list object with strings denoting each parameter required for creating a raw file object. Ex. [     random_ship_name,     random_survey_name,     random_echosounder,     random_raw_file, ]</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def get_random_raw_file_from_ncei() -&gt; List[str]:\n    \"\"\"Creates a test raw file for NCEI. This is used for testing purposes\n    only. Retries automatically if an error occurs.\n\n    Returns:\n        List[str]: A list object with strings denoting each parameter required\n            for creating a raw file object.\n            Ex. [\n                random_ship_name,\n                random_survey_name,\n                random_echosounder,\n                random_raw_file,\n            ]\n    \"\"\"\n\n    try:\n        # Get all of the ship names\n        all_ship_names = get_all_ship_names_in_ncei(\n            normalize=False, return_full_paths=False\n        )\n        random_ship_name = all_ship_names[randint(0, len(all_ship_names) - 1)]\n        # Get all of the surveys for this ship\n        all_surveys_for_this_ship = get_all_survey_names_from_a_ship(\n            ship_name=random_ship_name, return_full_paths=False\n        )\n        random_survey_name = all_surveys_for_this_ship[\n            randint(0, len(all_surveys_for_this_ship) - 1)\n        ]\n        # Get all of the echosounders in this survey\n        all_echosounders_for_this_survey = get_all_echosounders_in_a_survey(\n            ship_name=random_ship_name,\n            survey_name=random_survey_name,\n            return_full_paths=False,\n        )\n        random_echosounder = all_echosounders_for_this_survey[\n            randint(0, len(all_echosounders_for_this_survey) - 1)\n        ]\n        # Get all of the raw files in this echosounder\n        all_raw_files_in_echosounder = get_all_raw_file_names_from_survey(\n            ship_name=random_ship_name,\n            survey_name=random_survey_name,\n            echosounder=random_echosounder,\n            return_full_paths=False,\n        )\n        random_raw_file = all_raw_files_in_echosounder[\n            randint(0, len(all_raw_files_in_echosounder) - 1)\n        ]\n\n        return [\n            random_ship_name,\n            random_survey_name,\n            random_echosounder,\n            random_raw_file,\n        ]\n    except Exception:\n        return get_random_raw_file_from_ncei()\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.search_ncei_file_objects_for_string","title":"<code>search_ncei_file_objects_for_string(search_param='', file_extension='.raw')</code>","text":"<p>Searches NCEI for a file type's object keys that contain a particular string. This string can be anything, such as an echosounder name, ship name, survey name, or even a partial file name. The file type can be specified by the file_extension parameter. NOTE: This function takes a long time to run, as it has to search through ALL of NCEI's objects.</p> <p>Parameters:</p> Name Type Description Default <code>search_param</code> <code>str</code> <p>The string to search for. Defaults to \"\".</p> <code>''</code> <code>file_extension</code> <code>str</code> <p>The file extension to filter results by. Defaults to \".raw\".</p> <code>'.raw'</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being an object key that contains the search parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def search_ncei_file_objects_for_string(\n    search_param: str = \"\", file_extension: str = \".raw\"\n) -&gt; List[str]:\n    \"\"\"Searches NCEI for a file type's object keys that contain a particular\n    string. This string can be anything, such as an echosounder name,\n    ship name, survey name, or even a partial file name. The file type can be\n    specified by the file_extension parameter.\n    NOTE: This function takes a long time to run, as it has to search through\n    ALL of NCEI's objects.\n\n    Args:\n        search_param (str, optional): The string to search for. Defaults to \"\".\n        file_extension (str, optional): The file extension to filter results\n            by. Defaults to \".raw\".\n\n    Returns:\n        List[str]: A list of strings, each being an object key that contains\n            the search parameter.\n    \"\"\"\n\n    s3_client, _, _ = create_s3_objs()\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    page_iterator = paginator.paginate(Bucket=\"noaa-wcsd-pds\")\n    matching_object_keys = []\n    objects = page_iterator.search(\n        f\"Contents[?contains(Key, `{search_param}`)\"\n        f\" &amp;&amp; ends_with(Key, `{file_extension}`)][]\"\n    )\n    for item in objects:\n        print(item[\"Key\"])\n        matching_object_keys.append(item[\"Key\"])\n    return matching_object_keys\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.ncei_utils.search_ncei_objects_for_string","title":"<code>search_ncei_objects_for_string(search_param='')</code>","text":"<p>Searches NCEI for object keys that contain a particular string. This string can be anything, such as an echosounder name, ship name, survey name, or even a partial file name. NOTE: This function takes a long time to run, as it has to search through ALL of NCEI's objects. NOTE: Use a folder name as the search_param to get all object keys that contain that folder name. (e.g. '/EK80/')</p> <p>Parameters:</p> Name Type Description Default <code>search_param</code> <code>str</code> <p>The string to search for. Defaults to \"\".</p> <code>''</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of strings, each being an object key that contains the search parameter.</p> Source code in <code>src\\aalibrary\\utils\\ncei_utils.py</code> <pre><code>def search_ncei_objects_for_string(search_param: str = \"\") -&gt; List[str]:\n    \"\"\"Searches NCEI for object keys that contain a particular string. This\n    string can be anything, such as an echosounder name, ship name,\n    survey name, or even a partial file name.\n    NOTE: This function takes a long time to run, as it has to search through\n    ALL of NCEI's objects.\n    NOTE: Use a folder name as the search_param to get all object keys that\n    contain that folder name. (e.g. '/EK80/')\n\n    Args:\n        search_param (str, optional): The string to search for. Defaults to \"\".\n\n    Returns:\n        List[str]: A list of strings, each being an object key that contains\n            the search parameter.\n    \"\"\"\n\n    s3_client, _, _ = create_s3_objs()\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    page_iterator = paginator.paginate(Bucket=\"noaa-wcsd-pds\")\n    matching_object_keys = []\n    # Vpcs[?contains(`[\"vpc-blabla1\", \"vpc-blabla2\"]`, VpcId)].OtherKey\n    # objects = page_iterator.search(f\"\n    # Contents[?contains(Key, `{search_param}`) &amp;&amp; ends_with(Key, `.raw`)][]\")\n    objects = page_iterator.search(\n        f\"Contents[?contains(Key, `{search_param}`)][]\"\n    )\n    # objects = page_iterator.search(\"Contents[?ends_with(Key, `.csv`)][]\")\n    for item in objects:\n        print(item[\"Key\"])\n        matching_object_keys.append(item[\"Key\"])\n    return matching_object_keys\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker","title":"<code>sonar_checker</code>","text":"<p>Modules:</p> Name Description <code>ek_date_conversion</code> <p>Code originally developed for pyEcholab</p> <code>ek_raw_io</code> <p>Code originally developed for pyEcholab</p> <code>ek_raw_parsers</code> <p>Code originally developed for pyEcholab</p> <code>log</code> <code>misc</code> <code>sonar_checker</code>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion","title":"<code>ek_date_conversion</code>","text":"<p>Code originally developed for pyEcholab (https://github.com/CI-CMG/pyEcholab) by Rick Towler rick.towler@noaa.gov at NOAA AFSC.</p> <p>Contains functions to convert date information.</p> <p>TODO: merge necessary function into ek60.py or group everything into a class TODO: fix docstring</p> <p>Functions:</p> Name Description <code>nt_to_unix</code> <p>:param nt_timestamp_tuple: Tuple of two longs representing the NT date</p> <code>unix_to_nt</code> <p>Given a date, return the 2-element tuple used for timekeeping with SIMRAD echosounders</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.datetime_to_unix","title":"<code>datetime_to_unix(datetime_obj)</code>","text":"<p>:param datetime_obj: datetime object to convert :type datetime_obj: :class:<code>datetime.datetime</code></p> <p>:param tz: Timezone to use for converted time -- if None, uses timezone             information contained within datetime_obj :type tz: :class:datetime.tzinfo</p> <p>from pytz import utc from datetime import datetime epoch = datetime(1970, 1, 1, tzinfo=utc) assert datetime_to_unix(epoch) == 0</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_date_conversion.py</code> <pre><code>def datetime_to_unix(datetime_obj):\n    \"\"\"\n    :param datetime_obj: datetime object to convert\n    :type datetime_obj: :class:`datetime.datetime`\n\n    :param tz: Timezone to use for converted time -- if None, uses timezone\n                information contained within datetime_obj\n    :type tz: :class:datetime.tzinfo\n\n    &gt;&gt;&gt; from pytz import utc\n    &gt;&gt;&gt; from datetime import datetime\n    &gt;&gt;&gt; epoch = datetime(1970, 1, 1, tzinfo=utc)\n    &gt;&gt;&gt; assert datetime_to_unix(epoch) == 0\n    \"\"\"\n\n    timestamp = (datetime_obj - UTC_UNIX_EPOCH).total_seconds()\n\n    return timestamp\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.nt_to_unix","title":"<code>nt_to_unix(nt_timestamp_tuple, return_datetime=True)</code>","text":"<p>:param nt_timestamp_tuple: Tuple of two longs representing the NT date :type nt_timestamp_tuple: (long, long)</p> <p>:param return_datetime:  Return a datetime object instead of float :type return_datetime: bool</p> <p>Returns a datetime.datetime object w/ UTC timezone calculated from the nt time tuple</p> <p>lowDateTime, highDateTime = nt_timestamp_tuple</p> <p>The timestamp is a 64bit count of 100ns intervals since the NT epoch broken into two 32bit longs, least significant first:</p> <p>dt = nt_to_unix((19496896L, 30196149L)) match_dt = datetime.datetime(2011, 12, 23, 20, 54, 3, 964000, pytz_utc) assert abs(dt - match_dt) &lt;= dt.resolution</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_date_conversion.py</code> <pre><code>def nt_to_unix(nt_timestamp_tuple, return_datetime=True):\n    \"\"\"\n    :param nt_timestamp_tuple: Tuple of two longs representing the NT date\n    :type nt_timestamp_tuple: (long, long)\n\n    :param return_datetime:  Return a datetime object instead of float\n    :type return_datetime: bool\n\n\n    Returns a datetime.datetime object w/ UTC timezone\n    calculated from the nt time tuple\n\n    lowDateTime, highDateTime = nt_timestamp_tuple\n\n    The timestamp is a 64bit count of 100ns intervals since the NT epoch\n    broken into two 32bit longs, least significant first:\n\n    &gt;&gt;&gt; dt = nt_to_unix((19496896L, 30196149L))\n    &gt;&gt;&gt; match_dt = datetime.datetime(2011, 12, 23, 20, 54, 3, 964000, pytz_utc)\n    &gt;&gt;&gt; assert abs(dt - match_dt) &lt;= dt.resolution\n    \"\"\"\n\n    lowDateTime, highDateTime = nt_timestamp_tuple\n    sec_past_nt_epoch = ((highDateTime &lt;&lt; 32) + lowDateTime) * 1.0e-7\n\n    if return_datetime:\n        return UTC_NT_EPOCH + datetime.timedelta(seconds=sec_past_nt_epoch)\n\n    else:\n        sec_past_unix_epoch = sec_past_nt_epoch - EPOCH_DELTA_SECONDS\n        return sec_past_unix_epoch\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.unix_to_datetime","title":"<code>unix_to_datetime(unix_timestamp)</code>","text":"<p>:param unix_timestamp: Number of seconds since unix epoch (1/1/1970) :type unix_timestamp: float</p> <p>:param tz: timezone to use for conversion (default None = UTC) :type tz: None or tzinfo object (see datetime docs)</p> <p>:returns: datetime object :raises: ValueError if unix_timestamp is not of type float or datetime</p> <p>Returns a datetime object from a unix timestamp.  Simple wrapper for :func:<code>datetime.datetime.fromtimestamp</code></p> <p>from pytz import utc from datetime import datetime epoch = unix_to_datetime(0.0, tz=utc) assert epoch == datetime(1970, 1, 1, tzinfo=utc)</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_date_conversion.py</code> <pre><code>def unix_to_datetime(unix_timestamp):\n    \"\"\"\n    :param unix_timestamp: Number of seconds since unix epoch (1/1/1970)\n    :type unix_timestamp: float\n\n    :param tz: timezone to use for conversion (default None = UTC)\n    :type tz: None or tzinfo object (see datetime docs)\n\n    :returns: datetime object\n    :raises: ValueError if unix_timestamp is not of type float or datetime\n\n    Returns a datetime object from a unix timestamp.  Simple wrapper for\n    :func:`datetime.datetime.fromtimestamp`\n\n    &gt;&gt;&gt; from pytz import utc\n    &gt;&gt;&gt; from datetime import datetime\n    &gt;&gt;&gt; epoch = unix_to_datetime(0.0, tz=utc)\n    &gt;&gt;&gt; assert epoch == datetime(1970, 1, 1, tzinfo=utc)\n    \"\"\"\n\n    if isinstance(unix_timestamp, datetime.datetime):\n        if unix_timestamp.tzinfo is None:\n            unix_datetime = pytz_utc.localize(unix_timestamp)\n\n        elif unix_timestamp.tzinfo == pytz_utc:\n            unix_datetime = unix_timestamp\n\n        else:\n            unix_datetime = pytz_utc.normalize(unix_timestamp.astimezone(pytz_utc))\n\n    elif isinstance(unix_timestamp, float):\n        unix_datetime = pytz_utc.localize(datetime.datetime.fromtimestamp(unix_timestamp))\n\n    else:\n        errstr = \"Looking for a timestamp of type datetime.datetime or # of sec past unix epoch.\\n\"\n        errstr += \"Supplied timestamp '%s' of type %s.\" % (\n            str(unix_timestamp),\n            type(unix_timestamp),\n        )\n        raise ValueError(errstr)\n\n    return unix_datetime\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.unix_to_nt","title":"<code>unix_to_nt(unix_timestamp)</code>","text":"<p>Given a date, return the 2-element tuple used for timekeeping with SIMRAD echosounders</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.unix_to_nt--simple-conversion","title":"Simple conversion","text":"<p>dt = datetime.datetime(2011, 12, 23, 20, 54, 3, 964000, pytz_utc) assert (19496896L, 30196149L) == unix_to_nt(dt)</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.unix_to_nt--converting-back-and-forth-between-the-two-standards","title":"Converting back and forth between the two standards:","text":"<p>orig_dt = datetime.datetime.now(tz=pytz_utc) nt_tuple = unix_to_nt(orig_dt)</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.unix_to_nt--converting-back-may-not-yield-the-exact-original-date","title":"converting back may not yield the exact original date,","text":""},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_date_conversion.unix_to_nt--but-will-be-within-the-datetimes-precision","title":"but will be within the datetime's precision","text":"<p>back_to_dt = nt_to_unix(nt_tuple) d_mu_seconds = abs(orig_dt - back_to_dt).microseconds mu_sec_resolution = orig_dt.resolution.microseconds assert d_mu_seconds &lt;= mu_sec_resolution</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_date_conversion.py</code> <pre><code>def unix_to_nt(unix_timestamp):\n    \"\"\"\n    Given a date, return the 2-element tuple used for timekeeping with SIMRAD echosounders\n\n\n    #Simple conversion\n    &gt;&gt;&gt; dt = datetime.datetime(2011, 12, 23, 20, 54, 3, 964000, pytz_utc)\n    &gt;&gt;&gt; assert (19496896L, 30196149L) == unix_to_nt(dt)\n\n    #Converting back and forth between the two standards:\n    &gt;&gt;&gt; orig_dt = datetime.datetime.now(tz=pytz_utc)\n    &gt;&gt;&gt; nt_tuple = unix_to_nt(orig_dt)\n\n    #converting back may not yield the exact original date,\n    #but will be within the datetime's precision\n    &gt;&gt;&gt; back_to_dt = nt_to_unix(nt_tuple)\n    &gt;&gt;&gt; d_mu_seconds = abs(orig_dt - back_to_dt).microseconds\n    &gt;&gt;&gt; mu_sec_resolution = orig_dt.resolution.microseconds\n    &gt;&gt;&gt; assert d_mu_seconds &lt;= mu_sec_resolution\n    \"\"\"\n\n    if isinstance(unix_timestamp, datetime.datetime):\n        if unix_timestamp.tzinfo is None:\n            unix_datetime = pytz_utc.localize(unix_timestamp)\n\n        elif unix_timestamp.tzinfo == pytz_utc:\n            unix_datetime = unix_timestamp\n\n        else:\n            unix_datetime = pytz_utc.normalize(unix_timestamp.astimezone(pytz_utc))\n\n    else:\n        unix_datetime = unix_to_datetime(unix_timestamp)\n\n    sec_past_nt_epoch = (unix_datetime - UTC_NT_EPOCH).total_seconds()\n\n    onehundred_ns_intervals = int(sec_past_nt_epoch * 1e7)\n    lowDateTime = onehundred_ns_intervals &amp; 0xFFFFFFFF\n    highDateTime = onehundred_ns_intervals &gt;&gt; 32\n\n    return lowDateTime, highDateTime\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io","title":"<code>ek_raw_io</code>","text":"<p>Code originally developed for pyEcholab (https://github.com/CI-CMG/pyEcholab) by Rick Towler rick.towler@noaa.gov at NOAA AFSC.</p> <p>Contains low-level functions called by ./ek_raw_parsers.py</p> <p>Classes:</p> Name Description <code>RawSimradFile</code> <p>A low-level extension of the built in python file object allowing the reading/writing</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile","title":"<code>RawSimradFile</code>","text":"<p>               Bases: <code>BufferedReader</code></p> <p>A low-level extension of the built in python file object allowing the reading/writing of SIMRAD RAW files on datagram by datagram basis (instead of at the byte level.)</p> <p>Calls to the read method return parse datagrams as dicts.</p> <p>Methods:</p> Name Description <code>__next__</code> <p>Returns the next datagram (synonymous with self.read(1))</p> <code>iter_dgrams</code> <p>Iterates through the file, repeatedly calling self.next() until</p> <code>peek</code> <p>Returns the header of the next datagram in the file.  The file position is</p> <code>prev</code> <p>Returns the previous datagram 'behind' the current file pointer position</p> <code>read</code> <p>:param k: Number of datagrams to read</p> <code>readall</code> <p>Reads the entire file from the beginning and returns a list of datagrams.</p> <code>readline</code> <p>aliased to self.next()</p> <code>readlines</code> <p>aliased to self.read(-1)</p> <code>seek</code> <p>Performs the familiar 'seek' operation using datagram offsets</p> <code>skip</code> <p>Skips forward to the next datagram without reading the contents of the current one</p> <code>skip_back</code> <p>Skips backwards to the previous datagram without reading it's contents</p> <code>tell</code> <p>Returns the current file pointer offset by datagram number</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>class RawSimradFile(BufferedReader):\n    \"\"\"\n    A low-level extension of the built in python file object allowing the reading/writing\n    of SIMRAD RAW files on datagram by datagram basis (instead of at the byte level.)\n\n    Calls to the read method return parse datagrams as dicts.\n    \"\"\"\n\n    #: Dict object with datagram header/python class key/value pairs\n    DGRAM_TYPE_KEY = {\n        \"RAW\": parsers.SimradRawParser(),\n        \"CON\": parsers.SimradConfigParser(),\n        \"TAG\": parsers.SimradAnnotationParser(),\n        \"NME\": parsers.SimradNMEAParser(),\n        \"BOT\": parsers.SimradBottomParser(),\n        \"DEP\": parsers.SimradDepthParser(),\n        \"XML\": parsers.SimradXMLParser(),\n        \"IDX\": parsers.SimradIDXParser(),\n        \"FIL\": parsers.SimradFILParser(),\n        \"MRU\": parsers.SimradMRUParser(),\n    }\n\n    def __init__(\n        self,\n        name,\n        mode=\"rb\",\n        closefd=True,\n        return_raw=False,\n        buffer_size=1024 * 1024,\n        storage_options={},\n    ):\n        #  9-28-18 RHT: Changed RawSimradFile to implement BufferedReader instead of\n        #  io.FileIO to increase performance.\n\n        #  create a raw file object for the buffered reader\n        fmap = fsspec.get_mapper(name, **storage_options)\n        if isinstance(fmap.fs, LocalFileSystem):\n            fio = FileIO(name, mode=mode, closefd=closefd)\n        else:\n            fio = fmap.fs.open(fmap.root)\n\n        #  initialize the superclass\n        super().__init__(fio, buffer_size=buffer_size)\n        self._current_dgram_offset = 0\n        self._total_dgram_count = None\n        self._return_raw = return_raw\n\n    def _seek_bytes(self, bytes_, whence=0):\n        \"\"\"\n        :param bytes_: byte offset\n        :type bytes_: int\n\n        :param whence:\n\n        Seeks a file by bytes instead of datagrams.\n        \"\"\"\n\n        super().seek(bytes_, whence)\n\n    def _tell_bytes(self):\n        \"\"\"\n        Returns the file pointer position in bytes.\n        \"\"\"\n\n        return super().tell()\n\n    def _read_dgram_size(self):\n        \"\"\"\n        Attempts to read the size of the next datagram in the file.\n        \"\"\"\n\n        buf = self._read_bytes(4)\n        if len(buf) != 4:\n            self._seek_bytes(-len(buf), SEEK_CUR)\n            raise DatagramReadError(\n                \"Short read while getting dgram size\",\n                (4, len(buf)),\n                file_pos=(self._tell_bytes(), self.tell()),\n            )\n        else:\n            return struct.unpack(\"=l\", buf)[0]  # This return value is an int object.\n\n    def _bytes_remaining(self):\n        old_pos = self._tell_bytes()\n        self._seek_bytes(0, SEEK_END)\n        end_pos = self._tell_bytes()\n        offset = end_pos - old_pos\n        self._seek_bytes(old_pos, SEEK_SET)\n\n        return offset\n\n    def _read_timestamp(self):\n        \"\"\"\n        Attempts to read the datagram timestamp.\n        \"\"\"\n\n        buf = self._read_bytes(8)\n        if len(buf) != 8:\n            self._seek_bytes(-len(buf), SEEK_CUR)\n            raise DatagramReadError(\n                \"Short read while getting timestamp\",\n                (8, len(buf)),\n                file_pos=(self._tell_bytes(), self.tell()),\n            )\n\n        else:\n            lowDateField, highDateField = struct.unpack(\"=2L\", buf)\n            #  11/26/19 - RHT - modified to return the raw bytes\n            return lowDateField, highDateField, buf\n\n    def _read_dgram_header(self):\n        \"\"\"\n        :returns: dgram_size, dgram_type, (low_date, high_date)\n\n        Attempts to read the datagram header consisting of:\n\n            long        dgram_size\n            char[4]     type\n            long        lowDateField\n            long        highDateField\n        \"\"\"\n\n        try:\n            dgram_size = self._read_dgram_size()\n        except Exception:\n            if self.at_eof():\n                raise SimradEOF()\n            else:\n                raise\n\n        #  get the datagram type\n        buf = self._read_bytes(4)\n\n        if len(buf) != 4:\n            if self.at_eof():\n                raise SimradEOF()\n            else:\n                self._seek_bytes(-len(buf), SEEK_CUR)\n                raise DatagramReadError(\n                    \"Short read while getting dgram type\",\n                    (4, len(buf)),\n                    file_pos=(self._tell_bytes(), self.tell()),\n                )\n        else:\n            dgram_type = buf\n        dgram_type = dgram_type.decode(\"latin_1\")\n\n        #  11/26/19 - RHT\n        #  As part of the rewrite of read to remove the reverse seeking,\n        #  store the raw header bytes so we can prepend them to the raw\n        #  data bytes and pass it all to the parser.\n        raw_bytes = buf\n\n        #  read the timestamp - this method was also modified to return\n        #  the raw bytes\n        lowDateField, highDateField, buf = self._read_timestamp()\n\n        #  add the timestamp bytes to the raw_bytes string\n        raw_bytes += buf\n\n        return dict(\n            size=dgram_size,\n            type=dgram_type,\n            low_date=lowDateField,\n            high_date=highDateField,\n            raw_bytes=raw_bytes,\n        )\n\n    def _read_bytes(self, k):\n        \"\"\"\n        Reads raw bytes from the file\n        \"\"\"\n\n        return super().read(k)\n\n    def _read_next_dgram(self):\n        \"\"\"\n        Attempts to read the next datagram from the file.\n\n        Returns the datagram as a raw string\n        \"\"\"\n\n        #  11/26/19 - RHT - Modified this method so it doesn't \"peek\"\n        #  at the next datagram before reading which was inefficient.\n        #  To minimize changes to the code, methods to read the header\n        #  and timestamp were modified to return the raw bytes which\n        #  allows us to pass them onto the parser without having to\n        #  rewind and read again as was previously done.\n\n        #  store our current location in the file\n        old_file_pos = self._tell_bytes()\n\n        #  try to read the header of the next datagram\n        try:\n            header = self._read_dgram_header()\n        except DatagramReadError as e:\n            e.message = \"Short read while getting raw file datagram header\"\n            raise e\n\n        #  check for invalid time data\n        if (header[\"low_date\"], header[\"high_date\"]) == (0, 0):\n            logger.warning(\n                \"Skipping %s datagram w/ timestamp of (0, 0) at %sL:%d\",\n                header[\"type\"],\n                str(self._tell_bytes()),\n                self.tell(),\n            )\n            self.skip()\n            return self._read_next_dgram()\n\n        #  basic sanity check on size\n        if header[\"size\"] &lt; 16:\n            #  size can't be smaller than the header size\n            logger.warning(\n                \"Invalid datagram header: size: %d, type: %s, nt_date: %s.  dgram_size &lt; 16\",\n                header[\"size\"],\n                header[\"type\"],\n                str((header[\"low_date\"], header[\"high_date\"])),\n            )\n\n            #  see if we can find the next datagram\n            self._find_next_datagram()\n\n            #  and then return that\n            return self._read_next_dgram()\n\n        #  get the raw bytes from the header\n        raw_dgram = header[\"raw_bytes\"]\n\n        #  and append the rest of the datagram - we subtract 12\n        #  since we have already read 12 bytes: 4 for type and\n        #  8 for time.\n        raw_dgram += self._read_bytes(header[\"size\"] - 12)\n\n        #  determine the size of the payload in bytes\n        bytes_read = len(raw_dgram)\n\n        #  and make sure it checks out\n        if bytes_read &lt; header[\"size\"]:\n            logger.warning(\n                \"Datagram %d (@%d) shorter than expected length:  %d &lt; %d\",\n                self.tell(),\n                old_file_pos,\n                bytes_read,\n                header[\"size\"],\n            )\n            self._find_next_datagram()\n            return self._read_next_dgram()\n\n        #  now read the trailing size value\n        try:\n            dgram_size_check = self._read_dgram_size()\n        except DatagramReadError as e:\n            self._seek_bytes(old_file_pos, SEEK_SET)\n            e.message = \"Short read while getting trailing raw file datagram size for check\"\n            raise e\n\n        #  make sure they match\n        if header[\"size\"] != dgram_size_check:\n            # self._seek_bytes(old_file_pos, SEEK_SET)\n            logger.warning(\n                \"Datagram failed size check:  %d != %d @ (%d, %d)\",\n                header[\"size\"],\n                dgram_size_check,\n                self._tell_bytes(),\n                self.tell(),\n            )\n            logger.warning(\"Skipping to next datagram...\")\n            self._find_next_datagram()\n\n            return self._read_next_dgram()\n\n        #  add the header (16 bytes) and repeated size (4 bytes) to the payload\n        #  bytes to get the total bytes read for this datagram.\n        bytes_read = bytes_read + 20\n\n        if self._return_raw:\n            self._current_dgram_offset += 1\n            return raw_dgram\n        else:\n            nice_dgram = self._convert_raw_datagram(raw_dgram, bytes_read)\n            self._current_dgram_offset += 1\n            return nice_dgram\n\n    def _convert_raw_datagram(self, raw_datagram_string, bytes_read):\n        \"\"\"\n        :param raw_datagram_string: bytestring containing datagram (first 4\n            bytes indicate datagram type, such as 'RAW0')\n        :type raw_datagram_string: str\n\n        :param bytes_read: integer specifying the datagram size, including header\n            in bytes,\n        :type bytes_read: int\n\n        Returns a formatted datagram object using the data in raw_datagram_string\n        \"\"\"\n\n        #  11/26/19 - RHT - Modified this method to pass through the number of\n        #  bytes read so we can bubble that up to the user.\n\n        dgram_type = raw_datagram_string[:3].decode()\n        try:\n            parser = self.DGRAM_TYPE_KEY[dgram_type]\n        except KeyError:\n            # raise KeyError('Unknown datagram type %s,\n            # valid types: %s' % (str(dgram_type),\n            # str(self.DGRAM_TYPE_KEY.keys())))\n            return raw_datagram_string\n\n        nice_dgram = parser.from_string(raw_datagram_string, bytes_read)\n        return nice_dgram\n\n    def _set_total_dgram_count(self):\n        \"\"\"\n        Skips quickly through the file counting datagrams and stores the\n        resulting number in self._total_dgram_count\n\n        :raises: ValueError if self._total_dgram_count is not None (it has been set before)\n        \"\"\"\n        if self._total_dgram_count is not None:\n            raise ValueError(\n                \"self._total_dgram_count has already been set.  Call .reset() first if you really want to recount\"  # noqa\n            )\n\n        # Save current position for later\n        old_file_pos = self._tell_bytes()\n        old_dgram_offset = self.tell()\n\n        self._current_dgram_offset = 0\n        self._seek_bytes(0, SEEK_SET)\n\n        while True:\n            try:\n                self.skip()\n            except (DatagramReadError, SimradEOF):\n                self._total_dgram_count = self.tell()\n                break\n\n        # Return to where we started\n        self._seek_bytes(old_file_pos, SEEK_SET)\n        self._current_dgram_offset = old_dgram_offset\n\n    def at_eof(self):\n        old_pos = self._tell_bytes()\n        self._seek_bytes(0, SEEK_END)\n        eof_pos = self._tell_bytes()\n\n        # Check to see if we're at the end of file and raise EOF\n        if old_pos == eof_pos:\n            return True\n\n        # Othereise, go back to where we were and re-raise the original\n        # exception\n        else:\n            offset = old_pos - eof_pos\n            self._seek_bytes(offset, SEEK_END)\n            return False\n\n    def read(self, k):\n        \"\"\"\n        :param k: Number of datagrams to read\n        :type k: int\n\n        Reads the next k datagrams.  A list of datagrams is returned if k &gt; 1.  The entire\n        file is read from the CURRENT POSITION if k &lt; 0. (does not necessarily read from beginning\n        of file if previous datagrams were read)\n        \"\"\"\n\n        if k == 1:\n            try:\n                return self._read_next_dgram()\n            except Exception:\n                if self.at_eof():\n                    raise SimradEOF()\n                else:\n                    raise\n\n        elif k &gt; 0:\n            dgram_list = []\n\n            for m in range(k):\n                try:\n                    dgram = self._read_next_dgram()\n                    dgram_list.append(dgram)\n\n                except Exception:\n                    break\n\n            return dgram_list\n\n        elif k &lt; 0:\n            return self.readall()\n\n    def readall(self):\n        \"\"\"\n        Reads the entire file from the beginning and returns a list of datagrams.\n        \"\"\"\n\n        self.seek(0, SEEK_SET)\n        dgram_list = []\n\n        for raw_dgram in self.iter_dgrams():\n            dgram_list.append(raw_dgram)\n\n        return dgram_list\n\n    def _find_next_datagram(self):\n        old_file_pos = self._tell_bytes()\n        logger.warning(\"Attempting to find next valid datagram...\")\n\n        try:\n            while self.peek()[\"type\"][:3] not in list(self.DGRAM_TYPE_KEY.keys()):\n                self._seek_bytes(1, 1)\n        except DatagramReadError:\n            logger.warning(\"No next datagram found. Ending reading of file.\")\n            raise SimradEOF()\n        else:\n            logger.warning(\"Found next datagram:  %s\", self.peek())\n            logger.warning(\"Skipped ahead %d bytes\", self._tell_bytes() - old_file_pos)\n\n    def tell(self):\n        \"\"\"\n        Returns the current file pointer offset by datagram number\n        \"\"\"\n        return self._current_dgram_offset\n\n    def peek(self):\n        \"\"\"\n        Returns the header of the next datagram in the file.  The file position is\n        reset back to the original location afterwards.\n\n        :returns: [dgram_size, dgram_type, (low_date, high_date)]\n        \"\"\"\n\n        dgram_header = self._read_dgram_header()\n        if dgram_header[\"type\"].startswith(\"RAW0\"):\n            dgram_header[\"channel\"] = struct.unpack(\"h\", self._read_bytes(2))[0]\n            self._seek_bytes(-18, SEEK_CUR)\n        elif dgram_header[\"type\"].startswith(\"RAW3\"):\n            chan_id = struct.unpack(\"128s\", self._read_bytes(128))\n            dgram_header[\"channel_id\"] = chan_id.strip(\"\\x00\")\n            self._seek_bytes(-(16 + 128), SEEK_CUR)\n        else:\n            self._seek_bytes(-16, SEEK_CUR)\n\n        return dgram_header\n\n    def __next__(self):\n        \"\"\"\n        Returns the next datagram (synonymous with self.read(1))\n        \"\"\"\n\n        return self.read(1)\n\n    def prev(self):\n        \"\"\"\n        Returns the previous datagram 'behind' the current file pointer position\n        \"\"\"\n\n        self.skip_back()\n        raw_dgram = self.read(1)\n        self.skip_back()\n        return raw_dgram\n\n    def skip(self):\n        \"\"\"\n        Skips forward to the next datagram without reading the contents of the current one\n        \"\"\"\n\n        # dgram_size, dgram_type, (low_date, high_date) = self.peek()[:3]\n\n        header = self.peek()\n\n        if header[\"size\"] &lt; 16:\n            logger.warning(\n                \"Invalid datagram header: size: %d, type: %s, nt_date: %s.  dgram_size &lt; 16\",\n                header[\"size\"],\n                header[\"type\"],\n                str((header[\"low_date\"], header[\"high_date\"])),\n            )\n\n            self._find_next_datagram()\n\n        else:\n            self._seek_bytes(header[\"size\"] + 4, SEEK_CUR)\n            dgram_size_check = self._read_dgram_size()\n\n            if header[\"size\"] != dgram_size_check:\n                logger.warning(\n                    \"Datagram failed size check:  %d != %d @ (%d, %d)\",\n                    header[\"size\"],\n                    dgram_size_check,\n                    self._tell_bytes(),\n                    self.tell(),\n                )\n                logger.warning(\"Skipping to next datagram... (in skip)\")\n\n                self._find_next_datagram()\n\n        self._current_dgram_offset += 1\n\n    def skip_back(self):\n        \"\"\"\n        Skips backwards to the previous datagram without reading it's contents\n        \"\"\"\n\n        old_file_pos = self._tell_bytes()\n\n        try:\n            self._seek_bytes(-4, SEEK_CUR)\n        except IOError:\n            raise\n\n        dgram_size_check = self._read_dgram_size()\n\n        # Seek to the beginning of the datagram and read as normal\n        try:\n            self._seek_bytes(-(8 + dgram_size_check), SEEK_CUR)\n        except IOError:\n            raise DatagramSizeError\n\n        try:\n            dgram_size = self._read_dgram_size()\n\n        except DatagramSizeError:\n            logger.info(\"Error reading the datagram\")\n            self._seek_bytes(old_file_pos, SEEK_SET)\n            raise\n\n        if dgram_size_check != dgram_size:\n            self._seek_bytes(old_file_pos, SEEK_SET)\n            raise DatagramSizeError\n        else:\n            self._seek_bytes(-4, SEEK_CUR)\n\n        self._current_dgram_offset -= 1\n\n    def iter_dgrams(self):\n        \"\"\"\n        Iterates through the file, repeatedly calling self.next() until\n        the end of file is reached\n        \"\"\"\n\n        while True:\n            # new_dgram = self.next()\n            # yield new_dgram\n\n            try:\n                new_dgram = next(self)\n            except Exception:\n                logger.debug(\"Caught EOF?\")\n                raise StopIteration\n\n            yield new_dgram\n\n    # Unsupported members\n    def readline(self):\n        \"\"\"\n        aliased to self.next()\n        \"\"\"\n        return next(self)\n\n    def readlines(self):\n        \"\"\"\n        aliased to self.read(-1)\n        \"\"\"\n        return self.read(-1)\n\n    def seek(self, offset, whence):\n        \"\"\"\n        Performs the familiar 'seek' operation using datagram offsets\n        instead of raw bytes.\n        \"\"\"\n\n        if whence == SEEK_SET:\n            if offset &lt; 0:\n                raise ValueError(\"Cannot seek backwards from beginning of file\")\n            else:\n                self._seek_bytes(0, SEEK_SET)\n                self._current_dgram_offset = 0\n        elif whence == SEEK_END:\n            if offset &gt; 0:\n                raise ValueError(\"Use negative offsets when seeking backward from end of file\")\n\n            # Do we need to generate the total number of datagrams w/in the file?\n            try:\n                self._set_total_dgram_count()\n                # Throws a value error if _total_dgram_count has already been set.  We can ignore it\n            except ValueError:\n                pass\n\n            self._seek_bytes(0, SEEK_END)\n            self._current_dgram_offset = self._total_dgram_count\n\n        elif whence == SEEK_CUR:\n            pass\n        else:\n            raise ValueError(\n                \"Illegal value for 'whence' (%s), use 0 (beginning), 1 (current), or 2 (end)\"\n                % (str(whence))\n            )\n\n        if offset &gt; 0:\n            for k in range(offset):\n                self.skip()\n        elif offset &lt; 0:\n            for k in range(-offset):\n                self.skip_back()\n\n    def reset(self):\n        self._current_dgram_offset = 0\n        self._total_dgram_count = None\n        self._seek_bytes(0, SEEK_SET)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.__next__","title":"<code>__next__()</code>","text":"<p>Returns the next datagram (synonymous with self.read(1))</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def __next__(self):\n    \"\"\"\n    Returns the next datagram (synonymous with self.read(1))\n    \"\"\"\n\n    return self.read(1)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.iter_dgrams","title":"<code>iter_dgrams()</code>","text":"<p>Iterates through the file, repeatedly calling self.next() until the end of file is reached</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def iter_dgrams(self):\n    \"\"\"\n    Iterates through the file, repeatedly calling self.next() until\n    the end of file is reached\n    \"\"\"\n\n    while True:\n        # new_dgram = self.next()\n        # yield new_dgram\n\n        try:\n            new_dgram = next(self)\n        except Exception:\n            logger.debug(\"Caught EOF?\")\n            raise StopIteration\n\n        yield new_dgram\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.peek","title":"<code>peek()</code>","text":"<p>Returns the header of the next datagram in the file.  The file position is reset back to the original location afterwards.</p> <p>:returns: [dgram_size, dgram_type, (low_date, high_date)]</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def peek(self):\n    \"\"\"\n    Returns the header of the next datagram in the file.  The file position is\n    reset back to the original location afterwards.\n\n    :returns: [dgram_size, dgram_type, (low_date, high_date)]\n    \"\"\"\n\n    dgram_header = self._read_dgram_header()\n    if dgram_header[\"type\"].startswith(\"RAW0\"):\n        dgram_header[\"channel\"] = struct.unpack(\"h\", self._read_bytes(2))[0]\n        self._seek_bytes(-18, SEEK_CUR)\n    elif dgram_header[\"type\"].startswith(\"RAW3\"):\n        chan_id = struct.unpack(\"128s\", self._read_bytes(128))\n        dgram_header[\"channel_id\"] = chan_id.strip(\"\\x00\")\n        self._seek_bytes(-(16 + 128), SEEK_CUR)\n    else:\n        self._seek_bytes(-16, SEEK_CUR)\n\n    return dgram_header\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.prev","title":"<code>prev()</code>","text":"<p>Returns the previous datagram 'behind' the current file pointer position</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def prev(self):\n    \"\"\"\n    Returns the previous datagram 'behind' the current file pointer position\n    \"\"\"\n\n    self.skip_back()\n    raw_dgram = self.read(1)\n    self.skip_back()\n    return raw_dgram\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.read","title":"<code>read(k)</code>","text":"<p>:param k: Number of datagrams to read :type k: int</p> <p>Reads the next k datagrams.  A list of datagrams is returned if k &gt; 1.  The entire file is read from the CURRENT POSITION if k &lt; 0. (does not necessarily read from beginning of file if previous datagrams were read)</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def read(self, k):\n    \"\"\"\n    :param k: Number of datagrams to read\n    :type k: int\n\n    Reads the next k datagrams.  A list of datagrams is returned if k &gt; 1.  The entire\n    file is read from the CURRENT POSITION if k &lt; 0. (does not necessarily read from beginning\n    of file if previous datagrams were read)\n    \"\"\"\n\n    if k == 1:\n        try:\n            return self._read_next_dgram()\n        except Exception:\n            if self.at_eof():\n                raise SimradEOF()\n            else:\n                raise\n\n    elif k &gt; 0:\n        dgram_list = []\n\n        for m in range(k):\n            try:\n                dgram = self._read_next_dgram()\n                dgram_list.append(dgram)\n\n            except Exception:\n                break\n\n        return dgram_list\n\n    elif k &lt; 0:\n        return self.readall()\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.readall","title":"<code>readall()</code>","text":"<p>Reads the entire file from the beginning and returns a list of datagrams.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def readall(self):\n    \"\"\"\n    Reads the entire file from the beginning and returns a list of datagrams.\n    \"\"\"\n\n    self.seek(0, SEEK_SET)\n    dgram_list = []\n\n    for raw_dgram in self.iter_dgrams():\n        dgram_list.append(raw_dgram)\n\n    return dgram_list\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.readline","title":"<code>readline()</code>","text":"<p>aliased to self.next()</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def readline(self):\n    \"\"\"\n    aliased to self.next()\n    \"\"\"\n    return next(self)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.readlines","title":"<code>readlines()</code>","text":"<p>aliased to self.read(-1)</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def readlines(self):\n    \"\"\"\n    aliased to self.read(-1)\n    \"\"\"\n    return self.read(-1)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.seek","title":"<code>seek(offset, whence)</code>","text":"<p>Performs the familiar 'seek' operation using datagram offsets instead of raw bytes.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def seek(self, offset, whence):\n    \"\"\"\n    Performs the familiar 'seek' operation using datagram offsets\n    instead of raw bytes.\n    \"\"\"\n\n    if whence == SEEK_SET:\n        if offset &lt; 0:\n            raise ValueError(\"Cannot seek backwards from beginning of file\")\n        else:\n            self._seek_bytes(0, SEEK_SET)\n            self._current_dgram_offset = 0\n    elif whence == SEEK_END:\n        if offset &gt; 0:\n            raise ValueError(\"Use negative offsets when seeking backward from end of file\")\n\n        # Do we need to generate the total number of datagrams w/in the file?\n        try:\n            self._set_total_dgram_count()\n            # Throws a value error if _total_dgram_count has already been set.  We can ignore it\n        except ValueError:\n            pass\n\n        self._seek_bytes(0, SEEK_END)\n        self._current_dgram_offset = self._total_dgram_count\n\n    elif whence == SEEK_CUR:\n        pass\n    else:\n        raise ValueError(\n            \"Illegal value for 'whence' (%s), use 0 (beginning), 1 (current), or 2 (end)\"\n            % (str(whence))\n        )\n\n    if offset &gt; 0:\n        for k in range(offset):\n            self.skip()\n    elif offset &lt; 0:\n        for k in range(-offset):\n            self.skip_back()\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.skip","title":"<code>skip()</code>","text":"<p>Skips forward to the next datagram without reading the contents of the current one</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def skip(self):\n    \"\"\"\n    Skips forward to the next datagram without reading the contents of the current one\n    \"\"\"\n\n    # dgram_size, dgram_type, (low_date, high_date) = self.peek()[:3]\n\n    header = self.peek()\n\n    if header[\"size\"] &lt; 16:\n        logger.warning(\n            \"Invalid datagram header: size: %d, type: %s, nt_date: %s.  dgram_size &lt; 16\",\n            header[\"size\"],\n            header[\"type\"],\n            str((header[\"low_date\"], header[\"high_date\"])),\n        )\n\n        self._find_next_datagram()\n\n    else:\n        self._seek_bytes(header[\"size\"] + 4, SEEK_CUR)\n        dgram_size_check = self._read_dgram_size()\n\n        if header[\"size\"] != dgram_size_check:\n            logger.warning(\n                \"Datagram failed size check:  %d != %d @ (%d, %d)\",\n                header[\"size\"],\n                dgram_size_check,\n                self._tell_bytes(),\n                self.tell(),\n            )\n            logger.warning(\"Skipping to next datagram... (in skip)\")\n\n            self._find_next_datagram()\n\n    self._current_dgram_offset += 1\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.skip_back","title":"<code>skip_back()</code>","text":"<p>Skips backwards to the previous datagram without reading it's contents</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def skip_back(self):\n    \"\"\"\n    Skips backwards to the previous datagram without reading it's contents\n    \"\"\"\n\n    old_file_pos = self._tell_bytes()\n\n    try:\n        self._seek_bytes(-4, SEEK_CUR)\n    except IOError:\n        raise\n\n    dgram_size_check = self._read_dgram_size()\n\n    # Seek to the beginning of the datagram and read as normal\n    try:\n        self._seek_bytes(-(8 + dgram_size_check), SEEK_CUR)\n    except IOError:\n        raise DatagramSizeError\n\n    try:\n        dgram_size = self._read_dgram_size()\n\n    except DatagramSizeError:\n        logger.info(\"Error reading the datagram\")\n        self._seek_bytes(old_file_pos, SEEK_SET)\n        raise\n\n    if dgram_size_check != dgram_size:\n        self._seek_bytes(old_file_pos, SEEK_SET)\n        raise DatagramSizeError\n    else:\n        self._seek_bytes(-4, SEEK_CUR)\n\n    self._current_dgram_offset -= 1\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_io.RawSimradFile.tell","title":"<code>tell()</code>","text":"<p>Returns the current file pointer offset by datagram number</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_io.py</code> <pre><code>def tell(self):\n    \"\"\"\n    Returns the current file pointer offset by datagram number\n    \"\"\"\n    return self._current_dgram_offset\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers","title":"<code>ek_raw_parsers</code>","text":"<p>Code originally developed for pyEcholab (https://github.com/CI-CMG/pyEcholab) by Rick Towler rick.towler@noaa.gov at NOAA AFSC.</p> <p>The code has been modified to handle split-beam data and channel-transducer structure from different EK80 setups.</p> <p>Classes:</p> Name Description <code>SimradAnnotationParser</code> <p>ER60 Annotation datagram contains the following keys:</p> <code>SimradBottomParser</code> <p>Bottom Detection datagram contains the following keys:</p> <code>SimradConfigParser</code> <p>Simrad Configuration Datagram parser operates on dictionaries with the following keys:</p> <code>SimradDepthParser</code> <p>ER60 Depth Detection datagram (from .bot files) contain the following keys:</p> <code>SimradNMEAParser</code> <p>ER60 NMEA datagram contains the following keys:</p> <code>SimradRawParser</code> <p>Sample Data Datagram parser operates on dictionaries with the following keys:</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradAnnotationParser","title":"<code>SimradAnnotationParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>ER60 Annotation datagram contains the following keys:</p> <pre><code>type:         string == 'TAG0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:     datetime.datetime object of NT date, assumed to be UTC\n\ntext:         Annotation\n</code></pre> <p>The following methods are defined:</p> <pre><code>from_string(str):    parse a raw ER60 Annotation datagram\n                    (with leading/trailing datagram size stripped)\n\nto_string():         Returns the datagram as a raw string\n                     (including leading/trailing size fields)\n                     ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradAnnotationParser(_SimradDatagramParser):\n    \"\"\"\n    ER60 Annotation datagram contains the following keys:\n\n\n        type:         string == 'TAG0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:     datetime.datetime object of NT date, assumed to be UTC\n\n        text:         Annotation\n\n    The following methods are defined:\n\n        from_string(str):    parse a raw ER60 Annotation datagram\n                            (with leading/trailing datagram size stripped)\n\n        to_string():         Returns the datagram as a raw string\n                             (including leading/trailing size fields)\n                             ready for writing to disk\n    \"\"\"\n\n    def __init__(self):\n        headers = {0: [(\"type\", \"4s\"), (\"low_date\", \"L\"), (\"high_date\", \"L\")]}\n\n        _SimradDatagramParser.__init__(self, \"TAG\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        \"\"\"\"\"\"\n\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                data[field] = data[field].decode()\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        #        if version == 0:\n        #            data['text'] = raw_string[self.header_size(version):].strip('\\x00')\n        #            if isinstance(data['text'], bytes):\n        #                data['text'] = data['text'].decode()\n\n        if version == 0:\n            if sys.version_info.major &gt; 2:\n                data[\"text\"] = str(\n                    raw_string[self.header_size(version) :].strip(b\"\\x00\"),\n                    \"ascii\",\n                    errors=\"replace\",\n                )\n            else:\n                data[\"text\"] = unicode(  # noqa\n                    raw_string[self.header_size(version) :].strip(\"\\x00\"),\n                    \"ascii\",\n                    errors=\"replace\",\n                )\n\n        return data\n\n    def _pack_contents(self, data, version):\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            if data[\"text\"][-1] != \"\\x00\":\n                tmp_string = data[\"text\"] + \"\\x00\"\n            else:\n                tmp_string = data[\"text\"]\n\n            # Pad with more nulls to 4-byte word boundary if necessary\n            if len(tmp_string) % 4:\n                tmp_string += \"\\x00\" * (4 - (len(tmp_string) % 4))\n\n            datagram_fmt += \"%ds\" % (len(tmp_string))\n            datagram_contents.append(tmp_string)\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradBottomParser","title":"<code>SimradBottomParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>Bottom Detection datagram contains the following keys:</p> <pre><code>type:         string == 'BOT0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ndatetime:     datetime.datetime object of NT date converted to UTC\ntransceiver_count:  long uint with number of transceivers\ndepth:        [float], one value for each active channel\n</code></pre> <p>The following methods are defined:</p> <pre><code>from_string(str):    parse a raw ER60 Bottom datagram\n                    (with leading/trailing datagram size stripped)\n\nto_string():         Returns the datagram as a raw string\n                     (including leading/trailing size fields)\n                     ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradBottomParser(_SimradDatagramParser):\n    \"\"\"\n    Bottom Detection datagram contains the following keys:\n\n        type:         string == 'BOT0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        datetime:     datetime.datetime object of NT date converted to UTC\n        transceiver_count:  long uint with number of transceivers\n        depth:        [float], one value for each active channel\n\n    The following methods are defined:\n\n        from_string(str):    parse a raw ER60 Bottom datagram\n                            (with leading/trailing datagram size stripped)\n\n        to_string():         Returns the datagram as a raw string\n                             (including leading/trailing size fields)\n                             ready for writing to disk\n    \"\"\"\n\n    def __init__(self):\n        headers = {\n            0: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"transceiver_count\", \"L\"),\n            ]\n        }\n        _SimradDatagramParser.__init__(self, \"BOT\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        \"\"\"\"\"\"\n\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                data[field] = data[field].decode()\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        if version == 0:\n            depth_fmt = \"=%dd\" % (data[\"transceiver_count\"],)\n            depth_size = struct.calcsize(depth_fmt)\n            buf_indx = self.header_size(version)\n            data[\"depth\"] = np.fromiter(\n                struct.unpack(depth_fmt, raw_string[buf_indx : buf_indx + depth_size]),  # noqa\n                \"float\",\n            )\n\n        return data\n\n    def _pack_contents(self, data, version):\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n            if len(data[\"depth\"]) != data[\"transceiver_count\"]:\n                logger.warning(\n                    \"# of depth values %d does not match transceiver count %d\",\n                    len(data[\"depth\"]),\n                    data[\"transceiver_count\"],\n                )\n\n                data[\"transceiver_count\"] = len(data[\"depth\"])\n\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            datagram_fmt += \"%dd\" % (data[\"transceiver_count\"])\n            datagram_contents.extend(data[\"depth\"])\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradConfigParser","title":"<code>SimradConfigParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>Simrad Configuration Datagram parser operates on dictionaries with the following keys:</p> <pre><code>type:         string == 'CON0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:    datetime.datetime object of NT date, assumed to be UTC\n\nsurvey_name                     [str]\ntransect_name                   [str]\nsounder_name                    [str]\nversion                         [str]\nspare0                          [str]\ntransceiver_count               [long]\ntransceivers                    [list] List of dicts representing Transducer Configs:\n\nME70 Data contains the following additional values (data contained w/in first 14\n    bytes of the spare0 field)\n\nmultiplexing                    [short]  Always 0\ntime_bias                       [long] difference between UTC and local time in min.\nsound_velocity_avg              [float] [m/s]\nsound_velocity_transducer       [float] [m/s]\nbeam_config                     [str] Raw XML string containing beam config. info\n</code></pre> <p>Transducer Config Keys (ER60/ES60/ES70 sounders):     channel_id                      [str]   channel ident string     beam_type                       [long]  Type of channel (0 = Single, 1 = Split)     frequency                       [float] channel frequency     equivalent_beam_angle           [float] dB     beamwidth_alongship             [float]     beamwidth_athwartship           [float]     angle_sensitivity_alongship     [float]     angle_sensitivity_athwartship   [float]     angle_offset_alongship          [float]     angle_offset_athwartship        [float]     pos_x                           [float]     pos_y                           [float]     pos_z                           [float]     dir_x                           [float]     dir_y                           [float]     dir_z                           [float]     pulse_length_table              [float[5]]     spare1                          [str]     gain_table                      [float[5]]     spare2                          [str]     sa_correction_table             [float[5]]     spare3                          [str]     gpt_software_version            [str]     spare4                          [str]</p> <p>Transducer Config Keys (ME70 sounders):     channel_id                      [str]   channel ident string     beam_type                       [long]  Type of channel (0 = Single, 1 = Split)     reserved1                       [float] channel frequency     equivalent_beam_angle           [float] dB     beamwidth_alongship             [float]     beamwidth_athwartship           [float]     angle_sensitivity_alongship     [float]     angle_sensitivity_athwartship   [float]     angle_offset_alongship          [float]     angle_offset_athwartship        [float]     pos_x                           [float]     pos_y                           [float]     pos_z                           [float]     beam_steering_angle_alongship   [float]     beam_steering_angle_athwartship [float]     beam_steering_angle_unused      [float]     pulse_length                    [float]     reserved2                       [float]     spare1                          [str]     gain                            [float]     reserved3                       [float]     spare2                          [str]     sa_correction                   [float]     reserved4                       [float]     spare3                          [str]     gpt_software_version            [str]     spare4                          [str]</p> <p>from_string(str):   parse a raw config datagram                     (with leading/trailing datagram size stripped)</p> <p>to_string(dict):    Returns raw string (including leading/trailing size fields)                     ready for writing to disk</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradConfigParser(_SimradDatagramParser):\n    \"\"\"\n    Simrad Configuration Datagram parser operates on dictionaries with the following keys:\n\n        type:         string == 'CON0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:    datetime.datetime object of NT date, assumed to be UTC\n\n        survey_name                     [str]\n        transect_name                   [str]\n        sounder_name                    [str]\n        version                         [str]\n        spare0                          [str]\n        transceiver_count               [long]\n        transceivers                    [list] List of dicts representing Transducer Configs:\n\n        ME70 Data contains the following additional values (data contained w/in first 14\n            bytes of the spare0 field)\n\n        multiplexing                    [short]  Always 0\n        time_bias                       [long] difference between UTC and local time in min.\n        sound_velocity_avg              [float] [m/s]\n        sound_velocity_transducer       [float] [m/s]\n        beam_config                     [str] Raw XML string containing beam config. info\n\n\n    Transducer Config Keys (ER60/ES60/ES70 sounders):\n        channel_id                      [str]   channel ident string\n        beam_type                       [long]  Type of channel (0 = Single, 1 = Split)\n        frequency                       [float] channel frequency\n        equivalent_beam_angle           [float] dB\n        beamwidth_alongship             [float]\n        beamwidth_athwartship           [float]\n        angle_sensitivity_alongship     [float]\n        angle_sensitivity_athwartship   [float]\n        angle_offset_alongship          [float]\n        angle_offset_athwartship        [float]\n        pos_x                           [float]\n        pos_y                           [float]\n        pos_z                           [float]\n        dir_x                           [float]\n        dir_y                           [float]\n        dir_z                           [float]\n        pulse_length_table              [float[5]]\n        spare1                          [str]\n        gain_table                      [float[5]]\n        spare2                          [str]\n        sa_correction_table             [float[5]]\n        spare3                          [str]\n        gpt_software_version            [str]\n        spare4                          [str]\n\n    Transducer Config Keys (ME70 sounders):\n        channel_id                      [str]   channel ident string\n        beam_type                       [long]  Type of channel (0 = Single, 1 = Split)\n        reserved1                       [float] channel frequency\n        equivalent_beam_angle           [float] dB\n        beamwidth_alongship             [float]\n        beamwidth_athwartship           [float]\n        angle_sensitivity_alongship     [float]\n        angle_sensitivity_athwartship   [float]\n        angle_offset_alongship          [float]\n        angle_offset_athwartship        [float]\n        pos_x                           [float]\n        pos_y                           [float]\n        pos_z                           [float]\n        beam_steering_angle_alongship   [float]\n        beam_steering_angle_athwartship [float]\n        beam_steering_angle_unused      [float]\n        pulse_length                    [float]\n        reserved2                       [float]\n        spare1                          [str]\n        gain                            [float]\n        reserved3                       [float]\n        spare2                          [str]\n        sa_correction                   [float]\n        reserved4                       [float]\n        spare3                          [str]\n        gpt_software_version            [str]\n        spare4                          [str]\n\n    from_string(str):   parse a raw config datagram\n                        (with leading/trailing datagram size stripped)\n\n    to_string(dict):    Returns raw string (including leading/trailing size fields)\n                        ready for writing to disk\n    \"\"\"\n\n    COMMON_KEYS = [\n        (\"channel_id\", \"128s\"),\n        (\"beam_type\", \"l\"),\n        (\"frequency\", \"f\"),\n        (\"gain\", \"f\"),\n        (\"equivalent_beam_angle\", \"f\"),\n        (\"beamwidth_alongship\", \"f\"),\n        (\"beamwidth_athwartship\", \"f\"),\n        (\"angle_sensitivity_alongship\", \"f\"),\n        (\"angle_sensitivity_athwartship\", \"f\"),\n        (\"angle_offset_alongship\", \"f\"),\n        (\"angle_offset_athwartship\", \"f\"),\n        (\"pos_x\", \"f\"),\n        (\"pos_y\", \"f\"),\n        (\"pos_z\", \"f\"),\n        (\"dir_x\", \"f\"),\n        (\"dir_y\", \"f\"),\n        (\"dir_z\", \"f\"),\n        (\"pulse_length_table\", \"5f\"),\n        (\"spare1\", \"8s\"),\n        (\"gain_table\", \"5f\"),\n        (\"spare2\", \"8s\"),\n        (\"sa_correction_table\", \"5f\"),\n        (\"spare3\", \"8s\"),\n        (\"gpt_software_version\", \"16s\"),\n        (\"spare4\", \"28s\"),\n    ]\n\n    def __init__(self):\n        headers = {\n            0: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"survey_name\", \"128s\"),\n                (\"transect_name\", \"128s\"),\n                (\"sounder_name\", \"128s\"),\n                (\"version\", \"30s\"),\n                (\"spare0\", \"98s\"),\n                (\"transceiver_count\", \"l\"),\n            ],\n            1: [(\"type\", \"4s\"), (\"low_date\", \"L\"), (\"high_date\", \"L\")],\n        }\n\n        _SimradDatagramParser.__init__(self, \"CON\", headers)\n\n        self._transducer_headers = {\n            \"ER60\": self.COMMON_KEYS,\n            \"ES60\": self.COMMON_KEYS,\n            \"ES70\": self.COMMON_KEYS,\n            \"MBES\": [\n                (\"channel_id\", \"128s\"),\n                (\"beam_type\", \"l\"),\n                (\"frequency\", \"f\"),\n                (\"reserved1\", \"f\"),\n                (\"equivalent_beam_angle\", \"f\"),\n                (\"beamwidth_alongship\", \"f\"),\n                (\"beamwidth_athwartship\", \"f\"),\n                (\"angle_sensitivity_alongship\", \"f\"),\n                (\"angle_sensitivity_athwartship\", \"f\"),\n                (\"angle_offset_alongship\", \"f\"),\n                (\"angle_offset_athwartship\", \"f\"),\n                (\"pos_x\", \"f\"),\n                (\"pos_y\", \"f\"),\n                (\"pos_z\", \"f\"),\n                (\"beam_steering_angle_alongship\", \"f\"),\n                (\"beam_steering_angle_athwartship\", \"f\"),\n                (\"beam_steering_angle_unused\", \"f\"),\n                (\"pulse_length\", \"f\"),\n                (\"reserved2\", \"f\"),\n                (\"spare1\", \"20s\"),\n                (\"gain\", \"f\"),\n                (\"reserved3\", \"f\"),\n                (\"spare2\", \"20s\"),\n                (\"sa_correction\", \"f\"),\n                (\"reserved4\", \"f\"),\n                (\"spare3\", \"20s\"),\n                (\"gpt_software_version\", \"16s\"),\n                (\"spare4\", \"28s\"),\n            ],\n        }\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        data = {}\n        round6 = lambda x: round(x, ndigits=6)  # noqa\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n\n            #  handle Python 3 strings\n            if (sys.version_info.major &gt; 2) and isinstance(data[field], bytes):\n                data[field] = data[field].decode(\"latin_1\")\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        if version == 0:\n            data[\"transceivers\"] = {}\n\n            for field in [\"transect_name\", \"version\", \"survey_name\", \"sounder_name\"]:\n                data[field] = data[field].strip(\"\\x00\")\n\n            sounder_name = data[\"sounder_name\"]\n            if sounder_name == \"MBES\":\n                _me70_extra_values = struct.unpack(\"=hLff\", data[\"spare0\"][:14])\n                data[\"multiplexing\"] = _me70_extra_values[0]\n                data[\"time_bias\"] = _me70_extra_values[1]\n                data[\"sound_velocity_avg\"] = _me70_extra_values[2]\n                data[\"sound_velocity_transducer\"] = _me70_extra_values[3]\n                data[\"spare0\"] = data[\"spare0\"][:14] + data[\"spare0\"][14:].strip(\"\\x00\")\n\n            else:\n                data[\"spare0\"] = data[\"spare0\"].strip(\"\\x00\")\n\n            buf_indx = self.header_size(version)\n\n            try:\n                transducer_header = self._transducer_headers[sounder_name]\n                _sounder_name_used = sounder_name\n            except KeyError:\n                logger.warning(\n                    \"Unknown sounder_name:  %s, (no one of %s)\",\n                    sounder_name,\n                    list(self._transducer_headers.keys()),\n                )\n                logger.warning(\"Will use ER60 transducer config fields as default\")\n\n                transducer_header = self._transducer_headers[\"ER60\"]\n                _sounder_name_used = \"ER60\"\n\n            txcvr_header_fields = [x[0] for x in transducer_header]\n            txcvr_header_fmt = \"=\" + \"\".join([x[1] for x in transducer_header])\n            txcvr_header_size = struct.calcsize(txcvr_header_fmt)\n\n            for txcvr_indx in range(1, data[\"transceiver_count\"] + 1):\n                txcvr_header_values_encoded = struct.unpack(\n                    txcvr_header_fmt,\n                    raw_string[buf_indx : buf_indx + txcvr_header_size],  # noqa\n                )\n                txcvr_header_values = list(txcvr_header_values_encoded)\n                for tx_idx, tx_val in enumerate(txcvr_header_values_encoded):\n                    if isinstance(tx_val, bytes):\n                        txcvr_header_values[tx_idx] = tx_val.decode(\"latin_1\")\n\n                txcvr = data[\"transceivers\"].setdefault(txcvr_indx, {})\n\n                if _sounder_name_used in [\"ER60\", \"ES60\", \"ES70\"]:\n                    for txcvr_field_indx, field in enumerate(txcvr_header_fields[:17]):\n                        txcvr[field] = txcvr_header_values[txcvr_field_indx]\n\n                    txcvr[\"pulse_length_table\"] = np.fromiter(\n                        list(map(round6, txcvr_header_values[17:22])), \"float\"\n                    )\n                    txcvr[\"spare1\"] = txcvr_header_values[22]\n                    txcvr[\"gain_table\"] = np.fromiter(\n                        list(map(round6, txcvr_header_values[23:28])), \"float\"\n                    )\n                    txcvr[\"spare2\"] = txcvr_header_values[28]\n                    txcvr[\"sa_correction_table\"] = np.fromiter(\n                        list(map(round6, txcvr_header_values[29:34])), \"float\"\n                    )\n                    txcvr[\"spare3\"] = txcvr_header_values[34]\n                    txcvr[\"gpt_software_version\"] = txcvr_header_values[35]\n                    txcvr[\"spare4\"] = txcvr_header_values[36]\n\n                elif _sounder_name_used == \"MBES\":\n                    for txcvr_field_indx, field in enumerate(txcvr_header_fields):\n                        txcvr[field] = txcvr_header_values[txcvr_field_indx]\n\n                else:\n                    raise RuntimeError(\n                        \"Unknown _sounder_name_used (Should not happen, this is a bug!)\"\n                    )\n\n                txcvr[\"channel_id\"] = txcvr[\"channel_id\"].strip(\"\\x00\")\n                txcvr[\"spare1\"] = txcvr[\"spare1\"].strip(\"\\x00\")\n                txcvr[\"spare2\"] = txcvr[\"spare2\"].strip(\"\\x00\")\n                txcvr[\"spare3\"] = txcvr[\"spare3\"].strip(\"\\x00\")\n                txcvr[\"spare4\"] = txcvr[\"spare4\"].strip(\"\\x00\")\n                txcvr[\"gpt_software_version\"] = txcvr[\"gpt_software_version\"].strip(\"\\x00\")\n\n                buf_indx += txcvr_header_size\n\n        elif version == 1:\n            # CON1 only has a single data field:  beam_config, holding an xml string\n            data[\"beam_config\"] = raw_string[self.header_size(version) :].strip(\"\\x00\")\n\n        return data\n\n    def _pack_contents(self, data, version):\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n            if data[\"transceiver_count\"] != len(data[\"transceivers\"]):\n                logger.warning(\"Mismatch between 'transceiver_count' and actual # of transceivers\")\n                data[\"transceiver_count\"] = len(data[\"transceivers\"])\n\n            sounder_name = data[\"sounder_name\"]\n            if sounder_name == \"MBES\":\n                _packed_me70_values = struct.pack(\n                    \"=hLff\",\n                    data[\"multiplexing\"],\n                    data[\"time_bias\"],\n                    data[\"sound_velocity_avg\"],\n                    data[\"sound_velocity_transducer\"],\n                )\n                data[\"spare0\"] = _packed_me70_values + data[\"spare0\"][14:]\n\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            try:\n                transducer_header = self._transducer_headers[sounder_name]\n                _sounder_name_used = sounder_name\n            except KeyError:\n                logger.warning(\n                    \"Unknown sounder_name:  %s, (no one of %s)\",\n                    sounder_name,\n                    list(self._transducer_headers.keys()),\n                )\n                logger.warning(\"Will use ER60 transducer config fields as default\")\n\n                transducer_header = self._transducer_headers[\"ER60\"]\n                _sounder_name_used = \"ER60\"\n\n            txcvr_header_fields = [x[0] for x in transducer_header]\n            txcvr_header_fmt = \"=\" + \"\".join([x[1] for x in transducer_header])\n            txcvr_header_size = struct.calcsize(txcvr_header_fmt)  # noqa\n\n            for txcvr_indx, txcvr in list(data[\"transceivers\"].items()):\n                txcvr_contents = []\n\n                if _sounder_name_used in [\"ER60\", \"ES60\", \"ES70\"]:\n                    for field in txcvr_header_fields[:17]:\n                        txcvr_contents.append(txcvr[field])\n\n                    txcvr_contents.extend(txcvr[\"pulse_length_table\"])\n                    txcvr_contents.append(txcvr[\"spare1\"])\n\n                    txcvr_contents.extend(txcvr[\"gain_table\"])\n                    txcvr_contents.append(txcvr[\"spare2\"])\n\n                    txcvr_contents.extend(txcvr[\"sa_correction_table\"])\n                    txcvr_contents.append(txcvr[\"spare3\"])\n\n                    txcvr_contents.extend([txcvr[\"gpt_software_version\"], txcvr[\"spare4\"]])\n\n                    txcvr_contents_str = struct.pack(txcvr_header_fmt, *txcvr_contents)\n\n                elif _sounder_name_used == \"MBES\":\n                    for field in txcvr_header_fields:\n                        txcvr_contents.append(txcvr[field])\n\n                    txcvr_contents_str = struct.pack(txcvr_header_fmt, *txcvr_contents)\n\n                else:\n                    raise RuntimeError(\n                        \"Unknown _sounder_name_used (Should not happen, this is a bug!)\"\n                    )\n\n                datagram_fmt += \"%ds\" % (len(txcvr_contents_str))\n                datagram_contents.append(txcvr_contents_str)\n\n        elif version == 1:\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            datagram_fmt += \"%ds\" % (len(data[\"beam_config\"]))\n            datagram_contents.append(data[\"beam_config\"])\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradDepthParser","title":"<code>SimradDepthParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>ER60 Depth Detection datagram (from .bot files) contain the following keys:</p> <pre><code>type:         string == 'DEP0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:    datetime.datetime object of NT date, assumed to be UTC\ntransceiver_count:  [long uint] with number of transceivers\n\ndepth:        [float], one value for each active channel\nreflectivity: [float], one value for each active channel\nunused:       [float], unused value for each active channel\n</code></pre> <p>The following methods are defined:</p> <pre><code>from_string(str):    parse a raw ER60 Depth datagram\n                     (with leading/trailing datagram size stripped)\n\nto_string():         Returns the datagram as a raw string\n                     (including leading/trailing size fields)\n                     ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradDepthParser(_SimradDatagramParser):\n    \"\"\"\n    ER60 Depth Detection datagram (from .bot files) contain the following keys:\n\n        type:         string == 'DEP0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:    datetime.datetime object of NT date, assumed to be UTC\n        transceiver_count:  [long uint] with number of transceivers\n\n        depth:        [float], one value for each active channel\n        reflectivity: [float], one value for each active channel\n        unused:       [float], unused value for each active channel\n\n    The following methods are defined:\n\n        from_string(str):    parse a raw ER60 Depth datagram\n                             (with leading/trailing datagram size stripped)\n\n        to_string():         Returns the datagram as a raw string\n                             (including leading/trailing size fields)\n                             ready for writing to disk\n\n    \"\"\"\n\n    def __init__(self):\n        headers = {\n            0: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"transceiver_count\", \"L\"),\n            ]\n        }\n        _SimradDatagramParser.__init__(self, \"DEP\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        \"\"\"\"\"\"\n\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                data[field] = data[field].decode()\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        if version == 0:\n            data_fmt = \"=3f\"\n            data_size = struct.calcsize(data_fmt)\n\n            data[\"depth\"] = np.zeros((data[\"transceiver_count\"],))\n            data[\"reflectivity\"] = np.zeros((data[\"transceiver_count\"],))\n            data[\"unused\"] = np.zeros((data[\"transceiver_count\"],))\n\n            buf_indx = self.header_size(version)\n            for indx in range(data[\"transceiver_count\"]):\n                d, r, u = struct.unpack(\n                    data_fmt, raw_string[buf_indx : buf_indx + data_size]  # noqa\n                )\n                data[\"depth\"][indx] = d\n                data[\"reflectivity\"][indx] = r\n                data[\"unused\"][indx] = u\n\n                buf_indx += data_size\n\n        return data\n\n    def _pack_contents(self, data, version):\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n            lengths = [\n                len(data[\"depth\"]),\n                len(data[\"reflectivity\"]),\n                len(data[\"unused\"]),\n                data[\"transceiver_count\"],\n            ]\n\n            if len(set(lengths)) != 1:\n                min_indx = min(lengths)\n                logger.warning(\"Data lengths mismatched:  d:%d, r:%d, u:%d, t:%d\", *lengths)\n                logger.warning(\"  Using minimum value:  %d\", min_indx)\n                data[\"transceiver_count\"] = min_indx\n\n            else:\n                min_indx = data[\"transceiver_count\"]\n\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            datagram_fmt += \"%df\" % (3 * data[\"transceiver_count\"])\n\n            for indx in range(data[\"transceiver_count\"]):\n                datagram_contents.extend(\n                    [\n                        data[\"depth\"][indx],\n                        data[\"reflectivity\"][indx],\n                        data[\"unused\"][indx],\n                    ]\n                )\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradFILParser","title":"<code>SimradFILParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>EK80 FIL datagram contains the following keys:</p> <pre><code>type:               string == 'FIL1'\nlow_date:           long uint representing LSBytes of 64bit NT date\nhigh_date:          long uint representing MSBytes of 64bit NT date\ntimestamp:          datetime.datetime object of NT date, assumed to be UTC\nstage:              int\nchannel_id:         string\nn_coefficients:     int\ndecimation_factor:  int\ncoefficients:       np.complex64\n</code></pre> <p>The following methods are defined:</p> <pre><code>from_string(str):    parse a raw EK80 FIL datagram\n                    (with leading/trailing datagram size stripped)\n\nto_string():         Returns the datagram as a raw string\n                    (including leading/trailing size fields)\n                     ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradFILParser(_SimradDatagramParser):\n    \"\"\"\n    EK80 FIL datagram contains the following keys:\n\n\n        type:               string == 'FIL1'\n        low_date:           long uint representing LSBytes of 64bit NT date\n        high_date:          long uint representing MSBytes of 64bit NT date\n        timestamp:          datetime.datetime object of NT date, assumed to be UTC\n        stage:              int\n        channel_id:         string\n        n_coefficients:     int\n        decimation_factor:  int\n        coefficients:       np.complex64\n\n    The following methods are defined:\n\n        from_string(str):    parse a raw EK80 FIL datagram\n                            (with leading/trailing datagram size stripped)\n\n        to_string():         Returns the datagram as a raw string\n                            (including leading/trailing size fields)\n                             ready for writing to disk\n    \"\"\"\n\n    def __init__(self):\n        headers = {\n            1: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"stage\", \"h\"),\n                (\"spare\", \"2s\"),\n                (\"channel_id\", \"128s\"),\n                (\"n_coefficients\", \"h\"),\n                (\"decimation_factor\", \"h\"),\n            ]\n        }\n\n        _SimradDatagramParser.__init__(self, \"FIL\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        data = {}\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n\n            #  handle Python 3 strings\n            if (sys.version_info.major &gt; 2) and isinstance(data[field], bytes):\n                data[field] = data[field].decode(\"latin_1\")\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        if version == 1:\n            #  clean up the channel ID\n            data[\"channel_id\"] = data[\"channel_id\"].strip(\"\\x00\")\n\n            #  unpack the coefficients\n            indx = self.header_size(version)\n            block_size = data[\"n_coefficients\"] * 8\n            data[\"coefficients\"] = np.frombuffer(\n                raw_string[indx : indx + block_size], dtype=\"complex64\"  # noqa\n            )\n\n        return data\n\n    def _pack_contents(self, data, version):\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n            pass\n\n        elif version == 1:\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            datagram_fmt += \"%ds\" % (len(data[\"beam_config\"]))\n            datagram_contents.append(data[\"beam_config\"])\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradIDXParser","title":"<code>SimradIDXParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>ER60/EK80 IDX datagram contains the following keys:</p> <pre><code>type:         string == 'IDX0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:    datetime.datetime object of NT date, assumed to be UTC\nping_number:  int\ndistance :    float\nlatitude:     float\nlongitude:    float\nfile_offset:  int\n</code></pre> <p>The following methods are defined:</p> <pre><code>from_string(str):   Parse a raw ER60/EK80 IDX datagram\n                    (with leading/trailing datagram size stripped)\n\nto_string():    Returns the datagram as a raw string (including leading/trailing size\n                fields) ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradIDXParser(_SimradDatagramParser):\n    \"\"\"\n    ER60/EK80 IDX datagram contains the following keys:\n\n\n        type:         string == 'IDX0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:    datetime.datetime object of NT date, assumed to be UTC\n        ping_number:  int\n        distance :    float\n        latitude:     float\n        longitude:    float\n        file_offset:  int\n\n    The following methods are defined:\n\n        from_string(str):   Parse a raw ER60/EK80 IDX datagram\n                            (with leading/trailing datagram size stripped)\n\n        to_string():    Returns the datagram as a raw string (including leading/trailing size\n                        fields) ready for writing to disk\n    \"\"\"\n\n    def __init__(self):\n        headers = {\n            0: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                # ('dummy', 'L'),   # There are 4 extra bytes in this datagram\n                (\"ping_number\", \"L\"),\n                (\"distance\", \"d\"),\n                (\"latitude\", \"d\"),\n                (\"longitude\", \"d\"),\n                (\"file_offset\", \"L\"),\n            ]\n        }\n\n        _SimradDatagramParser.__init__(self, \"IDX\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        \"\"\"\n        Unpacks the data in raw_string into dictionary containing IDX data\n\n        :param raw_string:\n        :type raw_string: str\n\n        :returns: None\n        \"\"\"\n\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                #  first try to decode as utf-8 but fall back to latin_1 if that fails\n                try:\n                    data[field] = data[field].decode(\"utf-8\")\n                except:\n                    data[field] = data[field].decode(\"latin_1\")\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"timestamp\"] = data[\"timestamp\"].replace(tzinfo=None)\n        data[\"bytes_read\"] = bytes_read\n\n        return data\n\n    def _pack_contents(self, data, version):\n\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n\n            for field in self.header_fields(version):\n                if isinstance(data[field], str):\n                    data[field] = data[field].encode(\"latin_1\")\n                datagram_contents.append(data[field])\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradMRUParser","title":"<code>SimradMRUParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>EK80 MRU datagram contains the following keys:</p> <pre><code>type:         string == 'MRU0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:    datetime.datetime object of NT date, assumed to be UTC\nheave:        float\nroll :        float\npitch:        float\nheading:      float\n</code></pre> <p>Version 1 contains (from https://www3.mbari.org/products/mbsystem/formatdoc/KongsbergKmall/EMdgmFormat_RevH/html/kmBinary.html): # noqa</p> <p>Status word See 1)  uint32  4U Latitude    deg double  8F Longitude   deg double  8F Ellipsoid height    m   float   4F Roll    deg float   4F Pitch   deg float   4F Heading deg float   4F Heave   m   float   4F Roll rate   deg/s   float   4F Pitch rate  deg/s   float   4F Yaw rate    deg/s   float   4F North velocity  m/s float   4F East velocity   m/s float   4F Down velocity   m/s float   4F Latitude error  m   float   4F Longitude error m   float   4F Height error    m   float   4F Roll error  deg float   4F Pitch error deg float   4F Heading error   deg float   4F Heave error m   float   4F North acceleration  m/s2    float   4F East acceleration   m/s2    float   4F Down acceleration   m/s2    float   4F Delayed heave:  -   -   - UTC seconds s   uint32  4U UTC nanoseconds ns  uint32  4U Delayed heave   m   float   4F</p> <p>The following methods are defined:</p> <pre><code>from_string(str):   parse a raw EK80 MRU datagram\n                    (with leading/trailing datagram size stripped)\n\nto_string():        Returns the datagram as a raw string (including\n                    leading/trailing size fields) ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradMRUParser(_SimradDatagramParser):\n    \"\"\"\n    EK80 MRU datagram contains the following keys:\n\n\n        type:         string == 'MRU0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:    datetime.datetime object of NT date, assumed to be UTC\n        heave:        float\n        roll :        float\n        pitch:        float\n        heading:      float\n\n    Version 1 contains (from https://www3.mbari.org/products/mbsystem/formatdoc/KongsbergKmall/EMdgmFormat_RevH/html/kmBinary.html): # noqa\n\n    Status word See 1)  uint32  4U\n    Latitude    deg double  8F\n    Longitude   deg double  8F\n    Ellipsoid height    m   float   4F\n    Roll    deg float   4F\n    Pitch   deg float   4F\n    Heading deg float   4F\n    Heave   m   float   4F\n    Roll rate   deg/s   float   4F\n    Pitch rate  deg/s   float   4F\n    Yaw rate    deg/s   float   4F\n    North velocity  m/s float   4F\n    East velocity   m/s float   4F\n    Down velocity   m/s float   4F\n    Latitude error  m   float   4F\n    Longitude error m   float   4F\n    Height error    m   float   4F\n    Roll error  deg float   4F\n    Pitch error deg float   4F\n    Heading error   deg float   4F\n    Heave error m   float   4F\n    North acceleration  m/s2    float   4F\n    East acceleration   m/s2    float   4F\n    Down acceleration   m/s2    float   4F\n    Delayed heave:  -   -   -\n    UTC seconds s   uint32  4U\n    UTC nanoseconds ns  uint32  4U\n    Delayed heave   m   float   4F\n\n    The following methods are defined:\n\n        from_string(str):   parse a raw EK80 MRU datagram\n                            (with leading/trailing datagram size stripped)\n\n        to_string():        Returns the datagram as a raw string (including\n                            leading/trailing size fields) ready for writing to disk\n    \"\"\"\n\n    def __init__(self):\n        headers = {\n            0: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"heave\", \"f\"),\n                (\"roll\", \"f\"),\n                (\"pitch\", \"f\"),\n                (\"heading\", \"f\"),\n            ],\n            1: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"start_id\", \"4s\"),  # KMB#\n                (\"status_word\", \"L\"),\n                (\"dummy\", \"12s\"),\n                (\"latitude\", \"d\"),\n                (\"longitude\", \"d\"),\n                (\"ellipsoid_height\", \"f\"),\n                (\"roll\", \"f\"),\n                (\"pitch\", \"f\"),\n                (\"heading\", \"f\"),\n                (\"heave\", \"f\"),\n                (\"roll_rate\", \"f\"),\n                (\"pitch_rate\", \"f\"),\n                (\"yaw_rate\", \"f\"),\n                (\"velocity_north\", \"f\"),\n                (\"velocity_east\", \"f\"),\n                (\"velocity_down\", \"f\"),\n                (\"latitude_error\", \"f\"),\n                (\"longitude_error\", \"f\"),\n                (\"height_error\", \"f\"),\n                (\"roll_error\", \"f\"),\n                (\"pitch_error\", \"f\"),\n                (\"heading_error\", \"f\"),\n                (\"heave_error\", \"f\"),\n                (\"accel_north\", \"f\"),\n                (\"accel_east\", \"f\"),\n                (\"accel_down\", \"f\"),\n                (\"heave_delay_secs\", \"L\"),\n                (\"heave_delay_usecs\", \"L\"),\n                (\"heave_delay_m\", \"f\"),\n            ],\n        }\n\n        _SimradDatagramParser.__init__(self, \"MRU\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        \"\"\"\n        Unpacks the data in raw_string into dictionary containing MRU data\n\n        :param raw_string:\n        :type raw_string: str\n\n        :returns: None\n        \"\"\"\n\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                #  first try to decode as utf-8 but fall back to latin_1 if that fails\n                try:\n                    data[field] = data[field].decode(\"utf-8\")\n                except:\n                    data[field] = data[field].decode(\"latin_1\")\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"timestamp\"] = data[\"timestamp\"].replace(tzinfo=None)\n        data[\"bytes_read\"] = bytes_read\n\n        return data\n\n    def _pack_contents(self, data, version):\n\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n\n            for field in self.header_fields(version):\n                if isinstance(data[field], str):\n                    data[field] = data[field].encode(\"latin_1\")\n                datagram_contents.append(data[field])\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradNMEAParser","title":"<code>SimradNMEAParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>ER60 NMEA datagram contains the following keys:</p> <pre><code>type:         string == 'NME0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:     datetime.datetime object of NT date, assumed to be UTC\n\nnmea_string:  full (original) NMEA string\n</code></pre> <p>The following methods are defined:</p> <pre><code>from_string(str):    parse a raw ER60 NMEA datagram\n                    (with leading/trailing datagram size stripped)\n\nto_string():         Returns the datagram as a raw string\n                     (including leading/trailing size fields)\n                     ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradNMEAParser(_SimradDatagramParser):\n    \"\"\"\n    ER60 NMEA datagram contains the following keys:\n\n\n        type:         string == 'NME0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:     datetime.datetime object of NT date, assumed to be UTC\n\n        nmea_string:  full (original) NMEA string\n\n    The following methods are defined:\n\n        from_string(str):    parse a raw ER60 NMEA datagram\n                            (with leading/trailing datagram size stripped)\n\n        to_string():         Returns the datagram as a raw string\n                             (including leading/trailing size fields)\n                             ready for writing to disk\n    \"\"\"\n\n    nmea_head_re = re.compile(r\"\\$[A-Za-z]{5},\")  # noqa\n\n    def __init__(self):\n        headers = {\n            0: [(\"type\", \"4s\"), (\"low_date\", \"L\"), (\"high_date\", \"L\")],\n            1: [(\"type\", \"4s\"), (\"low_date\", \"L\"), (\"high_date\", \"L\"), (\"port\", \"32s\")],\n        }\n\n        _SimradDatagramParser.__init__(self, \"NME\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        \"\"\"\n        Parses the NMEA string provided in raw_string\n\n        :param raw_string:  Raw NMEA string (i.e. '$GPZDA,160012.71,11,03,2004,-1,00*7D')\n        :type raw_string: str\n\n        :returns: None\n        \"\"\"\n\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                data[field] = data[field].decode()\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        # Remove trailing \\x00 from the PORT field for NME1, rest of the datagram identical to NME0\n        if version == 1:\n            data[\"port\"] = data[\"port\"].strip(\"\\x00\")\n\n        if version == 0 or version == 1:\n            if sys.version_info.major &gt; 2:\n                data[\"nmea_string\"] = str(\n                    raw_string[self.header_size(version) :].strip(b\"\\x00\"),\n                    \"ascii\",\n                    errors=\"replace\",\n                )\n            else:\n                data[\"nmea_string\"] = unicode(  # noqa\n                    raw_string[self.header_size(version) :].strip(\"\\x00\"),\n                    \"ascii\",\n                    errors=\"replace\",\n                )\n\n            if self.nmea_head_re.match(data[\"nmea_string\"][:7]) is not None:\n                data[\"nmea_talker\"] = data[\"nmea_string\"][1:3]\n                data[\"nmea_type\"] = data[\"nmea_string\"][3:6]\n            else:\n                data[\"nmea_talker\"] = \"\"\n                data[\"nmea_type\"] = \"UNKNOWN\"\n\n        return data\n\n    def _pack_contents(self, data, version):\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            if data[\"nmea_string\"][-1] != \"\\x00\":\n                tmp_string = data[\"nmea_string\"] + \"\\x00\"\n            else:\n                tmp_string = data[\"nmea_string\"]\n\n            # Pad with more nulls to 4-byte word boundary if necessary\n            if len(tmp_string) % 4:\n                tmp_string += \"\\x00\" * (4 - (len(tmp_string) % 4))\n\n            datagram_fmt += \"%ds\" % (len(tmp_string))\n\n            # Convert to python string if needed\n            if isinstance(tmp_string, str):\n                tmp_string = tmp_string.encode(\"ascii\", errors=\"replace\")\n\n            datagram_contents.append(tmp_string)\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradRawParser","title":"<code>SimradRawParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>Sample Data Datagram parser operates on dictionaries with the following keys:</p> <pre><code>type:         string == 'RAW0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:    datetime.datetime object of NT date, assumed to be UTC\n\nchannel                         [short] Channel number\nmode                            [short] 1 = Power only, 2 = Angle only 3 = Power &amp; Angle\ntransducer_depth                [float]\nfrequency                       [float]\ntransmit_power                  [float]\npulse_length                    [float]\nbandwidth                       [float]\nsample_interval                 [float]\nsound_velocity                  [float]\nabsorption_coefficient          [float]\nheave                           [float]\nroll                            [float]\npitch                           [float]\ntemperature                     [float]\nheading                         [float]\ntransmit_mode                   [short] 0 = Active, 1 = Passive, 2 = Test, -1 = Unknown\nspare0                          [str]\noffset                          [long]\ncount                           [long]\n\npower                           [numpy array] Unconverted power values (if present)\nangle                           [numpy array] Unconverted angle values (if present)\n</code></pre> <p>from_string(str):   parse a raw sample datagram                     (with leading/trailing datagram size stripped)</p> <p>to_string(dict):    Returns raw string (including leading/trailing size fields)                     ready for writing to disk</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradRawParser(_SimradDatagramParser):\n    \"\"\"\n    Sample Data Datagram parser operates on dictionaries with the following keys:\n\n        type:         string == 'RAW0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:    datetime.datetime object of NT date, assumed to be UTC\n\n        channel                         [short] Channel number\n        mode                            [short] 1 = Power only, 2 = Angle only 3 = Power &amp; Angle\n        transducer_depth                [float]\n        frequency                       [float]\n        transmit_power                  [float]\n        pulse_length                    [float]\n        bandwidth                       [float]\n        sample_interval                 [float]\n        sound_velocity                  [float]\n        absorption_coefficient          [float]\n        heave                           [float]\n        roll                            [float]\n        pitch                           [float]\n        temperature                     [float]\n        heading                         [float]\n        transmit_mode                   [short] 0 = Active, 1 = Passive, 2 = Test, -1 = Unknown\n        spare0                          [str]\n        offset                          [long]\n        count                           [long]\n\n        power                           [numpy array] Unconverted power values (if present)\n        angle                           [numpy array] Unconverted angle values (if present)\n\n    from_string(str):   parse a raw sample datagram\n                        (with leading/trailing datagram size stripped)\n\n    to_string(dict):    Returns raw string (including leading/trailing size fields)\n                        ready for writing to disk\n    \"\"\"\n\n    def __init__(self):\n        headers = {\n            0: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"channel\", \"h\"),\n                (\"mode\", \"h\"),\n                (\"transducer_depth\", \"f\"),\n                (\"frequency\", \"f\"),\n                (\"transmit_power\", \"f\"),\n                (\"pulse_length\", \"f\"),\n                (\"bandwidth\", \"f\"),\n                (\"sample_interval\", \"f\"),\n                (\"sound_velocity\", \"f\"),\n                (\"absorption_coefficient\", \"f\"),\n                (\"heave\", \"f\"),\n                (\"roll\", \"f\"),\n                (\"pitch\", \"f\"),\n                (\"temperature\", \"f\"),\n                (\"heading\", \"f\"),\n                (\"transmit_mode\", \"h\"),\n                (\"spare0\", \"6s\"),\n                (\"offset\", \"l\"),\n                (\"count\", \"l\"),\n            ],\n            3: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"channel_id\", \"128s\"),\n                (\"data_type\", \"h\"),\n                (\"spare\", \"2s\"),\n                (\"offset\", \"l\"),\n                (\"count\", \"l\"),\n            ],\n            4: [\n                (\"type\", \"4s\"),\n                (\"low_date\", \"L\"),\n                (\"high_date\", \"L\"),\n                (\"channel_id\", \"128s\"),\n                (\"data_type\", \"h\"),\n                (\"spare\", \"2s\"),\n                (\"offset\", \"l\"),\n                (\"count\", \"l\"),\n            ],\n        }\n        _SimradDatagramParser.__init__(self, \"RAW\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                data[field] = data[field].decode(encoding=\"unicode_escape\")\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        if version == 0:\n            if data[\"count\"] &gt; 0:\n                block_size = data[\"count\"] * 2\n                indx = self.header_size(version)\n\n                if int(data[\"mode\"]) &amp; 0x1:\n                    data[\"power\"] = np.frombuffer(\n                        raw_string[indx : indx + block_size], dtype=\"int16\"  # noqa\n                    )\n                    indx += block_size\n                else:\n                    data[\"power\"] = None\n\n                if int(data[\"mode\"]) &amp; 0x2:\n                    data[\"angle\"] = np.frombuffer(\n                        raw_string[indx : indx + block_size], dtype=\"int8\"  # noqa\n                    )\n                    data[\"angle\"] = data[\"angle\"].reshape((-1, 2))\n                else:\n                    data[\"angle\"] = None\n\n            else:\n                data[\"power\"] = np.empty((0,), dtype=\"int16\")\n                data[\"angle\"] = np.empty((0, 2), dtype=\"int8\")\n\n        # RAW3 and RAW4 have the same format, only Datatype Bit 0-1 not used in RAW4\n        elif version == 3 or version == 4:\n            # result = 1j*Data[...,1]; result += Data[...,0]\n\n            #  clean up the channel ID\n            data[\"channel_id\"] = data[\"channel_id\"].strip(\"\\x00\")\n\n            if data[\"count\"] &gt; 0:\n                #  set the initial block size and indx value.\n                block_size = data[\"count\"] * 2\n                indx = self.header_size(version)\n\n                if data[\"data_type\"] &amp; 0b1:\n                    data[\"power\"] = np.frombuffer(\n                        raw_string[indx : indx + block_size], dtype=\"int16\"  # noqa\n                    )\n                    indx += block_size\n                else:\n                    data[\"power\"] = None\n\n                if data[\"data_type\"] &amp; 0b10:\n                    data[\"angle\"] = np.frombuffer(\n                        raw_string[indx : indx + block_size], dtype=\"int8\"  # noqa\n                    )\n                    data[\"angle\"] = data[\"angle\"].reshape((-1, 2))\n                    indx += block_size\n                else:\n                    data[\"angle\"] = None\n\n                #  determine the complex sample data type - this is contained in bits 2 and 3\n                #  of the datatype &lt;short&gt; value. I'm assuming the types are exclusive...\n                data[\"complex_dtype\"] = np.float16\n                type_bytes = 2\n                if data[\"data_type\"] &amp; 0b1000:\n                    data[\"complex_dtype\"] = np.float32\n                    type_bytes = 8\n\n                #  determine the number of complex samples\n                data[\"n_complex\"] = data[\"data_type\"] &gt;&gt; 8\n\n                #  unpack the complex samples\n                if data[\"n_complex\"] &gt; 0:\n                    #  determine the block size\n                    block_size = data[\"count\"] * data[\"n_complex\"] * type_bytes\n\n                    data[\"complex\"] = np.frombuffer(\n                        raw_string[indx : indx + block_size],  # noqa\n                        dtype=data[\"complex_dtype\"],\n                    )\n                    data[\"complex\"].dtype = np.complex64\n                    if version == 3:\n                        data[\"complex\"] = data[\"complex\"].reshape((-1, data[\"n_complex\"]))\n                else:\n                    data[\"complex\"] = None\n\n            else:\n                data[\"power\"] = np.empty((0,), dtype=\"int16\")\n                data[\"angle\"] = np.empty((0,), dtype=\"int8\")\n                data[\"complex\"] = np.empty((0,), dtype=\"complex64\")\n                data[\"n_complex\"] = 0\n\n        return data\n\n    def _pack_contents(self, data, version):\n        datagram_fmt = self.header_fmt(version)\n\n        datagram_contents = []\n\n        if version == 0:\n            if data[\"count\"] &gt; 0:\n                if (int(data[\"mode\"]) &amp; 0x1) and (len(data.get(\"power\", [])) != data[\"count\"]):\n                    logger.warning(\n                        \"Data 'count' = %d, but contains %d power samples.  Ignoring power.\"\n                    )\n                    data[\"mode\"] &amp;= ~(1 &lt;&lt; 0)\n\n                if (int(data[\"mode\"]) &amp; 0x2) and (len(data.get(\"angle\", [])) != data[\"count\"]):\n                    logger.warning(\n                        \"Data 'count' = %d, but contains %d angle samples.  Ignoring angle.\"\n                    )\n                    data[\"mode\"] &amp;= ~(1 &lt;&lt; 1)\n\n                if data[\"mode\"] == 0:\n                    logger.warning(\n                        \"Data 'count' = %d, but mode == 0.  Setting count to 0\",\n                        data[\"count\"],\n                    )\n                    data[\"count\"] = 0\n\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            if data[\"count\"] &gt; 0:\n                if int(data[\"mode\"]) &amp; 0x1:\n                    datagram_fmt += \"%dh\" % (data[\"count\"])\n                    datagram_contents.extend(data[\"power\"])\n\n                if int(data[\"mode\"]) &amp; 0x2:\n                    datagram_fmt += \"%dH\" % (data[\"count\"])\n                    datagram_contents.extend(data[\"angle\"])\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.ek_raw_parsers.SimradXMLParser","title":"<code>SimradXMLParser</code>","text":"<p>               Bases: <code>_SimradDatagramParser</code></p> <p>EK80 XML datagram contains the following keys:</p> <pre><code>type:         string == 'XML0'\nlow_date:     long uint representing LSBytes of 64bit NT date\nhigh_date:    long uint representing MSBytes of 64bit NT date\ntimestamp:    datetime.datetime object of NT date, assumed to be UTC\nsubtype:      string representing Simrad XML datagram type:\n              configuration, environment, or parameter\n\n[subtype]:    dict containing the data specific to the XML subtype.\n</code></pre> <p>The following methods are defined:</p> <pre><code>from_string(str):    parse a raw EK80 XML datagram\n                    (with leading/trailing datagram size stripped)\n\nto_string():         Returns the datagram as a raw string\n                     (including leading/trailing size fields)\n                     ready for writing to disk\n</code></pre> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\ek_raw_parsers.py</code> <pre><code>class SimradXMLParser(_SimradDatagramParser):\n    \"\"\"\n    EK80 XML datagram contains the following keys:\n\n\n        type:         string == 'XML0'\n        low_date:     long uint representing LSBytes of 64bit NT date\n        high_date:    long uint representing MSBytes of 64bit NT date\n        timestamp:    datetime.datetime object of NT date, assumed to be UTC\n        subtype:      string representing Simrad XML datagram type:\n                      configuration, environment, or parameter\n\n        [subtype]:    dict containing the data specific to the XML subtype.\n\n    The following methods are defined:\n\n        from_string(str):    parse a raw EK80 XML datagram\n                            (with leading/trailing datagram size stripped)\n\n        to_string():         Returns the datagram as a raw string\n                             (including leading/trailing size fields)\n                             ready for writing to disk\n    \"\"\"\n\n    #  define the XML parsing options - here we define dictionaries for various xml datagram\n    #  types. When parsing that xml datagram, these dictionaries are used to inform the parser about\n    #  type conversion, name wrangling, and delimiter. If a field is missing, the parser\n    #  assumes no conversion: type will be string, default mangling, and that there is only 1\n    #  element.\n    #\n    #  the dicts are in the form:\n    #       'XMLParamName':[converted type,'fieldname', 'parse char']\n    #\n    #  For example: 'PulseDurationFM':[float,'pulse_duration_fm',';']\n    #\n    #  will result in a return dictionary field named 'pulse_duration_fm' that contains a list\n    #  of float values parsed from a string that uses ';' to separate values. Empty strings\n    #  for fieldname and/or parse char result in the default action for those parsing steps.\n\n    channel_parsing_options = {\n        \"MaxTxPowerTransceiver\": [int, \"\", \"\"],\n        \"PulseDuration\": [float, \"\", \";\"],\n        \"PulseDurationFM\": [float, \"pulse_duration_fm\", \";\"],\n        \"SampleInterval\": [float, \"\", \";\"],\n        \"ChannelID\": [str, \"channel_id\", \"\"],\n        \"HWChannelConfiguration\": [str, \"hw_channel_configuration\", \"\"],\n    }\n\n    transceiver_parsing_options = {\n        \"TransceiverNumber\": [int, \"\", \"\"],\n        \"Version\": [str, \"transceiver_version\", \"\"],\n        \"IPAddress\": [str, \"ip_address\", \"\"],\n        \"Impedance\": [int, \"\", \"\"],\n    }\n\n    transducer_parsing_options = {\n        \"SerialNumber\": [str, \"transducer_serial_number\", \"\"],\n        \"Frequency\": [float, \"transducer_frequency\", \"\"],\n        \"FrequencyMinimum\": [float, \"transducer_frequency_minimum\", \"\"],\n        \"FrequencyMaximum\": [float, \"transducer_frequency_maximum\", \"\"],\n        \"BeamType\": [int, \"transducer_beam_type\", \"\"],\n        \"Gain\": [float, \"\", \";\"],\n        \"SaCorrection\": [float, \"\", \";\"],\n        \"MaxTxPowerTransducer\": [float, \"\", \"\"],\n        \"EquivalentBeamAngle\": [float, \"\", \"\"],\n        \"BeamWidthAlongship\": [float, \"\", \"\"],\n        \"BeamWidthAthwartship\": [float, \"\", \"\"],\n        \"AngleSensitivityAlongship\": [float, \"\", \"\"],\n        \"AngleSensitivityAthwartship\": [float, \"\", \"\"],\n        \"AngleOffsetAlongship\": [float, \"\", \"\"],\n        \"AngleOffsetAthwartship\": [float, \"\", \"\"],\n        \"DirectivityDropAt2XBeamWidth\": [\n            float,\n            \"directivity_drop_at_2x_beam_width\",\n            \"\",\n        ],\n        \"TransducerOffsetX\": [float, \"\", \"\"],\n        \"TransducerOffsetY\": [float, \"\", \"\"],\n        \"TransducerOffsetZ\": [float, \"\", \"\"],\n        \"TransducerAlphaX\": [float, \"\", \"\"],\n        \"TransducerAlphaY\": [float, \"\", \"\"],\n        \"TransducerAlphaZ\": [float, \"\", \"\"],\n    }\n\n    header_parsing_options = {\"Version\": [str, \"application_version\", \"\"]}\n\n    envxdcr_parsing_options = {\"SoundSpeed\": [float, \"transducer_sound_speed\", \"\"]}\n\n    environment_parsing_options = {\n        \"Depth\": [float, \"\", \"\"],\n        \"Acidity\": [float, \"\", \"\"],\n        \"Salinity\": [float, \"\", \"\"],\n        \"SoundSpeed\": [float, \"\", \"\"],\n        \"Temperature\": [float, \"\", \"\"],\n        \"Latitude\": [float, \"\", \"\"],\n        \"SoundVelocityProfile\": [float, \"\", \";\"],\n        \"DropKeelOffset\": [float, \"\", \"\"],\n        \"DropKeelOffsetIsManual\": [int, \"\", \"\"],\n        \"WaterLevelDraft\": [float, \"\", \"\"],\n        \"WaterLevelDraftIsManual\": [int, \"\", \"\"],\n    }\n\n    parameter_parsing_options = {\n        \"ChannelID\": [str, \"channel_id\", \"\"],\n        \"ChannelMode\": [int, \"\", \"\"],\n        \"PulseForm\": [int, \"\", \"\"],\n        \"Frequency\": [float, \"\", \"\"],\n        \"PulseDuration\": [float, \"\", \"\"],\n        \"SampleInterval\": [float, \"\", \"\"],\n        \"TransmitPower\": [float, \"\", \"\"],\n        \"Slope\": [float, \"\", \"\"],\n    }\n\n    def __init__(self):\n        headers = {0: [(\"type\", \"4s\"), (\"low_date\", \"L\"), (\"high_date\", \"L\")]}\n        _SimradDatagramParser.__init__(self, \"XML\", headers)\n\n    def _unpack_contents(self, raw_string, bytes_read, version):\n        \"\"\"\n        Parses the NMEA string provided in raw_string\n\n        :param raw_string:  Raw NMEA string (i.e. '$GPZDA,160012.71,11,03,2004,-1,00*7D')\n        :type raw_string: str\n\n        :returns: None\n        \"\"\"\n\n        def dict_to_dict(xml_dict, data_dict, parse_opts):\n            \"\"\"\n            dict_to_dict appends the ETree xml value dicts to a provided dictionary\n            and along the way converts the key name to conform to the project's\n            naming convention and optionally parses and or converts values as\n            specified in the parse_opts dictionary.\n            \"\"\"\n\n            for k in xml_dict:\n                #  check if we're parsing this key/value\n                if k in parse_opts:\n                    #  try to parse the string\n                    if parse_opts[k][2]:\n                        try:\n                            data = xml_dict[k].split(parse_opts[k][2])\n                        except:\n                            #  bad or empty parse character(s) provided\n                            data = xml_dict[k]\n                    else:\n                        #  no parse char provided - nothing to parse\n                        data = xml_dict[k]\n\n                    #  try to convert to specified type\n                    if isinstance(data, list):\n                        for i in range(len(data)):\n                            try:\n                                data[i] = parse_opts[k][0](data[i])\n                            except:\n                                pass\n                    else:\n                        data = parse_opts[k][0](data)\n\n                    #  and add the value to the provided dict\n                    if parse_opts[k][1]:\n                        #  add using the specified key name\n                        data_dict[parse_opts[k][1]] = data\n                    else:\n                        #  add using the default key name wrangling\n                        data_dict[camelcase2snakecase(k)] = data\n                else:\n                    #  nothing to do with the value string\n                    data = xml_dict[k]\n\n                    #  add the parameter to the provided dictionary\n                    data_dict[camelcase2snakecase(k)] = data\n\n        header_values = struct.unpack(\n            self.header_fmt(version), raw_string[: self.header_size(version)]\n        )\n        data = {}\n\n        for indx, field in enumerate(self.header_fields(version)):\n            data[field] = header_values[indx]\n            if isinstance(data[field], bytes):\n                data[field] = data[field].decode()\n\n        data[\"timestamp\"] = nt_to_unix((data[\"low_date\"], data[\"high_date\"]))\n        data[\"bytes_read\"] = bytes_read\n\n        if version == 0:\n            if sys.version_info.major &gt; 2:\n                xml_string = str(\n                    raw_string[self.header_size(version) :].strip(b\"\\x00\"),\n                    \"ascii\",\n                    errors=\"replace\",\n                )\n            else:\n                xml_string = unicode(  # noqa\n                    raw_string[self.header_size(version) :].strip(\"\\x00\"),\n                    \"ascii\",\n                    errors=\"replace\",\n                )\n\n            #  get the ElementTree element\n            root = ET.fromstring(xml_string)\n\n            #  get the XML message type\n            data[\"subtype\"] = root.tag.lower()\n\n            #  create the dictionary that contains the message data\n            data[data[\"subtype\"]] = {}\n\n            #  parse it\n            if data[\"subtype\"] == \"configuration\":\n                #  parse the Transceiver section\n                for tcvr in root.iter(\"Transceiver\"):\n                    #  parse the Transceiver section\n                    tcvr_xml = tcvr.attrib\n\n                    #  parse the Channel section -- this works with multiple channels\n                    #  under 1 transceiver\n                    for tcvr_ch in tcvr.iter(\"Channel\"):\n                        tcvr_ch_xml = tcvr_ch.attrib\n                        channel_id = tcvr_ch_xml[\"ChannelID\"]\n\n                        #  create the configuration dict for this channel\n                        data[\"configuration\"][channel_id] = {}\n\n                        #  add the transceiver data to the config dict (this is\n                        #  replicated for all channels)\n                        dict_to_dict(\n                            tcvr_xml,\n                            data[\"configuration\"][channel_id],\n                            self.transceiver_parsing_options,\n                        )\n\n                        #  add the general channel data to the config dict\n                        dict_to_dict(\n                            tcvr_ch_xml,\n                            data[\"configuration\"][channel_id],\n                            self.channel_parsing_options,\n                        )\n\n                        #  check if there are &gt;1 transducer under a single transceiver channel\n                        if len(list(tcvr_ch)) &gt; 1:\n                            ValueError(\"Found &gt;1 transducer under a single transceiver channel!\")\n                        else:  # should only have 1 transducer\n                            tcvr_ch_xducer = tcvr_ch.find(\n                                \"Transducer\"\n                            )  # get Element of this xducer\n                            f_par = tcvr_ch_xducer.findall(\"FrequencyPar\")\n                            # Save calibration parameters\n                            if f_par:\n                                cal_par = {\n                                    \"frequency\": np.array(\n                                        [int(f.attrib[\"Frequency\"]) for f in f_par]\n                                    ),\n                                    \"gain\": np.array([float(f.attrib[\"Gain\"]) for f in f_par]),\n                                    \"impedance\": np.array(\n                                        [float(f.attrib[\"Impedance\"]) for f in f_par]\n                                    ),\n                                    \"phase\": np.array([float(f.attrib[\"Phase\"]) for f in f_par]),\n                                    \"beamwidth_alongship\": np.array(\n                                        [float(f.attrib[\"BeamWidthAlongship\"]) for f in f_par]\n                                    ),\n                                    \"beamwidth_athwartship\": np.array(\n                                        [float(f.attrib[\"BeamWidthAthwartship\"]) for f in f_par]\n                                    ),\n                                    \"angle_offset_alongship\": np.array(\n                                        [float(f.attrib[\"AngleOffsetAlongship\"]) for f in f_par]\n                                    ),\n                                    \"angle_offset_athwartship\": np.array(\n                                        [float(f.attrib[\"AngleOffsetAthwartship\"]) for f in f_par]\n                                    ),\n                                }\n                                data[\"configuration\"][channel_id][\"calibration\"] = cal_par\n                            #  add the transducer data to the config dict\n                            dict_to_dict(\n                                tcvr_ch_xducer.attrib,\n                                data[\"configuration\"][channel_id],\n                                self.transducer_parsing_options,\n                            )\n\n                        # get unique transceiver channel number stored in channel_id\n                        tcvr_ch_num = TCVR_CH_NUM_MATCHER.search(channel_id)[0]\n\n                        # parse the Transducers section from the root\n                        # TODO Remove Transducers if doesn't exist\n                        xducer = root.find(\"Transducers\")\n                        if xducer is not None:\n                            # built occurrence lookup table for transducer name\n                            xducer_name_list = []\n                            for xducer_ch in xducer.iter(\"Transducer\"):\n                                xducer_name_list.append(xducer_ch.attrib[\"TransducerName\"])\n\n                            # find matching transducer for this channel_id\n                            match_found = False\n                            for xducer_ch in xducer.iter(\"Transducer\"):\n                                if not match_found:\n                                    xducer_ch_xml = xducer_ch.attrib\n                                    match_name = (\n                                        xducer_ch.attrib[\"TransducerName\"]\n                                        == tcvr_ch_xducer.attrib[\"TransducerName\"]\n                                    )\n                                    if xducer_ch.attrib[\"TransducerSerialNumber\"] == \"\":\n                                        match_sn = False\n                                    else:\n                                        match_sn = (\n                                            xducer_ch.attrib[\"TransducerSerialNumber\"]\n                                            == tcvr_ch_xducer.attrib[\"SerialNumber\"]\n                                        )\n                                    match_tcvr = (\n                                        tcvr_ch_num in xducer_ch.attrib[\"TransducerCustomName\"]\n                                    )\n\n                                    # if find match add the transducer mounting details\n                                    if (\n                                        Counter(xducer_name_list)[\n                                            xducer_ch.attrib[\"TransducerName\"]\n                                        ]\n                                        &gt; 1\n                                    ):\n                                        # if more than one transducer has the same name\n                                        # only check sn and transceiver unique number\n                                        match_found = match_sn or match_tcvr\n                                    else:\n                                        match_found = match_name or match_sn or match_tcvr\n\n                                    # add transducer mounting details\n                                    if match_found:\n                                        dict_to_dict(\n                                            xducer_ch_xml,\n                                            data[\"configuration\"][channel_id],\n                                            self.transducer_parsing_options,\n                                        )\n\n                        #  add the header data to the config dict\n                        h = root.find(\"Header\")\n                        dict_to_dict(\n                            h.attrib,\n                            data[\"configuration\"][channel_id],\n                            self.header_parsing_options,\n                        )\n\n            elif data[\"subtype\"] == \"parameter\":\n                #  parse the parameter XML datagram\n                for h in root.iter(\"Channel\"):\n                    parm_xml = h.attrib\n                    #  add the data to the environment dict\n                    dict_to_dict(parm_xml, data[\"parameter\"], self.parameter_parsing_options)\n\n            elif data[\"subtype\"] == \"environment\":\n                #  parse the environment XML datagram\n                for h in root.iter(\"Environment\"):\n                    env_xml = h.attrib\n                    #  add the data to the environment dict\n                    dict_to_dict(env_xml, data[\"environment\"], self.environment_parsing_options)\n\n                for h in root.iter(\"Transducer\"):\n                    transducer_xml = h.attrib\n                    #  add the data to the environment dict\n                    dict_to_dict(\n                        transducer_xml,\n                        data[\"environment\"],\n                        self.envxdcr_parsing_options,\n                    )\n\n        data[\"xml\"] = xml_string\n        return data\n\n    def _pack_contents(self, data, version):\n        def to_CamelCase(xml_param):\n            \"\"\"\n            convert name from project's convention to CamelCase for converting back to\n            XML to in Kongsberg's convention.\n            \"\"\"\n            idx = list(reversed([i for i, c in enumerate(xml_param) if c.isupper()]))\n            param_len = len(xml_param)\n            for i in idx:\n                #  check if we should insert an underscore\n                if idx &gt; 0 and idx &lt; param_len - 1:\n                    xml_param = xml_param[:idx] + \"_\" + xml_param[idx:]\n            xml_param = xml_param.lower()\n\n            return xml_param\n\n        datagram_fmt = self.header_fmt(version)\n        datagram_contents = []\n\n        if version == 0:\n            for field in self.header_fields(version):\n                datagram_contents.append(data[field])\n\n            if data[\"nmea_string\"][-1] != \"\\x00\":\n                tmp_string = data[\"nmea_string\"] + \"\\x00\"\n            else:\n                tmp_string = data[\"nmea_string\"]\n\n            # Pad with more nulls to 4-byte word boundary if necessary\n            if len(tmp_string) % 4:\n                tmp_string += \"\\x00\" * (4 - (len(tmp_string) % 4))\n\n            datagram_fmt += \"%ds\" % (len(tmp_string))\n\n            # Convert to python string if needed\n            if isinstance(tmp_string, str):\n                tmp_string = tmp_string.encode(\"ascii\", errors=\"replace\")\n\n            datagram_contents.append(tmp_string)\n\n        return struct.pack(datagram_fmt, *datagram_contents)\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.log","title":"<code>log</code>","text":"<p>Functions:</p> Name Description <code>verbose</code> <p>Set the verbosity for echopype print outs.</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.log.verbose","title":"<code>verbose(logfile=None, override=False)</code>","text":"<p>Set the verbosity for echopype print outs. If called it will output logs to terminal by default.</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.log.verbose--parameters","title":"Parameters","text":"<p>logfile : str, optional     Optional string path to the desired log file. override: bool     Boolean flag to override verbosity,     which turns off verbosity if the value is <code>False</code>.     Default is <code>False</code>.</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.log.verbose--returns","title":"Returns","text":"<p>None</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\log.py</code> <pre><code>def verbose(logfile: Optional[str] = None, override: bool = False) -&gt; None:\n    \"\"\"Set the verbosity for echopype print outs.\n    If called it will output logs to terminal by default.\n\n    Parameters\n    ----------\n    logfile : str, optional\n        Optional string path to the desired log file.\n    override: bool\n        Boolean flag to override verbosity,\n        which turns off verbosity if the value is `False`.\n        Default is `False`.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    if not isinstance(override, bool):\n        raise ValueError(\"override argument must be a boolean!\")\n    package_name = __name__.split(\".\")[0]  # Get the package name\n    loggers = _get_all_loggers()\n    verbose = True if override is False else False\n    _set_verbose(verbose)\n    for logger in loggers:\n        if package_name in logger.name:\n            handlers = [h.name for h in logger.handlers]\n            if logfile is None:\n                if LOGFILE_HANDLE_NAME in handlers:\n                    # Remove log file handler if it exists\n                    handler = next(filter(lambda h: h.name == LOGFILE_HANDLE_NAME, logger.handlers))\n                    logger.removeHandler(handler)\n            elif LOGFILE_HANDLE_NAME not in handlers:\n                # Only add the logfile handler if it doesn't exist\n                _set_logfile(logger, logfile)\n\n            if isinstance(logfile, str):\n                # Prevents multiple handler from propagating messages\n                # this way there are no duplicate line in logfile\n                logger.propagate = False\n            else:\n                logger.propagate = True\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.misc","title":"<code>misc</code>","text":"<p>Functions:</p> Name Description <code>camelcase2snakecase</code> <p>Convert string from CamelCase to snake_case</p> <code>depth_from_pressure</code> <p>Convert pressure to depth using UNESCO 1983 algorithm.</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.misc.camelcase2snakecase","title":"<code>camelcase2snakecase(camel_case_str)</code>","text":"<p>Convert string from CamelCase to snake_case e.g. CamelCase becomes camel_case.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\misc.py</code> <pre><code>def camelcase2snakecase(camel_case_str):\n    \"\"\"\n    Convert string from CamelCase to snake_case\n    e.g. CamelCase becomes camel_case.\n    \"\"\"\n    idx = list(reversed([i for i, c in enumerate(camel_case_str) if c.isupper()]))\n    param_len = len(camel_case_str)\n    for i in idx:\n        #  check if we should insert an underscore\n        if i &gt; 0 and i &lt; param_len:\n            camel_case_str = camel_case_str[:i] + \"_\" + camel_case_str[i:]\n\n    return camel_case_str.lower()\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.misc.depth_from_pressure","title":"<code>depth_from_pressure(pressure, latitude=30.0, atm_pres_surf=0.0)</code>","text":"<p>Convert pressure to depth using UNESCO 1983 algorithm.</p> <p>UNESCO. 1983. Algorithms for computation of fundamental properties of seawater (Pressure to Depth conversion, pages 25-27). Prepared by Fofonoff, N.P. and Millard, R.C. UNESCO technical papers in marine science, 44. http://unesdoc.unesco.org/images/0005/000598/059832eb.pdf</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.misc.depth_from_pressure--parameters","title":"Parameters","text":"<p>pressure : Union[float, FloatSequence]     Pressure in dbar latitude : Union[float, FloatSequence], default=30.0     Latitude in decimal degrees. atm_pres_surf : Union[float, FloatSequence], default=0.0     Atmospheric pressure at the surface in dbar.     Use the default 0.0 value if pressure is corrected to be 0 at the surface.     Otherwise, enter a correction for pressure due to air, sea ice and any other     medium that may be present</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.misc.depth_from_pressure--returns","title":"Returns","text":"<p>depth : NDArray[float]     Depth in meters</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\misc.py</code> <pre><code>def depth_from_pressure(\n    pressure: Union[float, FloatSequence],\n    latitude: Optional[Union[float, FloatSequence]] = 30.0,\n    atm_pres_surf: Optional[Union[float, FloatSequence]] = 0.0,\n) -&gt; NDArray[float]:\n    \"\"\"\n    Convert pressure to depth using UNESCO 1983 algorithm.\n\n    UNESCO. 1983. Algorithms for computation of fundamental properties of seawater (Pressure to\n    Depth conversion, pages 25-27). Prepared by Fofonoff, N.P. and Millard, R.C. UNESCO technical\n    papers in marine science, 44. http://unesdoc.unesco.org/images/0005/000598/059832eb.pdf\n\n    Parameters\n    ----------\n    pressure : Union[float, FloatSequence]\n        Pressure in dbar\n    latitude : Union[float, FloatSequence], default=30.0\n        Latitude in decimal degrees.\n    atm_pres_surf : Union[float, FloatSequence], default=0.0\n        Atmospheric pressure at the surface in dbar.\n        Use the default 0.0 value if pressure is corrected to be 0 at the surface.\n        Otherwise, enter a correction for pressure due to air, sea ice and any other\n        medium that may be present\n\n    Returns\n    -------\n    depth : NDArray[float]\n        Depth in meters\n    \"\"\"\n\n    def _as_nparray_check(v, check_vs_pressure=False):\n        \"\"\"\n        Convert to np.array if not already a np.array.\n        Ensure latitude and atm_pres_surf are of the same size and shape as\n        pressure if they are not scalar.\n        \"\"\"\n        v_array = np.array(v) if not isinstance(v, np.ndarray) else v\n        if check_vs_pressure:\n            if v_array.size != 1:\n                if v_array.size != pressure.size or v_array.shape != pressure.shape:\n                    raise ValueError(\"Sequence shape or size does not match pressure\")\n        return v_array\n\n    pressure = _as_nparray_check(pressure)\n    latitude = _as_nparray_check(latitude, check_vs_pressure=True)\n    atm_pres_surf = _as_nparray_check(atm_pres_surf, check_vs_pressure=True)\n\n    # Constants\n    g = 9.780318\n    c1 = 9.72659\n    c2 = -2.2512e-5\n    c3 = 2.279e-10\n    c4 = -1.82e-15\n    k1 = 5.2788e-3\n    k2 = 2.36e-5\n    k3 = 1.092e-6\n\n    # Calculate depth\n    pressure = pressure - atm_pres_surf\n    depth_w_g = c1 * pressure + c2 * pressure**2 + c3 * pressure**3 + c4 * pressure**4\n    x = np.sin(np.deg2rad(latitude))\n    gravity = g * (1.0 + k1 * x**2 + k2 * x**4) + k3 * pressure\n    depth = depth_w_g / gravity\n    return depth\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.sonar_checker","title":"<code>sonar_checker</code>","text":"<p>Functions:</p> Name Description <code>is_AD2CP</code> <p>Check if the provided file has a .ad2cp extension.</p> <code>is_AZFP</code> <p>Check if the specified XML file contains an  with string=\"AZFP\". <code>is_AZFP6</code> <p>Check if the provided file has a .azfp extension.</p> <code>is_EK60</code> <p>Check if a raw data file is from Simrad EK60 echosounder.</p> <code>is_EK80</code> <p>Check if a raw data file is from Simrad EK80 echosounder.</p> <code>is_ER60</code> <p>Check if a raw data file is from Simrad EK60 echosounder.</p>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.sonar_checker.is_AD2CP","title":"<code>is_AD2CP(raw_file)</code>","text":"<p>Check if the provided file has a .ad2cp extension.</p> <p>Parameters: raw_file (str): The name of the file to check.</p> <p>Returns: bool: True if the file has a .ad2cp extension, False otherwise.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\sonar_checker.py</code> <pre><code>def is_AD2CP(raw_file):\n    \"\"\"\n    Check if the provided file has a .ad2cp extension.\n\n    Parameters:\n    raw_file (str): The name of the file to check.\n\n    Returns:\n    bool: True if the file has a .ad2cp extension, False otherwise.\n    \"\"\"\n\n    # Check if the input is a string\n    if not isinstance(raw_file, str):\n        return False  # Return False if the input is not a string\n\n    # Use the str.lower() method to check for the .ad2cp extension\n    has_ad2cp_extension = raw_file.lower().endswith(\".ad2cp\")\n\n    # Return the result of the check\n    return has_ad2cp_extension\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.sonar_checker.is_AZFP","title":"<code>is_AZFP(raw_file)</code>","text":"<p>Check if the specified XML file contains an  with string=\"AZFP\". <p>Parameters: raw_file (str): The base name of the XML file (with or without extension).</p> <p>Returns: bool: True if  with string=\"AZFP\" is found, False otherwise. Source code in <code>src\\aalibrary\\utils\\sonar_checker\\sonar_checker.py</code> <pre><code>def is_AZFP(raw_file):\n    \"\"\"\n    Check if the specified XML file contains an &lt;InstrumentType&gt; with string=\"AZFP\".\n\n    Parameters:\n    raw_file (str): The base name of the XML file (with or without extension).\n\n    Returns:\n    bool: True if &lt;InstrumentType&gt; with string=\"AZFP\" is found, False otherwise.\n    \"\"\"\n\n    # Check if the filename ends with .xml or .XML, and strip the extension if it does\n    base_filename = raw_file.rstrip(\".xml\").rstrip(\".XML\")\n\n    # Create a list of possible filenames with both extensions\n    possible_files = [f\"{base_filename}.xml\", f\"{base_filename}.XML\"]\n\n    for full_filename in possible_files:\n        if os.path.isfile(full_filename):\n            try:\n                # Parse the XML file\n                tree = ET.parse(full_filename)\n                root = tree.getroot()\n\n                # Check for &lt;InstrumentType&gt; elements\n                for instrument in root.findall(\".//InstrumentType\"):\n                    if instrument.get(\"string\") == \"AZFP\":\n                        return True\n            except ET.ParseError:\n                print(f\"Error parsing the XML file: {full_filename}.\")\n\n    return False\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.sonar_checker.is_AZFP6","title":"<code>is_AZFP6(raw_file)</code>","text":"<p>Check if the provided file has a .azfp extension.</p> <p>Parameters: raw_file (str): The name of the file to check.</p> <p>Returns: bool: True if the file has a .azfp extension, False otherwise.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\sonar_checker.py</code> <pre><code>def is_AZFP6(raw_file):\n    \"\"\"\n    Check if the provided file has a .azfp extension.\n\n    Parameters:\n    raw_file (str): The name of the file to check.\n\n    Returns:\n    bool: True if the file has a .azfp extension, False otherwise.\n    \"\"\"\n\n    # Check if the input is a string\n    if not isinstance(raw_file, str):\n        return False  # Return False if the input is not a string\n\n    # Use the str.lower() method to check for the .azfp extension\n    has_azfp_extension = raw_file.lower().endswith(\".azfp\")\n\n    # Return the result of the check\n    return has_azfp_extension\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.sonar_checker.is_EK60","title":"<code>is_EK60(raw_file, storage_options)</code>","text":"<p>Check if a raw data file is from Simrad EK60 echosounder.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\sonar_checker.py</code> <pre><code>def is_EK60(raw_file, storage_options):\n    \"\"\"Check if a raw data file is from Simrad EK60 echosounder.\"\"\"\n    with RawSimradFile(raw_file, \"r\", storage_options=storage_options) as fid:\n        config_datagram = fid.read(1)\n        config_datagram[\"timestamp\"] = np.datetime64(\n            config_datagram[\"timestamp\"].replace(tzinfo=None), \"[ns]\"\n        )\n\n        try:\n            # Return True if the sounder name matches \"EK60\"\n            return config_datagram[\"sounder_name\"] in {\"ER60\", \"EK60\"}\n        except KeyError:\n            return False\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.sonar_checker.is_EK80","title":"<code>is_EK80(raw_file, storage_options)</code>","text":"<p>Check if a raw data file is from Simrad EK80 echosounder.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\sonar_checker.py</code> <pre><code>def is_EK80(raw_file, storage_options):\n    \"\"\"Check if a raw data file is from Simrad EK80 echosounder.\"\"\"\n    with RawSimradFile(raw_file, \"r\", storage_options=storage_options) as fid:\n        config_datagram = fid.read(1)\n        config_datagram[\"timestamp\"] = np.datetime64(\n            config_datagram[\"timestamp\"].replace(tzinfo=None), \"[ns]\"\n        )\n\n        # Return True if \"configuration\" exists in config_datagram\n        return \"configuration\" in config_datagram\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.sonar_checker.sonar_checker.is_ER60","title":"<code>is_ER60(raw_file, storage_options)</code>","text":"<p>Check if a raw data file is from Simrad EK60 echosounder.</p> Source code in <code>src\\aalibrary\\utils\\sonar_checker\\sonar_checker.py</code> <pre><code>def is_ER60(raw_file, storage_options):\n    \"\"\"Check if a raw data file is from Simrad EK60 echosounder.\"\"\"\n    with RawSimradFile(raw_file, \"r\", storage_options=storage_options) as fid:\n        config_datagram = fid.read(1)\n        config_datagram[\"timestamp\"] = np.datetime64(\n            config_datagram[\"timestamp\"].replace(tzinfo=None), \"[ns]\"\n        )\n        # Return True if the sounder name matches \"ER60\"\n        try:\n            return config_datagram[\"sounder_name\"] in {\"ER60\", \"EK60\"}\n        except KeyError:\n            return False\n</code></pre>"},{"location":"documentation/utils/#aalibrary.utils.timings","title":"<code>timings</code>","text":"<p>\"This script deals with the times associated with ingesting/preprocessing data from various sources. It works as follows: * A large file (usually 1 GB) is selected to repeatedly be downloaded and     uploaded to a GCP bucket. * Download and upload times are recorded for each of these n iterations. * The average of these times are presented.</p> <p>Functions:</p> Name Description <code>time_ingestion_and_upload_from_ncei</code> <p>Used for timing the ingestion from the NCEI AWS S3 bucket.</p>"},{"location":"documentation/utils/#aalibrary.utils.timings.time_ingestion_and_upload_from_ncei","title":"<code>time_ingestion_and_upload_from_ncei(n=10, ncei_file_url='https://noaa-wcsd-pds.s3.amazonaws.com/data/raw/Reuben_Lasker/RL2107/EK80/2107RL_CW-D20210813-T220732.raw', ncei_bucket='noaa-wcsd-pds', download_location='./')</code>","text":"<p>Used for timing the ingestion from the NCEI AWS S3 bucket.</p> Source code in <code>src\\aalibrary\\utils\\timings.py</code> <pre><code>def time_ingestion_and_upload_from_ncei(\n    n: int = 10,\n    ncei_file_url: str = (\n        \"https://noaa-wcsd-pds.s3.amazonaws.com/data/raw/\"\n        \"Reuben_Lasker/RL2107/EK80/\"\n        \"2107RL_CW-D20210813-T220732.raw\"\n    ),\n    ncei_bucket: str = \"noaa-wcsd-pds\",\n    download_location: str = \"./\",\n):\n    \"\"\"Used for timing the ingestion from the NCEI AWS S3 bucket.\"\"\"\n\n    download_times = []\n    upload_times = []\n    file_name = helpers.get_file_name_from_url(ncei_file_url)\n\n    for i in range(n):\n        start_time = time.time()\n        ncei_utils.download_single_file_from_aws(\n            file_url=ncei_file_url,\n            download_location=download_location,\n        )\n        time_elapsed = time.time() - start_time\n        print(\n            (\n                f\"Downloading took {time_elapsed} seconds.\"\n                f\"\\nThat's {1000/time_elapsed} mb/sec.\"\n            )\n        )\n        print(\"Uploading file to cloud storage\")\n        start_time = time.time()\n        cloud_utils.upload_file_to_gcp_bucket(\n            bucket=None,\n            blob_file_path=\"timing_test_raw_upload.raw\",\n            local_file_path=file_name,\n        )\n        time_elapsed = time.time() - start_time\n        print(\n            (\n                f\"Uploading took {time_elapsed} seconds.\"\n                f\"\\nThat's {1000/time_elapsed} mb/sec.\"\n            )\n        )\n\n    print(\n        (\n            \"Average download time for this file:\"\n            f\" {sum(download_times)/len(download_times)}\"\n        )\n    )\n    print(\n        (\n            \"Average upload time for this file:\"\n            f\" {sum(upload_times)/len(upload_times)}\"\n        )\n    )\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installing-aalibrary","title":"Installing <code>AALibrary</code>","text":"Google Cloud Workstations <p>If you are using Google Cloud Workstations, please follow the instructions outlined in the repo here.</p>"},{"location":"getting-started/installation/#step-1-installing-the-gcloud-cli-tool","title":"Step 1 - Installing the <code>gcloud</code> CLI Tool","text":"<p>To securely install AALibrary via pip, first we need to set up the Google Cloud CLI, <code>gcloud</code>:</p> WindowsLinux <p>To set up <code>gcloud</code> on windows follow the instructions here.</p> <p>Most GCP workstations, cloud shell editors, and compute engines come with <code>gcloud</code> preinstalled.</p> <p>If you have an instance without it installed, please follow the directions here to install it.</p>"},{"location":"getting-started/installation/#step-2-logging-into-gcloud","title":"Step 2 - Logging Into <code>gcloud</code>","text":"<p>Issue the following commands via Google Cloud SDK Shell (for Windows) or the terminal (for Linux). Follow the instructions to login to <code>gcloud</code> using your NOAA email. This authentication is necessary if you want to use <code>aalibrary</code> with its Google Cloud Platform capabilities.</p> <pre><code>gcloud auth login\ngcloud auth application-default login\n</code></pre>"},{"location":"getting-started/installation/#step-21-set-your-account-as-the-active-account-for-gcloud","title":"Step 2.1 - Set Your Account As The Active Account For <code>gcloud</code>","text":"<pre><code>gcloud config set account {ACCOUNT} \n</code></pre> <p>Here, <code>{ACCOUNT}</code> should be your noaa.gov email. The same one you used to sign-in in the step above.</p>"},{"location":"getting-started/installation/#step-22-set-the-aa-gcp-project-as-the-active-project-for-gcloud","title":"Step 2.2 - Set The AA GCP Project As The Active Project For <code>gcloud</code>","text":"<pre><code>gcloud config set project ggn-nmfs-aa-dev-1 \n</code></pre>"},{"location":"getting-started/installation/#step-3-optional-install-virtual-environment-before-the-pip-install","title":"Step 3 (Optional) - Install Virtual Environment Before The <code>pip install</code>","text":"<p>Some instances of Linux, such as GCP workstations, will not allow you to install packages. In this case, you will need to create a virtual environment to work out of.</p> WindowsLinux <p>Please use your preferred package manager to create a virtual environment.</p> <p>If you would like to have a virtual environment to run out of, please use the following command:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install python3-virtualenv -y\npython -m virtualenv my-venv\n</code></pre> <p>NOTE: Sudo:</p> <p>Sudo is not required for these commands. You can try to run these commands with <code>sudo</code> removed if you do not have permissions.</p>"},{"location":"getting-started/installation/#step-4-its-finally-pip-install-time","title":"Step 4 - It's Finally <code>pip install</code> Time","text":"<p>To finally be able to pip-install the library use the following command:</p> <p>If you do not have a virtual environment set up:</p> <pre><code>python -m pip install aalibrary@git+https://github.com/nmfs-ost/AA-SI_aalibrary.git\n</code></pre> <p>If you do have a virtual environment set up:</p> <pre><code>my-venv/bin/pip install aalibrary@git+https://github.com/nmfs-ost/AA-SI_aalibrary.git\n</code></pre> <p>NOTE: If using virtual environments:</p> <p>Since we have created a virtual environment, in order to use <code>aalibrary</code> simply replace all <code>python</code> commands with <code>{virtual env name}/bin/python</code> and all <code>pip</code> commands with <code>{virtual env name}/bin/pip</code>.</p>"},{"location":"getting-started/installation/#step-5-test-it-out","title":"Step 5 - Test it Out","text":"<p>Now that the library is installed, we can finally test it out. Open up a python  using the following command:</p> <pre><code># If you do not have a virtual environment\npython\n\n# If you have a virtual environment\nmy-venv/bin/python\n</code></pre> <p>Next, we will enter the following code line-by-line. This will run a test function that will allow us to quickly test connectivity in our environment.</p> <pre><code>from aalibrary import quick_test\nquick_test.start()\n</code></pre>"},{"location":"getting-started/permissions/","title":"Permissions","text":""},{"location":"getting-started/permissions/#heres-how-to-get-permissions","title":"Here's How To Get Permissions \ud83d\udcdd","text":""},{"location":"getting-started/permissions/#gcp-environment","title":"GCP Environment","text":"<p>There are currently two GCP environments being used by AASI. In order to use AALibrary fully, please get permissions to these by reaching out to Hannan Khan.</p>"},{"location":"getting-started/permissions/#gcp-workstations","title":"GCP Workstations","text":"<p>You can gain access to the workstations project by following the documentation here (see Section 1.2).</p>"},{"location":"getting-started/permissions/#ncei-s3-buckets","title":"NCEI S3 Buckets","text":"<p>The data stored in the NCEI s3 bucket is an archive, and open-data. This means that the data is accessible through empty parameters. You do not need any extra permissions to access this database. You can view the web UI here.</p>"},{"location":"getting-started/permissions/#ocean-data-lake-omao-access","title":"Ocean Data Lake (OMAO) Access","text":"<p>Not Available</p>"},{"location":"getting-started/permissions/#gcp-bigquery-metadata-database","title":"GCP BigQuery Metadata Database","text":"<p>This is the metadata database that was created for use in conjunction with AALibrary. Please reach out to Hannan Khan for access and usage.</p>"},{"location":"getting-started/testing/","title":"Let's Test \ud83d\udd75\ufe0f Our Code","text":"<p>Now that the library is installed, we can finally test it out. Open up a python  using the following command:</p> <pre><code>my-venv/bin/python\n</code></pre> <p>Next, we will enter the following code line-by-line. This will run a test function that will allow us to quickly test connectivity in our environment.</p> <pre><code>from aalibrary import quick_test\nquick_test.start()\n</code></pre>"},{"location":"troubleshooting/troubleshooting/","title":"Troubleshooting","text":""},{"location":"troubleshooting/troubleshooting/#cloud-errors","title":"Cloud Errors","text":""},{"location":"troubleshooting/troubleshooting/#the-google-cloud-sdk-warning","title":"The <code>Google Cloud SDK</code> Warning","text":"<p>UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds.</p> <p>warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)</p> <p>This is a common warning. It is possible to ignore it, or to actually suppress it if you do not like it. Suppressing this warning is not recommended since it will also lead to other, possibly helpful, warnings being suppressed as well.</p>"},{"location":"usage/configuration/","title":"Configuration","text":"<p>AALibrary comes with many default configuration options. For example, the default GCP project that will be used when creating GCP storage objects is the dev project <code>ggn-nmfs-aa-dev-1</code>.</p> <p>You can take a look at all of the default configs within code in the function signatures, or take a peek at the config.py file for variables that are used as standards within code.</p>"},{"location":"usage/configuration/#azure-configuration","title":"Azure Configuration","text":"<p>Azure configuration requires an <code>azure_config.ini</code> file that is used for storing connection strings and keys. You can create an empty file using the <code>create_azure_config_file()</code> function found in <code>helpers.py</code>.</p> <p>NOTE</p> <p>You will also need to have a space before and after the equals sign <code>=</code> when defining a value. For example, <code>azure_account_url = https...</code>.</p>"},{"location":"usage/conversions/","title":"Conversions","text":""},{"location":"usage/conversions/#converting-a-raw-into-netcdf","title":"Converting A Raw Into Netcdf","text":"<p>In order to convert a raw file into a netcdf, use the following example as a guide:</p> <pre><code>from aalibrary import utils\nfrom aalibrary.ingestion import convert_raw_to_netcdf\n\n# Create a GCP bucket object\ngcp_stor_client, gcp_bucket_name, gcp_bucket = utils.cloud_utils.setup_gcp_storage_objs()\n\n# This function takes care of downloading, converting, and uploading (caching) the netcdf file in gcp.\nconvert_raw_to_netcdf(file_name=\"2107RL_CW-D20210813-T220732.raw\",\n                      file_type=\"raw\",\n                      ship_name=\"Reuben_Lasker\",\n                      survey_name=\"RL2107\",\n                      echosounder=\"EK80\",\n                      data_source=\"NCEI\",\n                      file_download_directory=\"./\",\n                      overwrite=False,\n                      gcp_bucket=gcp_bucket,\n                      debug=False)\n</code></pre>"},{"location":"usage/downloads/","title":"Downloads","text":"<p>Here are some examples of download/ingestion functions that you can use in this library.</p>"},{"location":"usage/downloads/#downloading-a-raw-file-from-ncei","title":"Downloading A Raw File From NCEI","text":"<p>In order to download a raw file from NCEI, use the following example:</p> <pre><code>from aalibrary.ingestion import download_raw_file_from_ncei\n\n# This function takes care of downloading, converting, and uploading (caching) the netcdf file in gcp.\ndownload_raw_file_from_ncei(file_name=\"2107RL_CW-D20210813-T220732.raw\",\n                            file_type=\"raw\",\n                            ship_name=\"Reuben_Lasker\",\n                            survey_name=\"RL2107\",\n                            echosounder=\"EK80\",\n                            data_source=\"NCEI\",\n                            file_download_directory=\".\",\n                            upload_to_gcp=True,   # Set to True if you want to upload the raw file to gcp\n                            debug=False)\n</code></pre> <p>If you would like to just download a raw file, but do not care about it's source, you can use the following function:</p> <pre><code>from aalibrary.ingestion import download_raw_file\n\ndownload_raw_file(file_name=\"2107RL_CW-D20210813-T220732.raw\",\n                  file_type=\"raw\",\n                  ship_name=\"Reuben_Lasker\",\n                  survey_name=\"RL2107\",\n                  echosounder=\"EK80\",\n                  data_source=\"NCEI\",\n                  file_download_directory=\".\",\n                  debug=False)\n</code></pre>"},{"location":"usage/downloads/#downloading-an-entire-survey-from-ncei","title":"Downloading An Entire Survey From NCEI","text":"<p>Sometimes, you will need to download an entire survey from NCEI for analysis. This is possible using the AALibrary. Follow the code snippet below.</p> <p>NOTE</p> <p>This function will automatically create the appropriate subdirectories within the <code>download_directory</code> param that you have specified. For example: within the snippet below, the data will exist in <code>./test_data_dir/Reuben_Lasker/RL2107/...</code></p> <pre><code>from aalibrary.ingestion import download_survey_from_ncei\n\ndownload_survey_from_ncei(ship_name=\"Reuben_Lasker\",\n                          survey_name=\"RL2107\",\n                          download_directory=\"./test_data_dir\",\n                          debug: bool = False)\n</code></pre>"},{"location":"usage/downloads/#downloading-a-raw-file-from-azure-data-lake-omao","title":"Downloading A Raw File From Azure Data Lake (OMAO)","text":"<p>Use the following code if you would like to download a file from the Azure Data Lake. The code requires a <code>config.ini</code> file.</p> <p>NOTE: This file needs to have a <code>[DEFAULT]</code> section with a <code>azure_connection_string</code> variable set.</p> <pre><code>from aalibrary.ingestion import download_raw_file_from_azure\n\ndownload_raw_file_from_azure(\n    file_name=\"1601RL-D20160107-T074016.raw\",\n    file_type=\"raw\",\n    ship_name=\"Reuben_Lasker\",\n    survey_name=\"RL1601\",\n    echosounder=\"EK60\",\n    data_source=\"OMAO\",\n    file_download_directory=\".\",\n    config_file_path=\"./azure_config.ini\",\n    upload_to_gcp=True,\n    debug=True,\n)\n</code></pre> <p>If you would like a single file downloaded using a path, you can use the following much more simple code:</p> <pre><code>from aalibrary.ingestion import download_specific_file_from_azure\n\ndownload_specific_file_from_azure(\n    config_file_path=\"./azure_config.ini\",\n    container_name=\"testcontainer\",\n    file_path_in_container=\"RL2107_EK80_WCSD_EK80-metadata.json\",\n)\n</code></pre> <p>NOTE: Please keep in mind that this method creates a connection every single time you call it.</p>"},{"location":"usage/downloads/#downloading-a-netcdf","title":"Downloading A Netcdf","text":"<p>Netcdf files (converted over from raw) only exist in the GCP cache as of now. The following example takes care of downloading a particular raw file as netcdf4 (if it had already been converted and cached in GCP, otherwise an error message is thrown):</p> <pre><code>from aalibrary import utils\nfrom aalibrary.ingestion import download_netcdf_file\n\n# Create a GCP bucket object\ngcp_stor_client, gcp_bucket_name, gcp_bucket = utils.cloud_utils.setup_gcp_storage_objs()\n\n# This function takes care of downloading the netcdf.\ndownload_netcdf_file(\n                raw_file_name=\"2107RL_CW-D20210813-T220732.raw\",\n                file_type=\"netcdf\",\n                ship_name=\"Reuben_Lasker\",\n                survey_name=\"RL2107\",\n                echosounder=\"EK80\",\n                data_source=\"NCEI\",\n                file_download_location=\".\",\n                gcp_bucket=gcp_bucket,\n                debug=False)\n</code></pre>"},{"location":"usage/downloads/#downloading-multiple-files-from-a-survey","title":"Downloading Multiple Files From A Survey","text":"<pre><code>from aalibrary.ingestion import download_raw_file_from_ncei\n\nfile_names = [\"2107RL_CW-D20210813-T220732.raw\",\n              \"2107RL_CW-D20210706-T172335.raw\"]\nfor file_name in file_names:\n  download_raw_file_from_ncei(\n    file_name=file_name,\n    file_type=\"raw\",\n    ship_name=\"Reuben_Lasker\",\n    survey_name=\"RL2107\",\n    echosounder=\"EK80\",\n    data_source=\"NCEI\",\n    file_download_directory=\".\",\n    upload_to_gcp=True,   # Set to True if you want to upload the raw file to gcp\n    debug=False)\n</code></pre>"},{"location":"usage/scripts/","title":"One-Time Scripts","text":"<p>The repository for AALibrary also includes many useful, pre-made scripts that users can run. These scripts serve the purpose of being run once, and usually accomplish small but important tasks. You can take a look below:</p> <p>NOTE</p> <p>These scripts are self-contained. AALibrary does not have to be installed to run most of these scripts.</p>"},{"location":"usage/scripts/#scripts-location","title":"Scripts Location","text":"<p>Most scripts are located within the repo in the <code>other</code> folder.</p>"},{"location":"usage/scripts/#ncei-metadata-backfilling-script","title":"NCEI Metadata Backfilling Script","text":"<p>This script is located here. It is used for backfilling metadata from NCEI to the Metadata DB located in our GCP environment. You will need to extract all metadata from the NCEI database into an Excel file. The script will then automatically parse through the data in the Excel file and populate the Metadata DB in GCP.</p>"},{"location":"usage/uploads/","title":"Uploads","text":"<p>You can use AALibrary to upload your active acoustics data to our GCP environments. Make sure you have the necessary permissions for the correct GCP environment before you start.</p> <p>There are numerous methods for uploads, each with its own benefits. If uploading over network, the size of the data should be limited to 5 TBs in size, and the upload must take place during a time period where network traffic is minimal. This is done to limit the amount of bandwidth used.</p> <p>For a detailed description (pros/cons) of all upload tools/options, take a look at the Data Migration Options Documentation.</p> <p>INFO: More Info On GCP</p> <p>For more information on GCP implementation, please take a look at the GCP Overview Page.</p>"},{"location":"usage/uploads/#uploading-echosounder-files-to-gcp","title":"Uploading Echosounder Files To GCP","text":"<p>In order to upload selective Echosounder files (.raw, .idx, .bot, .nc) to GCP, use the following snippet. This function maintains the formatting and folder structure that AALibrary uses. This makes retrieval of the files using the AALibrary possible.</p> <pre><code>from aalibrary.egress import (\n    upload_local_echosounder_files_from_directory_to_gcp_storage_bucket\n)\nfrom aalibrary.utils.cloud_utils import setup_gcp_storage_objs\n\ngcp_stor_client, gcp_bucket_name, gcp_bucket = (\n    setup_gcp_storage_objs(\n        project_id=\"ggn-nmfs-aa-dev-1\",\n        gcp_bucket_name=\"ggn-nmfs-aa-dev-1-data\",\n    )\n)\nupload_local_echosounder_files_from_directory_to_gcp_storage_bucket(\n        local_echosounder_directory_to_upload=\"./Reuben_Lasker/RL2107/EK80/\",\n        ship_name=\"Reuben_Lasker\",\n        survey_name=\"RL2107\",\n        echosounder=\"EK80\",\n        data_source=\"HDD\", # &lt;== Refers to the fact that this is uploaded from local.\n        gcp_bucket=gcp_bucket,\n        debug=True,\n)\n</code></pre>"},{"location":"usage/uploads/#uploading-a-surveyfolder-as-is-to-gcp","title":"Uploading A Survey/Folder <code>As-Is</code> To GCP","text":"<p>If you would like to upload a folder to the GCP storage bucket as-is, you can use this function.</p> <pre><code>from aalibrary.egress import upload_folder_as_is_to_gcp\nfrom aalibrary.utils.cloud_utils import setup_gcp_storage_objs\n\n# Here we specify the project and bucket we would like to upload to\ngcp_stor_client, gcp_bucket_name, gcp_bucket = (\n    setup_gcp_storage_objs(\n        project_id=\"ggn-nmfs-aa-dev-1\",\n        gcp_bucket_name=\"ggn-nmfs-aa-dev-1-data\",\n    )\n)\n\n# You can also specify a 'destination prefix'; used for putting the folder\n# in a certain place within the bucket.\nupload_folder_as_is_to_gcp(\n    local_folder_path=\"./test_data_dir/Reuben_Lasker/\",\n    gcp_bucket=gcp_bucket,\n    destination_prefix=\"other/deletable/\",\n)\n</code></pre>"}]}